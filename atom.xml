<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-07-16T01:38:06.431Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Tim Chan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>中国历史路线演进图</title>
    <link href="http://example.com/2023/07/13/%E4%B8%AD%E5%9B%BD%E5%8E%86%E5%8F%B2%E8%B7%AF%E7%BA%BF%E5%9B%BE/"/>
    <id>http://example.com/2023/07/13/%E4%B8%AD%E5%9B%BD%E5%8E%86%E5%8F%B2%E8%B7%AF%E7%BA%BF%E5%9B%BE/</id>
    <published>2023-07-13T01:59:33.000Z</published>
    <updated>2023-07-16T01:38:06.431Z</updated>
    
    <content type="html"><![CDATA[<h3 id="从历史学习经验，人生充满希望！"><a href="#从历史学习经验，人生充满希望！" class="headerlink" title="从历史学习经验，人生充满希望！"></a>从历史学习经验，人生充满希望！</h3><p> <img src="/img/wuweiwu/world/china/history-line/01.jpg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;从历史学习经验，人生充满希望！&quot;&gt;&lt;a href=&quot;#从历史学习经验，人生充满希望！&quot; class=&quot;headerlink&quot; title=&quot;从历史学习经验，人生充满希望！&quot;&gt;&lt;/a&gt;从历史学习经验，人生充满希望！&lt;/h3&gt;&lt;p&gt; &lt;img src=&quot;/img/wu</summary>
      
    
    
    
    <category term="认知-修行-平衡" scheme="http://example.com/categories/%E8%AE%A4%E7%9F%A5-%E4%BF%AE%E8%A1%8C-%E5%B9%B3%E8%A1%A1/"/>
    
    
    <category term="中国历史路线" scheme="http://example.com/tags/%E4%B8%AD%E5%9B%BD%E5%8E%86%E5%8F%B2%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>什么是人性？</title>
    <link href="http://example.com/2023/07/13/%E4%BA%BA%E6%80%A7%E6%98%AF%E4%BB%80%E4%B9%88/"/>
    <id>http://example.com/2023/07/13/%E4%BA%BA%E6%80%A7%E6%98%AF%E4%BB%80%E4%B9%88/</id>
    <published>2023-07-13T01:59:33.000Z</published>
    <updated>2023-07-15T02:41:32.868Z</updated>
    
    <content type="html"><![CDATA[<ul><li>人性是一个概念性的词语，它是人对事物反应的情绪总和。</li><li>世上所有关系的结合和打破，就是关乎两个字：利益。人性背后的本质就是利益，而利益包含了物质利益和精神利益。</li><li>如何懂得人性？就是要有认知，认知来之两个途径：学习和实践。</li><li>道德，道和德拆解。道是一种自然规律。德是人自我约制的表象。舍得，先舍后得。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;人性是一个概念性的词语，它是人对事物反应的情绪总和。&lt;/li&gt;
&lt;li&gt;世上所有关系的结合和打破，就是关乎两个字：利益。人性背后的本质就是利益，而利益包含了物质利益和精神利益。&lt;/li&gt;
&lt;li&gt;如何懂得人性？就是要有认知，认知来之两个途径：学习和实践。&lt;/li</summary>
      
    
    
    
    <category term="认知-修行-平衡" scheme="http://example.com/categories/%E8%AE%A4%E7%9F%A5-%E4%BF%AE%E8%A1%8C-%E5%B9%B3%E8%A1%A1/"/>
    
    
    <category term="人性" scheme="http://example.com/tags/%E4%BA%BA%E6%80%A7/"/>
    
  </entry>
  
  <entry>
    <title>monica AI GPT 总结视频内容2</title>
    <link href="http://example.com/2023/06/15/monica-ai-exam2/"/>
    <id>http://example.com/2023/06/15/monica-ai-exam2/</id>
    <published>2023-06-15T05:25:10.000Z</published>
    <updated>2023-07-16T11:52:06.754Z</updated>
    
    <content type="html"><![CDATA[<p>Sam Altman - How to Succeed with a Startup](<a class="link"   href="https://www.youtube.com/watch?v=0lJKucu6HJc&ab_channel=YCombinator)%E7%9A%84%E8%AF%A6%E7%BB%86%E6%91%98%E8%A6%81%EF%BC%9A" >https://www.youtube.com/watch?v=0lJKucu6HJc&amp;ab_channel=YCombinator)的详细摘要：<i class="fas fa-external-link-alt"></i></a> <a class="link"   href="https://monica.im/" >Monica<i class="fas fa-external-link-alt"></i></a></p><p><a class="link"   href="https://www.youtube.com/watch?v=0lJKucu6HJc&ab_channel=YCombinator&t=0.03" >00:00<i class="fas fa-external-link-alt"></i></a>打造一个人们自发告诉朋友的伟大产品是创业公司成功的关键。</p><ul><li>成功的程度与建立一个人们喜爱的伟大产品成正比。</li><li>像谷歌和Facebook这样成功的公司就是被人们通过口碑发现的。</li><li>一个简单易懂的产品是成功的重要指标。</li><li>初创企业需要寻找一个正在经历或即将经历指数级增长的市场。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=0lJKucu6HJc&ab_channel=YCombinator&t=157.459" >02:37<i class="fas fa-external-link-alt"></i></a> 初创企业在下大赌注之前，需要有一个传道授业的创始人，一个雄心勃勃的愿景和每个用户的密集使用量。</p><ul><li>iPhone的成功表明，有些东西已经从根本上发生了变化，有一个新的计算平台将催生巨大的业务。</li><li>VR也许有一天会成为大生意，但目前，大多数拥有VR头盔的人从未或很少使用它。</li><li>初创企业至少需要一位传教士式的创始人，他可以用热情感染整个世界，让大家了解公司要做什么。</li><li>拥有一个雄心勃勃的愿景有助于建立一个团队和吸引投资者。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=0lJKucu6HJc&ab_channel=YCombinator&t=317.21" >05:17<i class="fas fa-external-link-alt"></i></a>对未来有自信和明确的看法，有雄心勃勃的愿景，但如果成功的话，规模巨大的初创企业将吸引最好的人才。</p><ul><li>对未来有一个自信和明确的看法，并且是一个明确的领导者，这与成功相关。</li><li>建立一个伟大的团队是一个创始人可以做的最重要的事情之一。</li><li>创始人要经历一个从建立产品到建立公司的过渡。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=0lJKucu6HJc&ab_channel=YCombinator&t=476.56" >07:56<i class="fas fa-external-link-alt"></i></a>让团队成员不断提出新的想法和 “我们会想出办法 “的精神对早期的创业团队很重要。</p><ul><li>初创企业通过快速行动和在事情不成功的情况下迅速调整来赢得胜利。</li><li>拥有说 “我知道了 “并偏重于行动的团队成员是很重要的。</li><li>没有经验的祝福会导致不可思议的事情，因为没有人告诉他们这很难。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=0lJKucu6HJc&ab_channel=YCombinator&t=635.589" >10:35<i class="fas fa-external-link-alt"></i></a>初创企业需要保持势头，有竞争优势，有合理的商业模式，并有发展计划。</p><ul><li>势头对于初创企业继续提供超出预期的结果至关重要</li><li>长期竞争优势是成功的必要条件</li><li>合理的商业模式是很重要的，即使在开始时没有完全想好。</li><li>一个增长和获取用户的计划是成功的必要条件。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=0lJKucu6HJc&ab_channel=YCombinator&t=793.58" >13:13<i class="fas fa-external-link-alt"></i></a>与大公司相比，初创企业在决策速度、在坏主意上取胜的能力以及在大平台转变上取胜方面具有优势。</p><ul><li>初创企业只需要一个人同意就可以追求一个想法，而大公司则需要所有人同意。</li><li>初创企业在不断变化的市场中具有敏捷性和速度的优势，使他们能够做出更好、更快的决定。</li><li>初创企业通常在大的平台转变中获胜，因为大公司在进行战略转移时比较慢。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Sam Altman - How to Succeed with a Startup](&lt;a class=&quot;link&quot;   href=&quot;https://www.youtube.com/watch?v=0lJKucu6HJc&amp;ab_channel=YCombinator)%E</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="GPT" scheme="http://example.com/tags/GPT/"/>
    
    <category term="monica-AI" scheme="http://example.com/tags/monica-AI/"/>
    
  </entry>
  
  <entry>
    <title>monica AI GPT 总结视频内容1</title>
    <link href="http://example.com/2023/06/15/monica-ai-exam1/"/>
    <id>http://example.com/2023/06/15/monica-ai-exam1/</id>
    <published>2023-06-15T05:21:10.000Z</published>
    <updated>2023-07-16T11:52:21.108Z</updated>
    
    <content type="html"><![CDATA[<p>OpenAI Sam Altman &amp; Greg Brockman: Fireside Chat in Seoul, Korea | SoftBank Ventures Asia](<a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia)%E7%9A%84%E8%AF%A6%E7%BB%86%E6%91%98%E8%A6%81%EF%BC%9A%5BMonica%5D(https://monica.im)" >https://www.youtube.com/watch?v=_hpuPi7YZX8&amp;ab_channel=SoftBankVenturesAsia)的详细摘要：[Monica](https://monica.im)<i class="fas fa-external-link-alt"></i></a></p><p><a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia&t=0.9" >00:00<i class="fas fa-external-link-alt"></i></a> Sam Altman和Greg Brockman讨论了他们对韩国文化的尊重和钦佩，以及韩国的创新人工智能初创企业。</p><ul><li>萨姆会说韩语，他喜欢韩国文化中传统和新事物的结合</li><li>格雷格非常尊重韩国的技术，特别是在互联网接入方面。</li><li>两人都对韩国的人工智能初创企业的发展程度和对聊天GPT技术的热情印象深刻。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia&t=435.9" >07:15<i class="fas fa-external-link-alt"></i></a>OpenAI不仅仅是一家语言模型公司，而是一家专注于深度学习和向AGI迈进的AI公司。</p><ul><li>OpenAI尝试了很多失败的东西，但只看到成功的东西。</li><li>实现AGI的进展可以用最大的项目规模来衡量。</li><li>OpenAI已经致力于不同的主题，包括机器人学和强化学习。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia&t=870.6" >14:30<i class="fas fa-external-link-alt"></i></a>销售一个模糊的愿景需要自信心，并在从现实中学习时调整战术。</p><ul><li>在人工智能领域没有正确的答案，只有随着时间推移变得更加准确的答案。</li><li>相信技术并有一个基本的赌注是很重要的。</li><li>最初的战术可能经不起时间的考验，需要随着新挑战的出现进行调整。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia&t=1306.919" >21:46<i class="fas fa-external-link-alt"></i></a>处理快速变化的技术的最佳建议是成为适应新技术的专家，并利用它来解决问题。</p><ul><li>具体的技术技能会很快变得过时。</li><li>适应新技术并找到使用它来解决问题的方法的元技能更重要。</li><li>不同的国家将以不同的方式监管人工智能的使用情况。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia&t=1743" >29:03<i class="fas fa-external-link-alt"></i></a>OpenAI有失败的项目，也有成功的项目，一些项目由于技术还没有准备好而失败，而另一些项目尽管一开始就觉得注定要失败，但结果还是成功了。</p><ul><li>OpenAI Universe是一个由于技术还没有准备好而失败的项目。</li><li>另一方面，API项目最初感觉是注定要失败的，但最后却获得了巨大的成功。</li><li>对于API项目，OpenAI经历了与客户交谈的痛苦，并在推出之前获得了早期采用者。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia&t=2178.78" >36:18<i class="fas fa-external-link-alt"></i></a> 人工智能将使人们更有生产力，并创造新的工作类型，但技术变革的速度是一个问题。</p><ul><li>许多领域对劳动力的需求过剩。</li><li>人工智能将创造出难以想象的新工作类别。</li><li>人类的创造力和能力将依然重要。</li><li>技术变革的速度是一个问题，UBI是潜在解决方案的一个有趣的组成部分。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia&t=2615.46" >43:35<i class="fas fa-external-link-alt"></i></a> OpenAI认为，内容创作者应该从人工智能技术中获益，要找到补偿的机制，必须进行实验。</p><ul><li>OpenAI认为，内容创作者应该为他们帮助AI变得更好的工作得到补偿。</li><li>实验是必要的，以找到补偿的机制。</li><li>OpenAI直接与社区中正在创作的人接触。</li></ul><p><a class="link"   href="https://www.youtube.com/watch?v=_hpuPi7YZX8&ab_channel=SoftBankVenturesAsia&t=3051.24" >50:51<i class="fas fa-external-link-alt"></i></a>人工智能的进步是许多术语的乘法，计算硬件是一个重要因素，还有数据和工程口径。</p><ul><li>AI的进步是计算能力、数据和工程口径的产物。</li><li>为了优化人工智能的进展，所有因素都应该是相等的。</li><li>定制硅领域对建立新的架构很有意思。</li><li>定制硅领域的障碍是建立伟大的软件。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;OpenAI Sam Altman &amp;amp; Greg Brockman: Fireside Chat in Seoul, Korea | SoftBank Ventures Asia](&lt;a class=&quot;link&quot;   href=&quot;https://www.youtub</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="GPT" scheme="http://example.com/tags/GPT/"/>
    
    <category term="monica-AI" scheme="http://example.com/tags/monica-AI/"/>
    
  </entry>
  
  <entry>
    <title>conda docker git vim ssr</title>
    <link href="http://example.com/2023/06/12/conda_docker_git_vim/"/>
    <id>http://example.com/2023/06/12/conda_docker_git_vim/</id>
    <published>2023-06-12T14:46:10.000Z</published>
    <updated>2023-07-16T11:46:27.744Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自建ssr服务器"><a href="#自建ssr服务器" class="headerlink" title="自建ssr服务器"></a>自建ssr服务器</h1><ul><li><a class="link"   href="https://github.com/Alvin9999/new-pac/wiki/%E8%87%AA%E5%BB%BAss%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%99%E7%A8%8B" >https://github.com/Alvin9999/new-pac/wiki/%E8%87%AA%E5%BB%BAss%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%99%E7%A8%8B<i class="fas fa-external-link-alt"></i></a></li></ul><h1 id="conda的使用笔记"><a href="#conda的使用笔记" class="headerlink" title="conda的使用笔记"></a>conda的使用笔记</h1><ol><li>创建环境conda create -n name python&#x3D;3.8</li><li>激活环境conda activate name</li><li>关闭环境conda deactivate</li><li>查看当前环境conda env –list</li><li>查看前期环境中pip列表pip list</li><li>查看指定pip包的安装pip list | grep packagename</li><li>删除环境conda remove -n name –all</li><li>克隆环境conda create -n BBB –clone AAA，B是A的克隆</li><li>转移环境<ol><li>conda转移：conda env export &gt; name.yaml,然后新系统下执行conda env create -f name.yaml</li><li>pip转移：pip freeze &gt; requirements.txt,然后新系统下pip install -r requirements.txt</li></ol></li></ol><h1 id="docker的使用笔记，轻量化虚拟技术"><a href="#docker的使用笔记，轻量化虚拟技术" class="headerlink" title="docker的使用笔记，轻量化虚拟技术"></a>docker的使用笔记，轻量化虚拟技术</h1><ol start="0"><li><p>缺点：争抢资源，权限过高；优点：可移植性高，环境封闭</p></li><li><p>基础概念：dockfile，images，container, volume</p><ol><li>dockfile：docker的配置文件</li><li>image：docker的镜像文件，用来创建container的母本</li><li>container：俗称容器，docker最终落地的形态，是一个运行程序的隔离环境。</li><li>volume：数据卷，</li></ol></li><li><p>dockerfile常见的命令(大写形式)：</p><ol><li>FROM：</li><li>WORKDIR：</li><li>COPY</li><li>RUN</li><li>CMD</li></ol></li><li><p>docker 常见命令</p><ol><li>docker create</li><li>docker ps -aq，查看所有的容器</li><li>docker images -q，查看所有的镜像</li><li>docker build</li><li>docker run -p -v<ol><li>数据和命令，通过映射的方式在宿主和容器之间进行传递，其中-p表示port，端口，-v表示volume，卷。 -d,daemon，在后台运行。–name，表示名字<figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="keyword">run</span><span class="language-bash"> -d --name music -p 264:264 -v /opt/musciplayer:/var/www/html/cache/music-player-docker</span></span><br></pre></td></tr></table></figure></li><li>例子：通过冒号作为宿主和容器之间的分隔符!<figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">宿主 : 容器</span><br></pre></td></tr></table></figure></li><li></li></ol></li><li>docker stop <container id>，停止指定容器，docker stop $(docker ps -aq)，停止所有容器</li><li>docker restart <container id></li><li>docker rm <container id>，删除指定容器，docker rm $(docker ps -aq)，删除所有容器</li><li>docker rmi <container id>，删除指定镜像，docker rmi $(docker images -q)，删除所有镜像</li><li>docker exec -it <container id> &#x2F;bin&#x2F;bash</li><li>docker volume create </li><li>docker cp <container id>:&#x2F;opt&#x2F;file.txt &#x2F;opt&#x2F;local&#x2F;</li><li>docker cp &#x2F;opt&#x2F;local&#x2F;file.txt <container id>:&#x2F;opt&#x2F;</li><li>docker有了专门清理资源(container、image、网络)的命令，docker system prune</li><li>docker image prune –force –all 或者docker image prune -f -a 删除所有不使用的镜像</li><li>docker container prune -f 删除所有停止的容器</li></ol></li><li><p>docker compose</p><ol><li>docker compose up</li><li>docker compose down</li></ol></li></ol><h1 id="git的使用笔记"><a href="#git的使用笔记" class="headerlink" title="git的使用笔记"></a>git的使用笔记</h1><h1 id="vim的使用笔记"><a href="#vim的使用笔记" class="headerlink" title="vim的使用笔记"></a>vim的使用笔记</h1><h1 id="ssh知识和问题集"><a href="#ssh知识和问题集" class="headerlink" title="ssh知识和问题集"></a>ssh知识和问题集</h1><ol><li><p>ssh连接发生错误信息：kex_exchange_identification: Connection closed by remote host</p><ul><li>解决方法：可能是认证的key失效了，进入~&#x2F;.ssh，删除know_hosts目录，重新连接一下</li></ul></li><li><p>基本命令<br>pwd 打印当前文件夹的路径<br>cd  改变目录, 相当于 windows 下的打开文件夹<br>ls  展示当前目录下的所有文件内容<br>mkdir 创建目录, 创建文件夹<br>rmdir 删除文件夹<br>touch 创建文件<br>rm 删除文件<br>cat 原意是指将文件与终端输出流连接, 通俗的说输出文件内容<br>less 或 more( Unix ) 命令可以分页的打开文件, 注意使用 q 退出<br>echo 命令 打印</p><blockquote><p>重定向, 如果文件存在, 则将文件截断, 并重新输入.</p><blockquote><p> 重定向. 不截断重定向<br>ipconfig&#x2F;all  查看ip信息<br>ping IP地址    查看是否连接成功<br>tips:<br>注意: 换行问题:在 linux 中使用 \n 表示换行;在 windows 中使用 \r\n 表示换行<br>注意:文件截断是指将文件的所有内容删除, 但是文件的创建时间等信息不更改</p></blockquote></blockquote></li><li><p>查看某一目录下所有文件夹的内存占用</p></li></ol><ul><li>du -sh .&#x2F;* –exclude proc</li></ul><ol start="4"><li>查看各磁盘内存情况</li></ol><ul><li>df -h</li></ul><ol start="5"><li><p>开启ssh远程连接服务</p><ol><li>测试是否安装了openssh server，sudo service ssh start</li><li>如果没有安装，则安装，sudo apt update &amp;&amp; sudo apt install openssh-server</li><li>安装完毕则可以开始连接了</li></ol></li><li><p>本地机器和远程服务器之间传送文件</p><ol><li>scp命令，scp  [参数] &lt;源地址（用户名@IP地址或主机名）&gt;:&lt;文件路径&gt; &lt;目的地址（用户名 @IP 地址或主机名）&gt;:&lt;文件路径&gt;</li><li>#拷贝文件夹，加-r参数</li><li>例子；scp -r &#x2F;home&#x2F;tim&#x2F;workspace&#x2F;github&#x2F;style <a class="link"   href="mailto:&#x74;&#105;&#x6d;&#x40;&#x31;&#57;&#50;&#x2e;&#x31;&#x36;&#x38;&#x2e;&#120;&#x78;&#120;&#x2e;&#x78;&#x78;" >&#x74;&#105;&#x6d;&#x40;&#x31;&#57;&#50;&#x2e;&#x31;&#x36;&#x38;&#x2e;&#120;&#x78;&#120;&#x2e;&#x78;&#x78;<i class="fas fa-external-link-alt"></i></a>:&#x2F;home&#x2F;tim&#x2F;github&#x2F;style</li></ol></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;自建ssr服务器&quot;&gt;&lt;a href=&quot;#自建ssr服务器&quot; class=&quot;headerlink&quot; title=&quot;自建ssr服务器&quot;&gt;&lt;/a&gt;自建ssr服务器&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&quot;link&quot;   href=&quot;https://github.c</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="conda-docker-git-vim-ssr" scheme="http://example.com/tags/conda-docker-git-vim-ssr/"/>
    
  </entry>
  
  <entry>
    <title>FLOPs</title>
    <link href="http://example.com/2023/05/31/FLOPs/"/>
    <id>http://example.com/2023/05/31/FLOPs/</id>
    <published>2023-05-31T05:19:10.000Z</published>
    <updated>2023-07-16T11:35:07.645Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NPU的性能参数分解"><a href="#NPU的性能参数分解" class="headerlink" title="NPU的性能参数分解"></a>NPU的性能参数分解</h1><p>注意相同 FLOPs 的两个模型其运行速度是会相差很多的，因为影响模型运行速度的两个重要因素只通过 FLOPs 是考虑不到的，比如 MAC（Memory Access Cost）和网络并行度；二是具有相同 FLOPs 的模型在不同的平台上可能运行速度不一样。</p><p>&#x3D;&#x3D;模型参数量的分析是为了了解内存占用情况，内存带宽其实比 FLOPs 更重要。目前的计算机结构下，单次内存访问比单次运算慢得多的多。&#x3D;&#x3D;对每一层网络，端侧设备需要：</p><p>从主内存中读取输入向量 &#x2F; feature map；<br>从主内存中读取权重并计算点积；<br>将输出向量或 feature map 写回主内存。</p><h2 id="双精度、单精度和半精度"><a href="#双精度、单精度和半精度" class="headerlink" title="双精度、单精度和半精度"></a>双精度、单精度和半精度</h2><p>CPU&#x2F;GPU 的浮点计算能力得区分不同精度的浮点数，分为双精度 FP64、单精度 FP32 和半精度 FP16。因为采用不同位数的浮点数的表达精度不一样，所以造成的计算误差也不一样，对于需要处理的数字范围大而且需要精确计算的科学计算来说，就要求采用双精度浮点数，而对于常见的多媒体和图形处理计算，32 位的单精度浮点计算已经足够了，对于要求精度更低的机器学习等一些应用来说，半精度 16 位浮点数就可以甚至 8 位浮点数就已经够用了。 对于浮点计算来说， CPU 可以同时支持不同精度的浮点运算，但在 GPU 里针对单精度和双精度就需要各自独立的计算单元。</p><h2 id="浮点计算能力"><a href="#浮点计算能力" class="headerlink" title="浮点计算能力"></a>浮点计算能力</h2><p>FLOPS：每秒浮点运算次数，每秒所执行的浮点运算次数，浮点运算包括了所有涉及小数的运算，比整数运算更费时间。下面几个是表示浮点运算能力的单位。我们一般常用 TFLOPS(Tops) 作为衡量 NPU&#x2F;GPU 性能&#x2F;算力的指标，比如海思 3519AV100 芯片的算力为 1.7Tops 神经网络运算性能。</p><p>MFLOPS（megaFLOPS）：等于每秒一佰万（&#x3D;10^6）次的浮点运算。<br>GFLOPS（gigaFLOPS）：等于每秒拾亿（&#x3D;10^9）次的浮点运算。<br>TFLOPS（teraFLOPS）：等于每秒万亿（&#x3D;10^12）次的浮点运算。<br>PFLOPS（petaFLOPS）：等于每秒千万亿（&#x3D;10^15）次的浮点运算。<br>EFLOPS（exaFLOPS）：等于每秒百亿亿（&#x3D;10^18）次的浮点运算。</p><h2 id="硬件利用率-Utilization"><a href="#硬件利用率-Utilization" class="headerlink" title="硬件利用率(Utilization)"></a>硬件利用率(Utilization)</h2><p>在这种情况下，利用率（Utilization）是可以有效地用于实际工作负载的芯片的原始计算能力的百分比。深度学习和神经网络使用相对数量较少的计算原语（computational primitives），而这些数量很少的计算原语却占用了大部分计算时间。矩阵乘法（MM）和转置是基本操作。MM 由乘法累加（MAC）操作组成。OPs&#x2F;s（每秒完成操作的数量）指标通过每秒可以完成多少个 MAC（每次乘法和累加各被认为是 1 个 operation，因此 MAC 实际上是 2 个 OP）得到。所以我们可以将利用率定义为实际使用的运算能力和原始运算能力的比值：<br>————————————————<br>版权声明：本文为CSDN博主「那个苏轼回不来了丶」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a class="link"   href="https://blog.csdn.net/qq_45763093/article/details/118519790" >https://blog.csdn.net/qq_45763093/article/details/118519790<i class="fas fa-external-link-alt"></i></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NPU的性能参数分解&quot;&gt;&lt;a href=&quot;#NPU的性能参数分解&quot; class=&quot;headerlink&quot; title=&quot;NPU的性能参数分解&quot;&gt;&lt;/a&gt;NPU的性能参数分解&lt;/h1&gt;&lt;p&gt;注意相同 FLOPs 的两个模型其运行速度是会相差很多的，因为影响模型运行速</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="FLOPs" scheme="http://example.com/tags/FLOPs/"/>
    
  </entry>
  
  <entry>
    <title>水墨先生算命</title>
    <link href="http://example.com/2023/05/28/mozi91_luck/"/>
    <id>http://example.com/2023/05/28/mozi91_luck/</id>
    <published>2023-05-28T02:40:33.000Z</published>
    <updated>2023-07-16T11:23:43.887Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/img/wuweiwu/about/mozi_luck/01.png" alt="01"><br><img src="/img/wuweiwu/about/mozi_luck/02.png" alt="02"><br><img src="/img/wuweiwu/about/mozi_luck/03.png" alt="03"><br><img src="/img/wuweiwu/about/mozi_luck/04.png" alt="04"><br><img src="/img/wuweiwu/about/mozi_luck/05.png" alt="05"><br><img src="/img/wuweiwu/about/mozi_luck/06.png" alt="06"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/img/wuweiwu/about/mozi_luck/01.png&quot; alt=&quot;01&quot;&gt;&lt;br&gt;&lt;img src=&quot;/img/wuweiwu/about/mozi_luck/02.png&quot; alt=&quot;02&quot;&gt;&lt;br&gt;&lt;img src=&quot;/img/wu</summary>
      
    
    
    
    <category term="认知-修行-平衡" scheme="http://example.com/categories/%E8%AE%A4%E7%9F%A5-%E4%BF%AE%E8%A1%8C-%E5%B9%B3%E8%A1%A1/"/>
    
    
    <category term="命理" scheme="http://example.com/tags/%E5%91%BD%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>现代机器学习监控的混乱局面：重新思考流媒体评估（4/4）</title>
    <link href="http://example.com/2023/05/24/model_monitoring_mess_part04/"/>
    <id>http://example.com/2023/05/24/model_monitoring_mess_part04/</id>
    <published>2023-05-24T14:45:16.000Z</published>
    <updated>2023-07-16T12:11:32.245Z</updated>
    
    <content type="html"><![CDATA[<h2 id="作者：Shreya-Shankar-原文连接：rethinking-ml-monitoring-4"><a href="#作者：Shreya-Shankar-原文连接：rethinking-ml-monitoring-4" class="headerlink" title=" 作者：Shreya Shankar- 原文连接：rethinking ml monitoring 4"></a> 作者：Shreya Shankar<br>- 原文连接：<a class="link"   href="https://www.shreya-shankar.com/rethinking-ml-monitoring-4/" >rethinking ml monitoring 4<i class="fas fa-external-link-alt"></i></a></h2><ul><li>在过去的几篇文章中，我讨论了流式ML评估，思考了要监控的内容（跨状态和组件轴），并探索了现有软件监控工具（如Prometheus）的故障模式。在这最后一篇文章中，<strong>我为ML监控中的问题提出了一个更广泛的研究议程</strong>，由 “真实世界 “的ML部署后问题所激发。</li></ul><h2 id="预备工作"><a href="#预备工作" class="headerlink" title="预备工作"></a>预备工作</h2><ul><li>我们将用一个例子来阐述这个议程：</li><li><strong>任务</strong>：对于乘坐出租车，我们想预测出租车乘客给司机的小费超过车费10%的概率。这是一个二元分类问题。预测值是在0和1之间的浮点数。</li><li><strong>数据集</strong>：我们使用从纽约市出租车联盟收集的2020年1月1日至5月31日的数据。这是一个 “时间上演变的表格数据 “的例子（这句话是从阿伦-库马尔那里偷来的）。</li><li><strong>SLI</strong>：我们衡量准确度，或者当四舍五入到最接近的整数时，正确预测的例子的分数。在ML社区，SLI通常被称为评估指标。</li><li><strong>管道结构</strong>：我们的例子只包括一个模型。2有两条管道–代表训练和推理–共享一些组件，如清洗和特征生成。请参考本系列的第三篇文章中的图表。</li></ul><h2 id="挑战的保护伞"><a href="#挑战的保护伞" class="headerlink" title="挑战的保护伞"></a>挑战的保护伞</h2><ul><li>我的研究议程主要集中在数据管理，而不是算法。</li></ul><h3 id="度量衡计算"><a href="#度量衡计算" class="headerlink" title="度量衡计算"></a>度量衡计算</h3><ul><li>我将指标计算分为粗粒度和细粒度两类。3 粗粒度指标是SLI（如准确率、精确率、召回率），与商业价值最接近，需要一些反馈，或标签。我们使用粗粒度的指标来检测ML性能问题。细粒度指标是可能表明或解释粗粒度指标变化的指标，不一定需要标签（例如，一个特征的两个连续滑动窗口之间的KL分歧）。我们使用细粒度的指标来诊断ML的性能问题。相关的、<strong>我认为许多组织都落入了这样的陷阱，即首先监测细粒度的指标、</strong> 仿佛一个不起眼的特征的中位值的一些变化会对是否重新训练一个模型提供任何可操作的洞察力。<strong>当粗粒度的度量应该被视为一等公民时。</strong></li></ul><h4 id="粗粒度的监控：检测ML性能问题"><a href="#粗粒度的监控：检测ML性能问题" class="headerlink" title="粗粒度的监控：检测ML性能问题"></a>粗粒度的监控：检测ML性能问题</h4><ul><li>令人惊讶的是，各组织发现要了解其SLI的实时价值具有挑战性。一些原因是：<ul><li><p><strong>预测和反馈组件之间的分离</strong>。如果一条流水线进行预测，而另一条流水线摄取反馈，我们就需要对一个高权重的属性进行连接。这可能不是一个研究问题本身，但却是一个需要考虑的恼人的工程问题，特别是在流媒体环境中。</p></li><li><p><strong>改变感兴趣的子群体</strong>。许多组织监测不同子群体（如客户）的SLI 随着时间的推移，新的子群体或人口统计学可能逐渐进入数据流。组织很难知道要监测哪些子人群–考虑到覆盖面（即支持）、时间变化或高培训损失。</p></li><li><p><strong>标签滞后</strong>。由于预测和反馈部分的分离，在我们做出预测后，其相应的反馈（即标签）在一段时间后（或根本没有）到达系统中。不同的预测或子组之间的延迟可能并不一致。在需要人类手动标注数据的情况下，延迟会加剧。此外，我们假设<strong>标签滞后是一个非平稳的时间序列</strong>（即未预料到的问题可能导致滞后，而且可能没有一个模式）。</p></li></ul></li></ul><h5 id="Scalable-Monitoring-Infrastructure"><a href="#Scalable-Monitoring-Infrastructure" class="headerlink" title="Scalable Monitoring Infrastructure"></a>Scalable Monitoring Infrastructure</h5><ul><li><p>解决前两个问题，特别是在规模上，需要更好的监控基础设施，优先考虑增量维护的连接，灵活定义SLI（即用户定义的功能），智能建议监控的内容，以及在部署后添加新的SLI定义并通过历史数据窗口计算的能力。我的上一篇文章展示了Prometheus如何不满足这些需求。为此，我正在考虑一个具有以下层次的ML监控系统：</p></li><li><p><strong>存储</strong>。我们需要一个时间序列数据库用于持久性存储（计算的度量值、直方图或总结、原始输出和反馈），以及用于快速连接和度量计算的内存流。</p></li><li><p><strong>执行</strong>。用户应该能够将度量函数指定为Python UDFs，我们可以用一个基于数据流的差分系统（Murray等人）在数据窗口上增量运行。考虑一个假想的用户工作流：</p></li><li><p><img src="/img/wuweiwu/ml/model_monitor_translate/code3.png"></p></li><li><p>在上面的工作流程中，用户定义他们自己的度量函数，并用记录函数来记录他们的代码。我们将需要加入输出和反馈–增量的，以节省时间–并在任意的窗口大小上计算度量。一个初步的原型4计算流式ML SLI，定义为Python UDFs，跨越不同的组件，显示出有希望的度量计算时间和最小的日志开销：</p></li><li><p><img src="/img/wuweiwu/ml/model_monitor_translate/querylatencyall.png"></p></li><li><p><img src="/img/wuweiwu/ml/model_monitor_translate/loggingtimeall.png"></p></li><li><p>图1：ML查询和记录的延迟。</p></li><li><p><strong>查询</strong>。支持快速和灵活的查询是最重要的。在用户查询之前预先计算和存储摘要–包括连接和度量值–会产生最低的延迟查询；然而，用户可能希望在查询时改变窗口大小和其他参数。在查询时间之前进行预计算和在飞行中计算所有内容之间的最佳权衡是什么？此外，随着用户添加新的UDF和新的子群的出现，我们如何有效地回填部署以来所有窗口的度量值？</p></li></ul><h5 id="估计有标签滞后的实时SLI"><a href="#估计有标签滞后的实时SLI" class="headerlink" title="估计有标签滞后的实时SLI"></a>估计有标签滞后的实时SLI</h5><ul><li>我们不仅需要考虑监测基础设施，而且还需要能够正确计算SLI。滞后为计算SLI引入了有趣的算法挑战。如果用户不能及时收到所有预测的标签（反馈），我们如何尽可能正确地估计实时SLI？</li></ul><h6 id="全面反馈"><a href="#全面反馈" class="headerlink" title="全面反馈"></a>全面反馈</h6><ul><li>在这种情况下，为了计算一个窗口的SLI，我们只需要执行一个流式连接。挑战发生在规模上，或者当我们的窗口尺寸太大，无法在内存中容纳预测和反馈时。一个自然的解决方案可能是执行近似的流式连接，但是众所周知，在连接之前对流式进行均匀的子采样，可以在结果中产生四倍的少的图元（Chaudhuri等人）。现有的流式连接的近似查询处理（AQP）技术在图元的数量或所产生的连接的代表性之间权衡准确性。在我们的案例中，我们关心的是后者，因为我们想使我们的SLI近似的误差最小化（即近似的精度应该接近于精确的精度）。所以我们可能不想利用最先进的宇宙抽样技术（Kandula等人）来保留大量的连接结果图元，因为它们不一定能提供准确的估计（Huang等人）。</li><li>为了优先考虑我们连接样本的代表性，我们可以从渐进式近似连接的分层抽样技术中得到启发（Tok等人）。直观地说，为了使我们的SLI近似值的误差最小化，我们应该构建具有相似预测误差（即ML模型目标认为的损失）的阶层或子组。不幸的是，在我们的高数据设置中，我们无法计算每个预测的误差（因为我们无法将每个预测连接到其相应的标签上）!也许我们可以在高层特征分组中识别 “典范”（即 “重要 “的数据点），并在分组标签上加入典范。例如，在我们的高尖预测问题中，我们可以将我们的预测和标签按邻里（例如FiDi、Tribeca、Midtown）分组，在这些组中挑选典范，汇总这些典范的预测和标签，并将它们连接起来以计算指标。研究的挑战在于设计出高效、高准确度的方法–可能是混合的ML和数据处理技术–来计算群体和典范。</li></ul><h6 id="无反馈"><a href="#无反馈" class="headerlink" title="无反馈"></a>无反馈</h6><ul><li><p>这种情况通常在部署后立即发生。在我们纽约市出租车的例子中，假设我们有两个原始数据来源：出租车传感器遥测数据（如里程、位置）和计价器数据（如支付信息）。计价器数据可能会在以后分批出现，促使我们找到其他方法来估计没有标签的实时性能。</p></li><li><p>一个想法是使用重要性加权（IW）技术（Sugiyama等人）。在高层次上，我们可以根据输入特征确定子组，算出每个子组的训练SLI（例如准确度），并根据实时（部署后，未标记）数据中每个子组的点的数量来加权这些准确度。在我们的例子中，基本的子组定义可以是街区–对于纽约市的每一个街区，我们会找到训练的准确性，并通过相应街区中的实时点的比例来加权，以获得其 “估计 “的准确性。然后，我们将每个社区的估计值汇总，得到一个整体的估计精度。对于一个更复杂（和更高置信度）的方法，我们可以构建不同的分组，并对所产生的SLI估计值进行平均。同样，研究的挑战在于确定这些分组，当然也包括评估这些方法的效果。</p></li><li><p>我们可以在训练集或直播流中确定子群。最便宜的选择是根据训练数据点来计算子群，因为这可以做一次。然而，有了这个 “静态子群 “选项，实时的SLI估计就变得不那么准确了，因为子群的代表性随时间而变化。因此，我们希望在实时数据中计算出适应性的子群。我们可以利用流式聚类算法，这些算法对不断变化的数据分布具有明确的鲁棒性（Zubaroğlu等人）；然而，这在我们的案例中是额外昂贵的，因为每次有新的实时数据进来，我们都需要重复地将训练数据集的点重新分配给聚类。此外，集群在训练集中可能很少或没有相应的数据点，使我们无法估计出SLI。因此，我们需要<strong>研究在高维的、不断变化的数据流中有效识别子群的方法，并考虑到参考数据集（即训练集）</strong>。</p></li></ul><h6 id="部分反馈"><a href="#部分反馈" class="headerlink" title="部分反馈"></a>部分反馈</h6><ul><li>这种情况是我在 “野外 “看到的ML管线中最常见的。通常情况下，实时数据只在时间表上标注，更多的时候，一些上游的数据收集问题影响了反馈的到来（例如，在Tribeca的某个地区有一个手机塔的中断，导致支付表的数据比预期的晚到）。<strong>假设标签滞后的分布是未知和非平稳的</strong>, (即训练一个单独的模型来预测哪些预测不会有反馈可能是不可行的），在这种情况下，我们如何估计实时SLI？</li><li>乍一看，也许我们可以简单地汇总全反馈和无反馈的估计值，按每个子组的数据点的数量加权。但现实情况是，标签滞后很少在各桶中均匀分布，识别具有类似反馈滞后时间的数据点组，对产生实时SLI的准确估计至关重要6。我们可以利用无反馈部分所描述的流式聚类算法，但这种聚类可能无法解释，或者只用谓词中的几个条款简单描述（Saisubramanian等人）。出于调试的目的，我们还关心这些 “滞后 “的数据点集群如何随时间变化，或者滞后的异常情况。</li><li>也许我们可以从流模式挖掘算法中获得灵感，比如频繁项集（Rajaraman和Ullman等人）。然而，这类算法是在标签不变的数据点窗口中寻找特征组，而我们想要应用频繁项集算法的数据点–那些没有反馈的数据点（即 “滞后 “点）–在我们计算出频繁项集后可能会得到反馈。因此，<strong>我们如何扩展流式频繁项集算法，以便在删除数据点后有效地重新计算项集</strong>？我们可以在频繁项集的增量维护工作的基础上（Tobji等人）。</li></ul><h4 id="细致的监控：诊断ML性能问题"><a href="#细致的监控：诊断ML性能问题" class="headerlink" title="细致的监控：诊断ML性能问题"></a>细致的监控：诊断ML性能问题</h4><ul><li>当SLI较低时，最优先考虑的是尽快使其回升。”细粒度 “监测类别涉及表现不佳的管道的 “根本原因分析”–模型是否应该重新训练，或者管道中的工程问题是否是失败的根本原因。此外，如果模型应该被重新训练（例如，有 “漂移 “或 “转移”），我们应该如何改变训练集以提高性能？</li></ul><h5 id="检测数据质量问题"><a href="#检测数据质量问题" class="headerlink" title="检测数据质量问题"></a>检测数据质量问题</h5><ul><li>针对ML可观察性的数据管理研究在自动识别数据管道中的工程问题方面取得了进展（Schelter等人，Breck等人）。诸如模式验证、检测批次内的异常值以及对特征统计的约束（如预期平均值、完整性和范围）等技术可以标示出意外的数据质量问题，如传感器损坏和不完整的上游数据摄取。对于少量的特征和对问题领域的高度熟悉，宣布界限或预期可能是可行的，但这能否扩展到高维设置–例如，当数据科学家向xgboost模型投掷2000或更多的特征时？此外，<strong>在 “工程问题 “和 “漂移 “之间划清界限是很难的</strong>、特别是如果我们想自动检测问题。像TFX（Modi等人）这样的工具允许用户监测感兴趣的距离指标，如KL发散和Kolmogorov-Smirnov测试统计，但在视觉检查的L1距离很低的情况下，这些工具会失效–这可能发生在成千上万的数据点的规模上（Breck等人）。缓解这个问题的策略是假设拥有一个FAANG公司可能正在处理的数据点的数量（如果不是数十亿，也是数亿）。如果在FAANG公司之外，我们可以用什么技术来弥补经验漂移d(p̂, q̂)和理论漂移d(p, q)之间的差距，其中p和q是两个不同的分布？</li></ul><h5 id="朝着重新训练模型的方向发展"><a href="#朝着重新训练模型的方向发展" class="headerlink" title="朝着重新训练模型的方向发展"></a>朝着重新训练模型的方向发展</h5><ul><li><p>在研究和实践界都存在一个巨大的问题，那就是 <strong>“分配转变 “是一个定义不明确的、负担过重的短语</strong>、造成全面的混乱。当人们说 “分布转移 “时，他们指的是一种现象，即一个数据集来自与另一个数据集不同的分布。”分布转移 “可能会导致ML性能下降–例如，在一个出租车公司供应商的数据上训练的模型在取自另一个出租车公司供应商的数据上可能表现不佳。这个重载的短语包含了不同类型的转变；例如：</p></li><li><p><strong>概念转变</strong>：输入特征和目标输出（即标签）之间关系的变化。这方面的一个例子是华尔街的年终奖金导致乘客在一周内多给小费。</p></li><li><p><strong>协变量转移</strong>：训练数据中输入变量分布的变化，而不是目标输出分布。这方面的一个例子是在新年前夕在中城（时代广场的落球）收到更多的出租车乘坐。</p></li><li><p><strong>年龄转移</strong>：一个输入变量的分布随着时间的推移预期增加或减少。这方面的一个例子是出租车的总里程数，它只能随着时间的推移而增加。</p></li><li><p>在 “分布转变 “方面的很多研究和现有方法都集中在比较两组有限的数据。正如我在本系列文章前面提到的，在实践中，我们关心的是在无限的数据流上部署模型，或在可预见的未来的数据。难道我们应该任意地将生产数据切割成两个固定大小的数据集，以想知道是否有 “分布转移”？这似乎并不正确。在流式ML设置中，我们真正关心的问题是：在什么时候我的模型对我当前的数据不能像预期那样工作？(即我何时需要重新训练我的模型？)。</p></li><li><p>在现实中，SLI的下降是由不同类型的转变组合而成的，特别是在高度非平稳的环境中。从产品的角度考虑，告诉用户 “78%来自概念转变，22%来自协变量转变”，即使我们能精确地确定这种细分，其可操作性如何？<strong>我们希望告诉用户何时以及如何在部署后重新训练模型</strong>,鉴于当前数据窗口中出现的 “异常”。假设我们有标签或反馈Y和输入数据或特征X，其中Xi代表所有数据点中第i个特征的数据。按照颗粒度增加的顺序，异常情况的类型可以包括：：</p></li><li><p>P(Y | X)有变化，但P(X)没有变化</p></li><li><p>P(Y | X)没有变化，但P(X)有变化</p></li><li><p>P(Xi)中的移位</p></li><li><p>P(Xi | Xj)中的移位，其中i ≠ j</p></li><li><p>P(Xi | Xj , Xk , …)中的移位，其中i ≠ j ≠ k</p></li><li><p><strong>在规模上跟踪许多距离度量</strong>。如前所述，为了接近上述的异常情况，现有的工作提出在连续的滑动窗口和训练集上跟踪KL分歧和Kolmogorov-Smirnov检验等指标（即Breck等人所述的训练-服务倾斜）。在内存中保留许多实时数据的窗口和训练集可能是不可行的，因此我们可以利用AQP技术来计算具有合理误差的距离指标。例如，度量计算功能可以在特征的直方图上运行，而不是在完整的数据流上运行；然而，直方图槽在流式设置中需要随着数据的演变而变化。研究的挑战在于将增量维护的近似直方图（Gibbons等人）的想法与自适应直方图（Leow等人）的想法相结合，以产生不断变化的数据窗口的总结。</p></li><li><p><strong>识别距离度量中的静止性</strong>。通常情况下，模型在周末、工作时间以外或节假日表现不佳。一个有趣的想法是训练一个模型来识别滑动窗口中观察到的P（Y | X）和P（X）之间的距离是否存在季节性模式。追踪所有的特征组合是不切实际的（Heise等人），那么是否有可能利用预测寻找算法（如Wu等人）来简化追踪距离指标的空间？</p></li><li><p><strong>自我调整的训练集</strong>。最后，根据检测到的异常类型，我们可以建议增加或改变训练集的方法。例如，在P(X)发生变化但P(Y | X)不变的情况下，我们可以建议对代表性不足的子群体进行增加采样。在P(Y | X)变化但P(X)不变的情况下，也许用户可以在最近的数据窗口上重新训练他们的模型。研究的挑战在于具体的、有用的提示，以构建新的训练数据集来避免低性能的陷阱。</p></li></ul><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><ul><li><p><strong>目前的ML监控仪表盘信息量过大</strong>.用户看到数以百计，甚至数以千计的柱状图和图表，试图将 “分布转移 “可视化。<strong>这些图表没有可操作性，特别是当人们可以用同一组图表来讲述两个相互矛盾的故事时。</strong>例如，一个用户可以说，一个模型需要重新训练，因为一个特征的平均值在过去的几天里急剧下降了。另一方面，用户可以说这个特征不是三个最重要的特征之一，所以重新训练模型不会有很大影响。因此，从界面的角度来看，在给定数百个图形的情况下，在何时重新训练一个模型上划线是一个任意的过程。我们如何想出更好的可视化工具？</p></li><li><p><strong>仪表盘的目标</strong>。监测可视化的目的是帮助用户与他们的数据和模型保持同步。为此，好的监测仪表板应该毫不含糊地回答具体问题，如：：</p><ul><li>实时ML的性能是什么？</li><li>这个性能是比预期的高还是低？比要求的（即满足SLOs）？</li><li>性能降低的原因是数据质量问题吗？</li><li>模型应该被重新训练吗？</li></ul></li><li><p>仪表盘应该只包含少数几个描述 “统治它们的单一指标”（即SLI）的图，这样用户就不会被淹没了。</p></li><li><p><strong>仪表盘的挑战</strong>。在两个以上的维度上实现指标的可视化是非常困难的。不幸的是，在涉及到ML指标时，我们至少有以下几个维度：</p><ul><li>度量值</li><li>组件（即管道中跨组件的连接，见本系列第二篇文章中的单组件与跨组件的讨论）。</li><li>状态（即输入和输出的历史值）</li><li>子种群（包括特征群）</li><li>时间（一般意义上的时间，例如，绘制过去6个月内100天窗口的滚动平均值）</li></ul></li><li><p>我们如何开发出能明确传达所有这些维度的信息而又没有太多认知开销的可视化产品？</p></li><li><p><strong>争取实现有洞察力的可视化</strong>。与前文所述的细粒度监测信息相结合，一个好的仪表盘将有深刻的可视化，让用户对分布如何变化有直观感受。ML工程师使用的现有的 “最先进的 “可视化（通过口口相传确定，所以要慎重对待8）包括比较两个数据集的静态条形图直方图。这在流式ML环境中很难推理。什么新的可视化类型可以解释细粒度指标的变化？这里有一个高尖预测的 “动态 “小提琴图的例子，显示了输出的分布如何随时间变化：</p></li><li><p><img src="/img/wuweiwu/ml/model_monitor_translate/output_violin.gif"></p></li><li><p>图2：产出随时间的分布。</p></li><li><p>在上面的例子中，由于我们有充分的反馈（即所有的预测都有标签），SLI（准确性）有确认的下降。在我们需要近似SLI和假设推理细粒度指标的情况下，也许用户可以直观地看到这样一个可视化的季节性。这绝对不是万能的解决方案，但要回到主要的问题：<strong>研究的挑战在于如何以有原则的方式提出更好的可视化来理解数据漂移，并根据用户工作的数据和ML任务自动将其呈现给用户。</strong></p></li></ul><h3 id="数据集和基准"><a href="#数据集和基准" class="headerlink" title="数据集和基准"></a>数据集和基准</h3><ul><li>由于缺乏对 “真实世界 “流式ML任务的访问，许多研究人员和开发人员主要用玩具数据或合成分布转移来工作。为此，有几个问题围绕着数据集和基准来加速ML监测的进展：</li></ul><ol><li><strong>一个实时数据流的存储库，对应于可操作的ML任务。</strong>一个好的数据流的属性包括：它是无限的，代表了一个真实世界的现象，并且有一个比预测天气或股票价格更容易解决的ML任务。一个影响特别大的问题可能是以太坊天然气价格预测。具体来说，一个模型能否以95%的置信度输出未来一小时内一笔交易所需的最低天然气价格？另一个选择是将现有的基准转换为流格式（例如，从WILDS的训练和测试分布（D和D’）中取样点xt，用一个函数，当时间戳t小时，xt∈D的概率很高，当t大时，xt∈D’的概率很高）。理想情况下，我们收集更多的 “时间演化的表格数据流”，因为这种类型的数据在研究界大量存在。</li><li><strong>以直观的方式从资源库中查询数据的接口</strong>,大多数数据科学家并不使用流式系统，那么我们如何让他们进行范围查询并接收Pandas或PyTorch数据集？我们如何保障用户在试验新想法时不被标签泄露或意外地偷看到未来？</li><li><strong>用于创建和评估训练模型的策略的接口</strong>（我敢说是DSL？)mltrace的愿景是成为一个类似于React的库，用户在其中定义管道的组件，并在每个组件运行之前和之后运行触发器。在这些触发器中，用户可以根据自己的标准决定重新训练模型–比如数据漂移指标或预定更新。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>有了关于一个ML任务的足够的上下文，就有可能解决该任务特有的数据管理问题。但在构建一个通用的监控工具时，还有其他一些挑战，这些挑战源于ML管道和系统复杂性的增加，包括：</li><li><strong>壮大的数据科学团队和工具栈</strong>。软件工程表明，团队和工具栈的分散性使其很难保持系统的可持续性。这将适用于ML，特别是当可能的ML数据管理工具的数量每年大量增加时。调试一个别人训练的模型是一件很痛苦的事情。</li><li><strong>模型堆叠</strong>。许多组织将模型连锁在一起以产生最终的预测。漂移检测对于一个模型来说已经很困难了。将错误的预测追溯到需要调试的特定模型上，似乎更具挑战性。</li><li><strong>无法解释的特征</strong>。许多组织使用ML来产生嵌入，并将其作为特征输入到下游的ML任务中。数据质量警报，如用户定义的特征列的约束，然后无法构建。</li><li><strong>将组件作为容器化应用进行部署</strong>。在Kubernetes集群中很难做ML。容器化基础设施主要适用于无状态的应用程序，不幸的是，在线和持续学习是有状态的（即模型权重被更新，需要在预测服务荚之间共享）。</li><li><strong>多模态数据</strong>。我在这篇文章中概述的许多解决方案想法都是针对表格数据的。我们可以在图像、音频和视频案例中使用什么技术？我敢说，信息的 “数据湖”？</li><li>我还没有太深入地思考这些临时性的挑战，但我怀疑一个好的ML监测工具至少会意识到这些挑战。最后，我想以个人名义结束这个系列。10我很感激有时间批判性地思考ML监控，以及行业专家和学术合作者的支持。我很幸运能读博士，我为自己能写出的论文感到兴奋!</li><li>感谢Divyahans Gupta和我的导师Aditya Parameswaran的头脑风暴帮助和对许多草案的反馈。</li></ul><hr><h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><ol><li>通常情况下，我谈论的是可观察性。监测是可观察性中的一个子问题，有最有趣的研究问题（在我看来）。</li><li>新的挑战出现在模型的堆叠上，或者将模型串联在一起，形成最终的预测结果。</li><li>不清楚 “粗粒度 “和 “细粒度 “是否是这里的最佳术语。DG建议采用 “外部 “与 “内部 “的衡量标准。如果您对此有任何想法，请告诉我！</li><li>基于 timely-dataflow（基于Rust的差分数据流实现）。向Peter Schafhalter致敬，感谢他在这方面的快速工作。</li><li>我应该进一步阐述这些方法。</li><li>更不用说，确定哪些子组有较高的滞后时间，对调试工作有帮助。</li><li>这里不点名，但如果你有兴趣，请查看ML监控公司的演示。</li><li>我们RISELab的几个人正在进行一项采访研究，以正式撰写部署后ML维护的 “最佳实践”。</li><li>深度学习的危险性就属于这个范畴。</li><li>向那些能看完本系列所有4篇文章的人表示敬意。我是认真的。我很确定我还没有读完这里的所有字。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;作者：Shreya-Shankar-原文连接：rethinking-ml-monitoring-4&quot;&gt;&lt;a href=&quot;#作者：Shreya-Shankar-原文连接：rethinking-ml-monitoring-4&quot; class=&quot;headerlink&quot; t</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MLOps" scheme="http://example.com/tags/MLOps/"/>
    
  </entry>
  
  <entry>
    <title>现代机器学习监控的混乱局面：重新思考流媒体评估（3/4）</title>
    <link href="http://example.com/2023/05/24/model_monitoring_mess_part03/"/>
    <id>http://example.com/2023/05/24/model_monitoring_mess_part03/</id>
    <published>2023-05-24T11:12:16.000Z</published>
    <updated>2023-07-16T12:10:50.194Z</updated>
    
    <content type="html"><![CDATA[<ul><li>作者：Shreya Shankar</li><li>原文连接：<a class="link"   href="https://www.shreya-shankar.com/rethinking-ml-monitoring-3/" >rethinking ml monitoring 3<i class="fas fa-external-link-alt"></i></a></li></ul><hr><ul><li>在上一篇文章中，我调查了现有的部署后问题，并将它们分为两个轴：状态和组件。我提到，监控跨组件的状态指标，如模型的准确性，对于维护ML管道至关重要，但在现有的工具中却很困难。在这篇文章中，我们将亲身体验这些困难：我们将用Prometheus（一种流行的软件监控工具）扩展一个玩具ML管道，以提供ML监控。在这个过程中，我们将看到Prometheus的许多不足之处，从代码混乱到算法的低效率</li></ul><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><ul><li>有一天在工作中，当我在Slack频道中浏览AWS警报的时候，我突然意识到我的infra同事和我对 “标签 “一词的解释非常不同。他指的是一个标识符。我是指预测的真实值。我们只是在一个大项目进行了几个月后才发现我们是错位的。值得庆幸的是，我们从未在同一个任务上合作过，所以对我们没有任何影响。但我想知道这种错位是否会在其他组织中造成问题。</li><li>我认为关于ML监测的讨论中最令人困惑的方面是术语。像 “度量 “和 “标签 “这样的术语是超载的。在这篇文章中，我将利用以下定义：<ul><li>指标：一个汇总数据的函数，以评估管道的性能如何（例如，平均值、准确度）。</li><li>普罗米修斯公制：数字测量的时间序列</li><li>标识符：分配给一个对象或对象集合的唯一名称</li><li>预测：由一个ML模型做出的输出</li><li>反馈：预测的 “真实 “值</li><li>服务水平指标（SLI）：一个汇总预测和反馈的函数，以评估管道的性能如何（例如，准确性）</li></ul></li></ul><h2 id="ML任务，数据源，和管道"><a href="#ML任务，数据源，和管道" class="headerlink" title="ML任务，数据源，和管道"></a>ML任务，数据源，和管道</h2><ul><li>在这个练习中，我使用纽约市出租车联盟的数据构建了一个ML管道来预测出租车乘客是否会给司机高额小费（二元分类）。使用Prometheus，我们将监测<strong>累积的准确性</strong>，或者说自部署以来所有预测的准确性。训练和推理管道在架构上共享一些组件，看起来像这样：</li><li><img src="/img/wuweiwu/ml/model_monitor_translate/pipeline.svg" alt="ml pipeline"></li><li>图1：ML管道架构。</li><li>由于这是一个二元分类问题，推理组件产生0和1之间的浮动值预测，而反馈组件返回0或1的值。我把我的管道和实验代码寄存在这个资源库中，结构如下：</li><li><img src="/img/wuweiwu/ml/model_monitor_translate/repo_structure.png"></li><li>我在2020年1月的数据上运行训练管道，模拟2020年2月1日开始的部署。我没有进行任何再训练，主要是因为这个练习的目的是展示Prometheus的故障模式，而不是调试由 “数据漂移 “导致的低SLI。</li></ul><h2 id="Prometheus入门"><a href="#Prometheus入门" class="headerlink" title="Prometheus入门"></a>Prometheus入门</h2><ul><li><p>普罗米修斯(Prometheus)是一个开源的软件监控工具：</p><ol><li>收集和存储指标，如响应延迟</li><li>允许用户通过一种名为PromQL的查询语言查询公制值的集合（例如平均延迟）。</li></ol></li><li><p>普罗米修斯支持四种类型的度量值：</p><ul><li><strong>计数器</strong>：一个单调增长的累积度量。例如，可用于跟踪预测服务的数量。</li><li><strong>仪表</strong>：一个代表单一数值的度量，可以任意改变。例如，可以用来跟踪当前的内存使用情况。</li><li><strong>直方图</strong>：一个将观察到的数值归类到用户预先定义的桶中的度量衡。这有很高的服务器端成本，因为服务器在查询时要计算量值。</li><li><strong>摘要</strong>：在一个滑动的时间窗口中跟踪用户预先定义的量化指标。这具有较低的服务器端成本，因为量值是在登录时配置和跟踪的。另外，摘要指标一般不支持查询中的聚合。</li></ul></li><li><p>通常情况下，企业的DevOps或SRE人员使用Prometheus来监控软件SLO。用户用他们的应用程序代码来记录Metric值。这些值被刮取并存储在Prometheus服务器中。这些值可以使用PromQL进行查询，并导出到Grafana等可视化工具中。该架构看起来像这样：</p></li><li><p><img src="/img/wuweiwu/ml/model_monitor_translate/promarchitecture.png" alt="prom architecture"></p></li><li><p>图2：普罗米修斯架构。</p></li><li><p>Ivan Velichko的这一系列教育性的帖子很好地解释了普罗米修斯。我将总结他的一些关键点：</p><ul><li>普罗米修斯不是一个时间序列数据库（TSDB）。它只是利用了一个TSDB。</li><li>由于普罗米修斯定期刮取数值，如果度量值的变化比刮取间隔更频繁，一些度量类型（如仪表）会失去精度。这个问题不适用于单调增长的度量（如计数器）。</li><li>Metrics可以用任意的标识符来记录，这样在查询时，用户可以通过标识符的值来过滤Metrics。</li><li>PromQL很灵活–用户可以在不同的窗口大小上计算许多不同的度量值聚集，这些参数可以在查询时指定。</li></ul></li><li><p>Velichko承认，PromQL在实际软件应用中的使用 “远非微不足道”（也就是说，令人讨厌）。然而，在学习了矢量匹配和其他语法之后，我认为它并不太糟糕–特别是当我们不需要加入Metrics的时候。PromQL查询通常不会太长，而且在查询时有许多辅助函数可以使用。但我们会看到它对ML监控的情况有多糟糕。</p></li></ul><h2 id="Prometheus-🤝-ML"><a href="#Prometheus-🤝-ML" class="headerlink" title="Prometheus 🤝 ML"></a>Prometheus 🤝 ML</h2><ul><li>以下问题将有助于评估Prometheus是否是一个合适的ML监控解决方案：<ol><li>我们可以使用Prometheus Metrics来跟踪我们的ML管道中的任何ML指标吗？将 “ML度量 “映射到Prometheus Metric类型并不直接。对于单一组件的有状态指标，也许我们想使用直方图或汇总指标。如果我们对跨组件的有状态度量感兴趣，我们需要考虑如何将不同组件的度量 “连接 “起来，以计算ML SLI，如准确度和精确度。</li><li>用PromQL编写ML SLI有多难？</li><li>ML SLI的查询延时是多少？</li></ol></li></ul><h2 id="管线仪表"><a href="#管线仪表" class="headerlink" title="管线仪表"></a>管线仪表</h2><h3 id="跨组件的有状态度量"><a href="#跨组件的有状态度量" class="headerlink" title="跨组件的有状态度量"></a>跨组件的有状态度量</h3><ul><li>普罗米修斯的指标类型（计数器、测量仪、直方图或摘要）没有一个明显地映射到我们想要测量的SLI：累积精度。相反，我们将使用2个Gauge Metric3–一个用于管道预测，一个用于反馈–并在PromQL中聚合它们来计算准确性。在mext&#x2F;prometheus_ml_ext.py中，我定义了一个BinaryClassificationMetric类，其中包含Gauge Metrics以及logOutputs和logFeedbacks方法，以便在每次推理调用后更新它们。对应用程序进行仪表化是非常直接的。下面是inference&#x2F;main.py中的Prometheus专用代码：</li><li><img src="/img/wuweiwu/ml/model_monitor_translate/code1.png" alt="inference code"></li><li>将累积的准确度表现为两个Gauge Metrics并不完全是直接的，但我仍将这种经验评为简单。</li></ul><h3 id="单一成分的有状态度量"><a href="#单一成分的有状态度量" class="headerlink" title="单一成分的有状态度量"></a>单一成分的有状态度量</h3><ul><li>ML监控解决方案经常监控输入和输出的聚集，如中位数和p90，以粗略地测量 “数据漂移”。有时，他们还计算更复杂的统计测试（如Kolmogorov-Smirnov测试），我肯定永远无法用PromQL来写。这些方法既不健全也不完整，但为了这个练习的目的，我们可以用直方图度量来跟踪输出值的各种百分位数。下面是inference&#x2F;main.py中的相关仪表代码：</li><li><img src="/img/wuweiwu/ml/model_monitor_translate/code2.png"></li><li>这种整合比跨组件的情况更容易，但一个主要的缺点是，我们需要提前定义我们的直方图桶。这有两个原因：（1）我们常常不知道输出的分布是什么样子的，（2）分布可能随着数据的 “漂移 “而改变。</li></ul><h2 id="用于ML-SLI的PromQL"><a href="#用于ML-SLI的PromQL" class="headerlink" title="用于ML SLI的PromQL"></a>用于ML SLI的PromQL</h2><ul><li><p>现在我们已经检测了我们的管道，我们可以通过Docker-Compose启动我们的容器，开始使用PromQL刮取记录的Metric值并提取我们的ML指标。使用PromQL结构和一个很大的白板，我想出了以下查询：</p></li><li><p><img src="/img/wuweiwu/ml/model_monitor_translate/cmp.png"></p></li><li><p>在ML SLI中使用PromQL有几个问题：</p></li><li><p><strong>不正确性</strong>。当我第一次运行查询的准确性时，我很惊讶，结果并不完全准确（哈哈）。这是因为我的搜刮间隔是15秒，这对于我产生新预测的速度来说太大了。把搜刮间隔缩短到5秒，提高了查询的精确性，但使Prometheus容器的速度变慢，消耗更多的内存和计算资源。</p></li><li><p><strong>滑动窗口的挑战</strong>。即使在几个小时后，我也无法弄清楚如何在固定的窗口大小上计算前3个指标中的任何一个（交叉成分）。我没有发现关于在滑动窗口上计算PromQL中的连接的资源。我不太擅长使用Prometheus，所以请让我知道是否有可能通过窗口计算这些指标。</p></li><li><p><strong>复杂的查询</strong>。表中的最后3个指标（单成分）并不像前3个（跨成分）那样错综复杂。我不会期望任何数据科学家编写这些跨组件的PromQL查询，特别是对于那些简单地调用scikit-learn模块的函数。一个理想的监测工具应该允许用户将自定义的Python函数作为指标传入，并在后端有效地产生这些指标的值。</p></li></ul><h2 id="查询延时"><a href="#查询延时" class="headerlink" title="查询延时"></a>查询延时</h2><ul><li>在本小节中，我将重点讨论延迟问题，特别是跨组件查询的延迟。为了计算类似SLI的准确性，如上一小节所示，我们需要对output_id做一个连接。这是对Prometheus的严重滥用，因为output_id的cardinality显然会随着ML管道的预测数量而增长。<strong>Prometheus并不是为了处理高卡数的标识符，更不用说高卡数的连接了。</strong></li><li>为了证明Prometheus的扩展性有多差，我设计了一个小型的Postgres后端，将预测和反馈放在以时间戳为索引的表中。我计算了PromQL和PostgreSQL的准确度，并测量了与管道产生的预测数量有关的延时：</li><li><img src="/img/wuweiwu/ml/model_monitor_translate/querylatency.png"></li><li>图3：ML查询的延迟。</li><li>由于Prometheus度量值不是急于计算的（也就是说，当用户想在Grafana上查询或绘制一段时间的度量值时，它们都会被计算出来）、<strong>这种延迟是不可接受的，而且不能扩展。</strong> 随着更多预测的产生，许多想要跟踪实时ML SLI的组织可能无法足够快地更新或刷新他们的SLI。也许在某些领域，每天甚至每小时计算一次SLI可能就足够了，但对于数据和用户偏好经常变化的领域来说，这就不适用了。我知道我在使用Prometheus的情况下，它并不是为之设计的，但总的来说，这些问题共同突出了企业的需要：（1）有一个ML监控团队，在Postgres或现有的DBMS之上创建一个层，或者（2）利用一个专门用于ML监控的专有供应商。我相信，现在、<strong>我们需要更好的ML监测实践和工具。</strong></li></ul><h2 id="回顾总结"><a href="#回顾总结" class="headerlink" title="回顾总结"></a>回顾总结</h2><ul><li><p>在这篇文章中，我强调了使用Prometheus进行ML监控的一些主要隐患，最明显的是：</p></li><li><p>需要使用多个普罗米修斯公制类型进行跨组件监测</p></li><li><p>需要提前定义直方图桶以进行单组件监控</p></li><li><p>查询结果的正确性取决于搜刮时间间隔</p></li><li><p>无法处理滑动窗口</p></li><li><p>看起来很恶心的PromQL查询</p></li><li><p>跨组件指标的高延迟（即高cardinality连接）。</p></li><li><p>在这个系列的下一篇也是最后一篇文章中，我将讨论建立一个通用的ML监控工具的一些关键要求和想法。我非常高兴能与大家分享，同时还有一个监测实时ML SLI的原型。更多的内容将陆续推出，新年快乐!</p></li><li><p>感谢Divyahans Gupta, Preetum Nakkiran, 和Peter Schafhalter对许多草案的反馈。</p></li></ul><hr><h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><ol><li>这个帖子是针对ML工程师和信息员的。我建议对数据库（如表、连接、索引）、ML SLI（如准确性、精确性、召回率）和查询语言（如SQL、PromQL）有基本认识。</li><li>我用大写的Metric来指代Prometheus Metric的抽象概念。</li><li>我为预测和反馈选择了Gauge Metric，因为它们表示可以上升或下降的数值。由于反馈和推理组件在ML管道中通常是相互分离的，所以真的没有办法（我能想到的）避免连接。</li><li>如果这些查询是错误的，我也不会感到惊讶。如果有错误，请纠正我。</li><li>也许这不是一个失败的模式–我只是想不通。如果我错了，请让我知道!</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;作者：Shreya Shankar&lt;/li&gt;
&lt;li&gt;原文连接：&lt;a class=&quot;link&quot;   href=&quot;https://www.shreya-shankar.com/rethinking-ml-monitoring-3/&quot; &gt;rethinking ml </summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MLOps" scheme="http://example.com/tags/MLOps/"/>
    
  </entry>
  
  <entry>
    <title>现代机器学习监控的混乱局面：重新思考流媒体评估（2/4）</title>
    <link href="http://example.com/2023/05/24/model_monitoring_mess_part02/"/>
    <id>http://example.com/2023/05/24/model_monitoring_mess_part02/</id>
    <published>2023-05-24T10:19:16.000Z</published>
    <updated>2023-07-16T12:05:01.795Z</updated>
    
    <content type="html"><![CDATA[<ul><li>作者：Shreya Shankar</li><li>原文连接：<a class="link"   href="https://www.shreya-shankar.com/rethinking-ml-monitoring-2/" >rethinking ml monitoring 2<i class="fas fa-external-link-alt"></i></a></li></ul><hr><ul><li>在《现代ML监控乱象》系列的最后一篇文章中，我讨论了现有流式ML指标失败的一些方式。在这篇文章中，我从 “以数据为中心 “的角度过渡到软件工程的角度：我调查了现有的部署后问题，并将其分类，以激励更好的开源ML监控工具。</li></ul><h2 id="作为ML从业者，我们如何分配我们的时间？"><a href="#作为ML从业者，我们如何分配我们的时间？" class="headerlink" title="作为ML从业者，我们如何分配我们的时间？"></a>作为ML从业者，我们如何分配我们的时间？</h2><ul><li>最近，Zillow因其最新的ML灾难而在社交媒体上引起热议。一些人声称他们的ML购房和售房项目因低质量的预测而关闭。另一些人则认为，关闭的原因是没有关于如何对预测采取行动的组织程序–员工手动推翻了预测，以满足他们自己的配额。谁知道问题是什么，但我认为Zillow的新闻变得非常流行的一个重要原因是<strong>我们所做的许多ML应用在生产中都是不稳定的。</strong> 我们不知道他们是否或何时会失败。我们不知道，在组织上，如何对模型输出采取行动。而且我们肯定不知道当我们怀疑ML管道或管理方面出现一些故障时该怎么做。</li><li>为了剖析如何改善应用ML的令人苦恼的状态，我认为了解ML从业者目前如何分配他们的时间很重要。经过广泛的搜索，我在一篇关于87%的机器学习项目失败的文章中发现了一个相当准确的细分（在我看来），即行业ML项目是如何分配时间的：</li><li><img src="/img/wuweiwu/ml/model_monitor_translate/time-spend.png" alt="time spend"></li><li>图1：分解ML项目时间分配的图表，取自iiot-world。</li><li>图1显然解释了为什么数据标签初创公司目前在MLOps市场上获胜，但我更感兴趣的是，随着越来越少的ML项目失败，细分市场将如何变化。就像在软件方面一样，我怀疑在未来5年内，图表中的大部分大众都会在运营化方面。<strong>我们如何从数据化中去除质量，并将更多的质量放在监测上？</strong></li></ul><h2 id="构建一个ML管道"><a href="#构建一个ML管道" class="headerlink" title="构建一个ML管道"></a>构建一个ML管道</h2><ul><li><p>上图所示的分类对于失败的ML项目来说感觉很准确。在我以前的ML工作的任期结束时，我们最终遵循这个过程来建立新的成功的ML管道：</p><ol><li>想出针对ML的服务水平目标（SLO），比如在每月的窗口中90%的召回率 🪟</li><li>编写为占位的ML管道输出计算SLO的函数（例如，基线预测概率） 🪠</li><li>编写确认校准输出的函数 ⚖️</li><li>编写管道的所有组件或阶段，除了模型训练&#x2F;评估之外 🔧</li><li>进行仔细的探索性数据分析，编写清理数据、选择特征等的管道组件 📊</li><li>将上述所有内容生产化 🚧</li><li>训练&#x2F;评估逻辑回归模型或决策树 🌲</li><li>逐步改善模型 📈</li></ol></li><li><p>这里的关键是，<strong>在管道的一些骨干部分投入生产之前，我们没有做任何建模工作</strong>、消除了花在数据预处理上的时间，这可能是特定的训练&#x2F;测试分割。然而，在我上面描述的范式中，我们在管道中的数据处理组件中仍然有bug（我认为这是不可避免的）。当采用这种模式时，我注意到我的大部分开发人员的时间从 “研发 “建模工作转移到监控生产ML服务。</p></li><li><p>几个月来，我一直记录着我个人在监控时面临的部署后问题。缺少数据导致空值键的连接，子组表示法随时间变化，将错误的模型推广到生产中，上游依赖关系（如嵌入）变化，Spark节点故障……这个清单似乎没有尽头。每当我面临一个新问题时，我都试图设计一个警报，以防止未来出现这种故障模式。<strong>随后，管道代码逐渐变得不可持续，而我在生产中仍然面临着许多新的挑战–促使我思考更清洁的部署后问题的分解。</strong></p></li></ul><h2 id="部署后问题的分类"><a href="#部署后问题的分类" class="headerlink" title="部署后问题的分类"></a>部署后问题的分类</h2><ul><li>假设一个团队为一项任务定义了一个具体的ML SLO，比如在每月的窗口中90%的召回。重新表述我关于部署后问题的问题–这些SLO没有达到的原因有哪些？我喜欢Neptune.ai博客文章中的这个表格：</li></ul><table><thead><tr><th></th><th>生产挑战</th><th>关键问题</th></tr></thead><tbody><tr><td>1</td><td>数据分布变化</td><td>为什么我的特征值会有突然的变化？</td></tr><tr><td>2</td><td>生产中的模型所有权</td><td>谁拥有生产中的模型？DevOps团队？工程师？数据科学家？</td></tr><tr><td>3</td><td>训练-部署不一致</td><td>尽管我们在开发过程中进行了严格的测试和验证尝试，但为什么该模型在生产中给出的结果不佳？</td></tr><tr><td>4</td><td>模型&#x2F;概念漂移</td><td>为什么我的模型在生产中表现良好，而随着时间的推移，性能突然下降？</td></tr><tr><td>5</td><td>黑匣子模型</td><td>我如何根据商业目标并向相关的利益相关者解释和说明我的模型的预测？</td></tr><tr><td>6</td><td>一致的对手</td><td>我怎样才能确保我的模型的安全？我的模型被攻击了吗？</td></tr><tr><td>7</td><td>模型准备情况</td><td>我如何将我的模型的较新版本的结果与正在生产的版本进行比较？</td></tr><tr><td>8</td><td>管道健康问题</td><td>为什么我的训练管道在执行时失败？为什么再培训工作需要这么长时间才能运行？</td></tr><tr><td>9</td><td>表现不佳的系统</td><td>为什么我的预测服务的延迟非常高？为什么我的不同模型的延迟有很大的不同？</td></tr><tr><td>10</td><td>极端事件的案例（异常值）</td><td>我将如何在极端和非计划的情况下跟踪我的模型的效果和性能？</td></tr><tr><td>11</td><td>数据质量问题</td><td>我怎样才能确保生产数据的处理方式与训练数据的处理方式相同？</td></tr></tbody></table><ul><li>图2：部署后问题表，取自Neptune.ai。</li><li>说实话，很多博客文章读起来就像洗衣清单。作为一个懒人，我绝不会在每个拉动请求上手动检查每个清单项目。<strong>我们如何使这些挑战更容易推理，以便我们能够建立一个监测工具？</strong> 戴上我的软件工程帽子，我发现将这些生产挑战（以及其他挑战）提炼成四种类型的问题是很有帮助的，这些问题沿着两个轴线（状态和组件）分组：</li><li><img src="/img/wuweiwu/ml/model_monitor_translate/state-componet.jpg" alt="state and component"></li><li>图3：将少数部署后的问题按状态和组件轴进行分类。</li><li>让我定义一下 “状态 “和 “组件”，以便图3真正有意义。很多时候，只有当你将数据点与历史上的一组数据点进行比较时，你才知道ML系统中存在一个错误。例如，也许代表传感器数据的表格中的一列主要是空值。是传感器刚刚坏了，还是它一直都在坏？为了回答这样的问题，你需要跟踪历史数据，或状态。此外，你可能需要来自多个不同组件的信息来诊断一个ML系统的故障模式。例如，假设模型训练的数据和模型推断预测的实时数据之间存在差异。显然，一个调试器应该关心ML管道的训练和推理部分。我把这种错误定义为跨组件的错误。</li><li>对我来说，理解这些轴线，就可以知道为什么ML监控从根本上说比传统的软件监控更难，也更有区别。软件SLI完全属于单组件区域，如平均响应延迟。最复杂的软件SLI是有状态的和单组件的，促使了像Prometheus这样的工具。<strong>但最基本的ML SLI，如准确度，是有状态的和跨组件的–最难处理的桶。</strong> 一旦你能够有效地监控这种有状态的跨组件指标（如实时准确性），以确定何时有bug，了解有状态的单组件指标，如第90个百分点的特征值，是很有用的，因为你就可以确定bug在你的管道中的位置。</li></ul><h2 id="通过状态-组件视角查看现有的工具"><a href="#通过状态-组件视角查看现有的工具" class="headerlink" title="通过状态-组件视角查看现有的工具"></a>通过状态-组件视角查看现有的工具</h2><ul><li>我个人对一个完全开源的监控解决方案很感兴趣，它可以有效地、快速地、以一种需要用户付出最小努力的方式来跟踪可能属于这4组中任何一组的指标。现有的工具能做到这一点吗？从我的研究中，我发现开源监控工具只涉及4个组的一个子集。我在头脑中把现有的工具分为以下几个 “层次”：<ol><li>缠绕着的报告 scipy.your_favorite_statistical_test(finite_sample_from_dist_A_that_you_define, finite_sample_from_dist_B_that_you_define) —用户被迫跟踪状态和组件。在这里，用户需要解决数据管理问题，即为他们关心的每个组件存储历史输入和输出，编写作业来反复对感兴趣的数据进行子采样并创建这些报告，并在这个过程中组织所有工件。</li><li>工作流协调工具（如Airflow、Argo、Kubeflow）–用户被迫跟踪状态。在这里，仪表盘会向你显示单个管道运行中不同任务或组件的详细结果。但用户无法真正访问当前运行中的历史运行值。</li><li>Postgres数据库持有单个组件的历史输入和输出，以及一个显示np.some_aggregation(dist_A)随时间变化的仪表板–用户被迫跟踪组件。在这里，用户需要知道要监控哪些组件，手动连接组件的输入和输出以进行任何自定义的聚合（例如，F1分数），并定义他们自己的标准，即如何以及何时对仪表盘的结果采取行动。例如，如果一个特征的平均值变化了10%，用户会怎么做？</li><li>专有的供应商，鉴于对你的数据和模型的访问，为你监控一切。不知道这些公司是做什么的，所以我不作评论。</li></ol></li></ul><h2 id="回顾总结"><a href="#回顾总结" class="headerlink" title="回顾总结"></a>回顾总结</h2><ul><li>在这篇文章中，我讨论了如何通过两个简单的轴对部署后问题进行分类：状态和组件。我不想讨论如何监测我在图3中描述的每个问题，但回到我在本期第一篇文章中提出的一个问题–要如何扩展现有工具以支持有状态的组件和跨组件的情况？是否有可能轻松地扩展它们？在下一篇文章中，我将探讨如何调整Prometheus以监测ML，并研究它的缺陷所在。</li><li>感谢Laszlo Sragner对早期草案的反馈。</li></ul><hr><h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><ol><li>我喜欢数据科学博客文章中点击率高的标题。🙃</li><li>这并不是要挖苦博文作者；而是要说明生产ML问题的广泛性。</li><li>这并不明显，为什么像准确性这样的实时指标的近似是一个有状态的和跨组件的程序。这是有状态的，因为你需要一组历史预测和标签来计算这个指标。这是跨组件的，因为在许多情况下，提供预测的组件与收集反馈（标签）的组件不同。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;作者：Shreya Shankar&lt;/li&gt;
&lt;li&gt;原文连接：&lt;a class=&quot;link&quot;   href=&quot;https://www.shreya-shankar.com/rethinking-ml-monitoring-2/&quot; &gt;rethinking ml </summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MLOps" scheme="http://example.com/tags/MLOps/"/>
    
  </entry>
  
  <entry>
    <title>现代机器学习监控的混乱局面：重新思考流媒体评估（1/4）</title>
    <link href="http://example.com/2023/05/24/model_monitoring_mess_part01/"/>
    <id>http://example.com/2023/05/24/model_monitoring_mess_part01/</id>
    <published>2023-05-24T09:01:16.000Z</published>
    <updated>2023-07-16T12:02:11.652Z</updated>
    
    <content type="html"><![CDATA[<ul><li>作者：Shreya Shankar</li><li><ul><li>原文连接：<a class="link"   href="https://www.shreya-shankar.com/rethinking-ml-monitoring-1/" >rethinking ml monitoring 1<i class="fas fa-external-link-alt"></i></a></li></ul></li></ul><hr><ul><li>我对MLOps感兴趣的部分原因是有太多的工具，而且我无法相信一些受人尊敬的软件老手和学者正在向左、向右、向中间推出新的创业公司。在我之前的公司，使用现有的DevOps工具来简化ML的部署和生产，让我走得很远–但还不够远，促使我思考MLOps问题的定制解决方案，比如ML监控。</li><li>我喜欢当博士生，因为我可以思考问题，而没有人嚷嚷着要我建立什么。事实是，当涉及到监测ML时，我不知道到底应该建立什么。ML监控的现状是，不加掩饰，一团糟。在这个由四篇文章组成的文集中，我说明了我对获得一个好的开源ML监控解决方案的想法，其大纲如下：</li></ul><ol><li>重新思考流媒体机器学习的评估(本篇)</li><li>对部署后的各种问题进行分类</li><li>调查现有的DevOps工具（如Prometheus）在机器学习监控方面的不足之处</li><li>建立一个通用的机器学习监测解决方案的具体研究挑战</li></ol><h2 id="重新思考流媒体ML的评估"><a href="#重新思考流媒体ML的评估" class="headerlink" title="重新思考流媒体ML的评估"></a>重新思考流媒体ML的评估</h2><ul><li>在我讨论什么是监控之前，我将把监控定义为：跟踪指标以确定应用程序何时失败。有很多文章都在说服你，监控对于机器学习应用是必要的。下面是我对为什么我们需要监控的两点看法：</li></ul><ol><li>生产中的应用程序将不可避免地遇到问题。我们希望尽早发现并解决这些问题，以尽量减少停机时间。</li><li>机器学习越来越多地被部署在高风险的场景中（例如，累犯、贷款、信用卡欺诈、招聘、自动驾驶汽车），其中的监管是不可避免的。</li></ol><ul><li>许多文章还讨论了在生产型ML系统中会出现哪些类型的错误，促使人们去监测什么。我个人对所有可能发生的bug以及必须执行和监控的大量测试和指标清单感到不知所措，以确保管道的 “良好 “健康。在我以前的工作中，我曾经有过一次生存危机，因为我不知道为什么要监控成千上万的东西；我只是认为这是管道 “不失败 “的必要条件。但 “失败 “是什么意思，特别是在ML背景下？</li></ul><h2 id="为什么造成这样的局面？"><a href="#为什么造成这样的局面？" class="headerlink" title="为什么造成这样的局面？"></a>为什么造成这样的局面？</h2><ul><li>机器学习领域有一个有趣的评估历史。针对ML的指标被设计用来评估一个特定模型在特定数据集上的表现。为了在 “学术 “环境中评估一个模型，几十年来，我们一直在测量固定的指标，如准确率、精确率和召回率，这些指标是我们的模型在训练期间没有看到的 “验证 “集。为了评估一个模型是否能推广到新的数据，机器学习入门课程强调了检查过拟合的重要性（即验证集指标应该接近训练集指标）。这真的是我们宣称可推广性所需要的全部吗？</li><li>在今天这个基准的黄金时代，我们并没有真正质疑这种在一些静态或固定的数据集上计算指标的评估程序（有一些例外）。最近，我的朋友Deb和其他ML领导人一直在提高人们对当前通用的ML评估方法在评估ML系统中与关键子群体（如种族）和外部性（如能源消耗）相关的失败模式方面的认识。我提到这项工作是因为我认为它非常重要，尽管与我在这里要谈的内容正交。</li><li>我们很清楚，在实践中，虽然所有的模型都是错误的，但有些模型是有用的。在 “行业 “背景下的ML评估，毫不奇怪，可以追溯到50年代和60年代的军事背景下。ROC曲线是在第二次世界大战期间发明的，用于对战场上的敌方物体进行分类，我想主要是因为从业者需要计算有用的模型，并随着时间的推移抛弃无用的模型。我喜欢把ROC曲线和PR曲线看作是更好的指标 “聚合”，它有助于告知模型输出的哪个阈值，以及这种阈值如何随时间变化。当在实践中（例如医疗）长期部署ML时，这种聚合显然是必要的。</li><li>因为在实践中，我们与数据流打交道，而不是固定的数据集，ML监测的行业标准（可能是软件监测的结果）遵循这些步骤：</li></ul><ol><li>选择他们认为代表模型性能的指标和阈值</li><li>选择一个滑动窗口大小（单位是基于时间的，如天或周）来计算指标。</li><li>当指标值下降到阈值以下时，设置警报。</li><li>警报后，手动或自动触发重新训练</li></ol><ul><li>这个过程假设故障被定义为指标值下降到其阈值以下。为什么我们需要把这个定义弄正确呢？在你不需要的时候触发重新训练的影响可能是不好的：它可能会浪费计算，或者如果最近的窗口不能代表未来的数据，它实际上可能会降低性能。在需要的时候不触发重新训练的影响是，你的性能将继续恶化–甚至可能无声无息。因此，我们希望我们的警报是健全的（即，没有错误的警报）和完整的（即，每次出现故障时都触发）。不幸的是，我总是得到太多或者太少的警报，促使我质疑–我们的评估程序有什么问题？</li></ul><h2 id="我们目前的做法有什么问题？"><a href="#我们目前的做法有什么问题？" class="headerlink" title="我们目前的做法有什么问题？"></a>我们目前的做法有什么问题？</h2><ul><li><p>现在我将论证为什么这种在数据流上评估ML的程序被打破了。假设我们有一个从时间t&#x3D;0开始的数据流。我们在t&#x3D;0和t&#x3D;i之间的数据上训练和验证一个模型。我们在t&#x3D;i处 “部署”，并连续计算跨越d天的滚动窗口的ML指标。</p></li><li><p><img src="/img/wuweiwu/ml/model_monitor_translate/train-d.png"></p></li><li><p>有一些自然和常见的现象会被计入公制计算中：</p><ul><li><strong>代表性差异</strong>。各个窗口的类比可能不一样（例如，一个窗口的阳性比例可能与另一个窗口的阳性比例有很大不同）。</li><li><strong>不同的样本大小</strong>。每个窗口中的数据点数量可能不同（例如，周日收到的请求数量少于周一收到的请求数量）。</li><li><strong>延迟的反馈</strong>。由于合理的事件（如失去互联网连接），标签可能会滞后出现，使得没有标签的预测不可能被纳入当前窗口的评估指标。</li></ul></li><li><p>在每一种情况下，即使你测量auROC和auPRC这样的聚合，指标值也会发生剧烈变化–而模型与预期任务的 “一致性”（即预测能力）或 “概念漂移 “没有任何变化。当我们触发重新训练时，我们隐含地相信我们的模型不具有我们预期的预测能力。如果我们的滚动窗口与我们的保留验证集的假设不一致（例如，我们的滚动窗口跨度为一周，而我们的验证集跨度为一个月），我们怎么能对这个信念有信心？大多数时候，我们并没有明确意识到在训练时所作的所有假设。<strong>我认为在一个滚动窗口上评估的ML特定指标告诉你的是模型的一致性，而不是该窗口中数据的属性。</strong></p></li><li><p>因此，由于我们不知道如何评价模型与预期任务的一致性，对数据流的评价在不同的组织中是不同的。我们不知道在哪里为可接受的业务绩效划线，所以我们看所有可辩护的指标（例如auROC），并试图为它们的一些聚合进行优化。虽然我们没有更好的选择（据我所知），但这显然是破绽百出的，并使ML在如此多的产品设置中变得毫无用处。许多行业的ML资深人士谈到，我们需要从ML指标到商业结果的清晰明确的映射–成熟的组织有一个ML模型的 “SLO “概念，其中BizDev和数据人员合作，以确定单一任务（SLO）的指标集合、窗口大小和警报程序。当SLO与任务不一致时，你不知道一个指标的下降是否应该触发一个重新训练！这种合作来计算SLO参数！这种计算SLO参数的合作迫使人们定义如何对模型输出采取行动。疯狂的是，可能需要几个月甚至一年的时间来确定正确的参数来计算SLO。更重要的是，与我的兴趣更相关的是—<strong>这样一个针对特定环境的ML SLO选择程序与建立一个通用工具来监控ML管道是不一致的。</strong></p></li></ul><h2 id="回顾总结"><a href="#回顾总结" class="headerlink" title="回顾总结"></a>回顾总结</h2><ul><li><p>因此，在我所概述的流媒体ML评估中确实有两个不同的问题：</p><ol><li>要使目前的 “行业标准 “评估程序（选择指标、阈值和窗口大小）发挥作用需要时间和许多资源</li><li>这个程序可能永远不会成功，这取决于数据的性质，而且通用性很差（即，需要同样多的时间和资源来解决新任务的评估协议或SLO）–使得ML在生产中真的很难实现</li></ol></li><li><p>我猜想，与ML相关的敏捷宣言的延伸以及围绕ML的优点和缺点的更好的教育可以解决第一个问题。第二个问题对我来说似乎更难解决–作为一个ML信息员，我对规定具体的SLO不感兴趣；我感兴趣的是建立基础设施，让任何人都能轻松监控他们关心的ML SLO。</p></li><li><p><strong>我希望我们的ML社区能更深入地思考如何普遍评估将在数据流上运行的模型的一致性或预测能力。</strong> 也许我们不能比我们已经有的程序做得更好，但我很好奇–我们能不能建立技术来理解ML结果的特定数据的时间性，并使用这些信息来为我们选择SLO？我们能不能建立更多的一般性指标，明确地与业务成果挂钩，如 “ML Apdex Score”？这些一般的指标能否对我上面概述的所有三种现象–代表性差异、不同的样本量和延迟反馈–保持稳健？我不知道这最终会是什么样子，但我相当有信心，能够阐明和衡量正确的SLO将为从生产性ML应用中获得价值提供巨大的突破。我很高兴我们能取得进展。</p></li><li><p>感谢Alex Tamkin、Rolando Garcia和Peter Schafhalter对许多草案的反馈。</p></li></ul><hr><h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><ol><li>没有任何MLOps公司付钱给我写这个。我无法对隐藏在付费墙之外的解决方案发表明智的看法，所以我对现有的专有工具不做任何声明。另外，我认为完全开源的ML监控解决方案是有市场的。</li><li>对我来说，最有用的MLOps资源来自博客文章、Slack频道和口碑，这很疯狂。这个领域完全处于起步阶段。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;作者：Shreya Shankar&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;原文连接：&lt;a class=&quot;link&quot;   href=&quot;https://www.shreya-shankar.com/rethinking-ml-monitoring-1/&quot; &gt;rethi</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="MLOps" scheme="http://example.com/tags/MLOps/"/>
    
  </entry>
  
  <entry>
    <title>何仁讀通鑑 观后感</title>
    <link href="http://example.com/2023/05/21/%E4%BD%95%E4%BB%81%E8%AF%BB%E9%80%9A%E9%89%B4/"/>
    <id>http://example.com/2023/05/21/%E4%BD%95%E4%BB%81%E8%AF%BB%E9%80%9A%E9%89%B4/</id>
    <published>2023-05-21T05:29:33.000Z</published>
    <updated>2023-07-16T01:38:21.691Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a class="link"   href="https://www.youtube.com/watch?v=nrfczKWPZUo&list=PLl7zeOiApUFV6s8wLEO9bRoojIqDmCNPY&index=95" >何仁讀通鑑-Youtube<i class="fas fa-external-link-alt"></i></a></li></ul><h2 id="第二集：三家分晋"><a href="#第二集：三家分晋" class="headerlink" title="第二集：三家分晋"></a>第二集：三家分晋</h2><ul><li>蠢人-&gt;小人-&gt;君子-&gt;圣人</li><li>德才兼备谓之圣人，才德皆无谓之蠢人。</li><li>有德无才谓之君子，有才无德谓之小人。</li></ul><h2 id="第三集：商鞅变法"><a href="#第三集：商鞅变法" class="headerlink" title="第三集：商鞅变法"></a>第三集：商鞅变法</h2><ul><li>过程：<ul><li>为鞅对秦孝公说：</li><li>变法开始的时候，普通大众是不会自动自觉与你一起规划的，但是等到有成果可以享受的时候，他们就会心服诚服了 </li><li>立志实现至善大德之人不应该迎接草根阶层的想法，希望建立宏伟工业的人不能寄望于大众能够与你合作，</li><li>所以，只要能够使国家富强，圣人并不会一味固守旧法， 但是既得利益者就会反驳，因为他们要保住自己的利益能够维持下去，</li><li>为鞅答道：普通人因循守旧，知识分子满足于道听途说 ，他们只适宜于规规矩矩地做官生活，你不能和他们讨论超出这个特定范围的事物， </li><li>聪明人要制定规矩，蠢人就会加以掣肘；有才能的人要改变现有的秩序，没出息的人就会来捣乱。</li><li>秦孝公表示十分认同！</li></ul></li><li>变法涵盖经济，社会，政制等多个方面</li><li>变革，制定规则并且严格执行，重奖，严罚，大众和权贵都要遵守规则，变法才能发挥作用</li><li>支持精英治国，支持专业性，精英政治，以结果为第一评估原则</li><li>没有为自己埋好退路</li></ul><h2 id="第四集：怀王弱智"><a href="#第四集：怀王弱智" class="headerlink" title="第四集：怀王弱智"></a>第四集：怀王弱智</h2><ul><li>贪，狂，怨，软</li></ul><h2 id="第五集：秦王逐客"><a href="#第五集：秦王逐客" class="headerlink" title="第五集：秦王逐客"></a>第五集：秦王逐客</h2><ul><li>既要用才，亦要防才。所谓疑人不用，用人不疑好像是一个极端的说法，中国人最终都要找到一个中庸之道，找到一个平衡之处。</li><li>一代君主一朝臣</li></ul><h2 id="第八集：南越归汉"><a href="#第八集：南越归汉" class="headerlink" title="第八集：南越归汉"></a>第八集：南越归汉</h2><ul><li>真诚信件，趋利避害，不急一时的耐心</li></ul><h2 id="第九集：七国之乱"><a href="#第九集：七国之乱" class="headerlink" title="第九集：七国之乱"></a>第九集：七国之乱</h2><ul><li>固本培元</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a class=&quot;link&quot;   href=&quot;https://www.youtube.com/watch?v=nrfczKWPZUo&amp;list=PLl7zeOiApUFV6s8wLEO9bRoojIqDmCNPY&amp;index=95&quot; &gt;何仁讀通鑑-Youtub</summary>
      
    
    
    
    <category term="认知-修行-平衡" scheme="http://example.com/categories/%E8%AE%A4%E7%9F%A5-%E4%BF%AE%E8%A1%8C-%E5%B9%B3%E8%A1%A1/"/>
    
    
    <category term="从历史学习经验" scheme="http://example.com/tags/%E4%BB%8E%E5%8E%86%E5%8F%B2%E5%AD%A6%E4%B9%A0%E7%BB%8F%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>Mac上通过airas2无限速下载网盘文件</title>
    <link href="http://example.com/2023/05/03/mac-ariel/"/>
    <id>http://example.com/2023/05/03/mac-ariel/</id>
    <published>2023-05-03T05:57:38.000Z</published>
    <updated>2023-07-16T01:32:13.287Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a class="link"   href="https://www.mintimate.cn/2019/06/21/Aria2/" >参考<i class="fas fa-external-link-alt"></i></a></li></ul><h2 id="度盘"><a href="#度盘" class="headerlink" title="度盘"></a>度盘</h2><ol><li>使用浏览器：chrome，科学上网，安装插件Tampermonkey，</li><li>安装前前下载插件，地址<a class="link"   href="https://greasyfork.org/zh-CN/scripts/463171-%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98%E5%8D%83%E5%8D%83%E4%B8%8B%E8%BD%BD%E5%8A%A9%E6%89%8B" >https://greasyfork.org/zh-CN/scripts/463171-%E7%99%BE%E5%BA%A6%E7%BD%91%E7%9B%98%E5%8D%83%E5%8D%83%E4%B8%8B%E8%BD%BD%E5%8A%A9%E6%89%8B<i class="fas fa-external-link-alt"></i></a></li><li>mac上安装aria2<ol><li>安装brew</li><li>通过brew安装aria2，brew install aria2</li><li>aira2.conf配置，附件</li><li>启动aira2后端程序</li></ol></li><li>mac上安装ariaNg，地址<a class="link"   href="https://github.com/mayswind/AriaNg-Native/releases" >https://github.com/mayswind/AriaNg-Native/releases<i class="fas fa-external-link-alt"></i></a></li><li>配置ariaRPC密码，连接成功</li><li>打开度盘网页页面，勾选单个你需要下载的文件，点击上方的千千下载插件按钮，进入获取下载链接页面，关注公共号，获取验证码，点击aira下载，并且输入你的token，就是上面设置的airaRCP密码，就可以高速下载了</li></ol><h2 id="雷盘"><a href="#雷盘" class="headerlink" title="雷盘"></a>雷盘</h2><ol><li>在greasyFork安装网盘下载助手，地址<a class="link"   href="https://greasyfork.org/zh-CN/scripts/436446-%E7%BD%91%E7%9B%98%E7%9B%B4%E9%93%BE%E4%B8%8B%E8%BD%BD%E5%8A%A9%E6%89%8B" >https://greasyfork.org/zh-CN/scripts/436446-%E7%BD%91%E7%9B%98%E7%9B%B4%E9%93%BE%E4%B8%8B%E8%BD%BD%E5%8A%A9%E6%89%8B<i class="fas fa-external-link-alt"></i></a></li><li>进入迅雷下载页面，勾选单个下载文件，点击上方的下载助手，选择RPC下载，填写你的配置端口和token后便可以成功下载</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a class=&quot;link&quot;   href=&quot;https://www.mintimate.cn/2019/06/21/Aria2/&quot; &gt;参考&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 </summary>
      
    
    
    
    <category term="小而美-工具" scheme="http://example.com/categories/%E5%B0%8F%E8%80%8C%E7%BE%8E-%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="下载工具" scheme="http://example.com/tags/%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>常霖法師講-學佛修行信願行</title>
    <link href="http://example.com/2023/03/18/buddism/"/>
    <id>http://example.com/2023/03/18/buddism/</id>
    <published>2023-03-18T03:58:38.000Z</published>
    <updated>2023-07-16T01:27:38.842Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、信：眾生皆可成佛，包含世界萬物，一草一木。"><a href="#一、信：眾生皆可成佛，包含世界萬物，一草一木。" class="headerlink" title="一、信：眾生皆可成佛，包含世界萬物，一草一木。"></a>一、信：眾生皆可成佛，包含世界萬物，一草一木。</h2><ul><li><a class="link"   href="https://youtu.be/UQiJj-gldWw" >【淨薈主辦】常霖法師講「學佛修行信願行」之一: 信 - YouTube<i class="fas fa-external-link-alt"></i></a></li><li>佛偈：<ul><li>人生難得今已得，佛法難聞今已聞；</li><li>此身不向今生度，更待何生度此身？</li></ul></li><li>世间万物必经阶段：成住坏空</li><li>佛曰：眾生皆苦，都有很多不完美，很多缺憾，所以我們需要修佛。</li><li>學佛是學習與自己的心相處，修佛是提升自己的覺察力，成佛是成為更好的自己，每個人心中都有一個佛陀，我們需要不斷進行修行，成為自己的佛陀，才能擺脫六道輪迴，進入西方極樂世界。</li><li>六道輪迴：天道，地道，人道，餓鬼道，地獄道，阿修羅道。只有在人道才能進行修行。</li></ul><h2 id="二、愿"><a href="#二、愿" class="headerlink" title="二、愿"></a>二、愿</h2><ul><li><a class="link"   href="https://youtu.be/TupysCEEJHA" >【淨薈主辦】常霖法師講「學佛修行信願行」之二: 願 - YouTube<i class="fas fa-external-link-alt"></i></a><br>1、 佛缘 -&gt; 佛愿，由因-&gt;果，被动-&gt;主动，清楚自己想要什么，愿意做，并且坚持做，直到达成所愿。</li></ul><h2 id="三、行"><a href="#三、行" class="headerlink" title="三、行"></a>三、行</h2><ul><li><a class="link"   href="https://youtu.be/Qizh3j_GPG4" >【淨薈主辦】常霖法師講「學佛修行信願行」之三: 行 - YouTube<i class="fas fa-external-link-alt"></i></a></li><li>一念一动一缘生</li><li>学佛，学菩萨<ul><li>观世音菩萨，喜欢帮助众生</li><li>地藏菩萨，没有什么做不了的事情，有心者事竟成</li><li>逆行菩萨，针对你搞你，但是在这个过程中你可以学习和进步，最终其实是帮助你</li></ul></li><li>打座是提升你的觉察力</li><li>一切都是最好的安排</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、信：眾生皆可成佛，包含世界萬物，一草一木。&quot;&gt;&lt;a href=&quot;#一、信：眾生皆可成佛，包含世界萬物，一草一木。&quot; class=&quot;headerlink&quot; title=&quot;一、信：眾生皆可成佛，包含世界萬物，一草一木。&quot;&gt;&lt;/a&gt;一、信：眾生皆可成佛，包含世界萬物</summary>
      
    
    
    
    <category term="认知-修行-平衡" scheme="http://example.com/categories/%E8%AE%A4%E7%9F%A5-%E4%BF%AE%E8%A1%8C-%E5%B9%B3%E8%A1%A1/"/>
    
    
    <category term="佛学" scheme="http://example.com/tags/%E4%BD%9B%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>停一停 心呼吸 每日銘言 202307</title>
    <link href="http://example.com/2023/02/03/%E5%BF%83%E9%9D%92%E5%B9%B4-202307/"/>
    <id>http://example.com/2023/02/03/%E5%BF%83%E9%9D%92%E5%B9%B4-202307/</id>
    <published>2023-02-03T02:21:33.000Z</published>
    <updated>2023-07-16T01:40:53.428Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th>日期</th><th>銘言</th></tr></thead><tbody><tr><td>7月16号</td><td><img src="/img/wuweiwu/zen_sayings/2023/07/16.jpg"></td></tr><tr><td>7月14号</td><td><img src="/img/wuweiwu/zen_sayings/2023/07/006.jpg"></td></tr><tr><td>7月12号</td><td><img src="/img/wuweiwu/zen_sayings/2023/07/005.jpg"></td></tr><tr><td>7月9号</td><td><img src="/img/wuweiwu/zen_sayings/2023/07/004.jpg"></td></tr><tr><td>7月7号</td><td><img src="/img/wuweiwu/zen_sayings/2023/07/003.jpg"></td></tr><tr><td>7月6号</td><td><img src="/img/wuweiwu/zen_sayings/2023/07/002.jpg"></td></tr><tr><td>7月3号</td><td><img src="/img/wuweiwu/zen_sayings/2023/07/001.jpg"></td></tr><tr><td>7月2号</td><td><img src="/img/wuweiwu/zen_sayings/2023/07/000.jpg"></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;日期&lt;/th&gt;
&lt;th&gt;銘言&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;7月16号&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;/img/wuweiwu/zen_sayings/2023/07/16.jpg</summary>
      
    
    
    
    <category term="认知-修行-平衡" scheme="http://example.com/categories/%E8%AE%A4%E7%9F%A5-%E4%BF%AE%E8%A1%8C-%E5%B9%B3%E8%A1%A1/"/>
    
    
    <category term="佛学" scheme="http://example.com/tags/%E4%BD%9B%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Training your own datasets with Darknet</title>
    <link href="http://example.com/2021/11/07/Training-your-own-datasets-with-Darknet/"/>
    <id>http://example.com/2021/11/07/Training-your-own-datasets-with-Darknet/</id>
    <published>2021-11-07T06:48:57.000Z</published>
    <updated>2023-07-16T01:37:04.638Z</updated>
    
    <content type="html"><![CDATA[<ul><li>date, 2018-11-07 14:48:57</li></ul><h3 id="Data-collection-amp-labeling"><a href="#Data-collection-amp-labeling" class="headerlink" title="Data collection &amp; labeling"></a>Data collection &amp; labeling</h3><ol><li>Use your own way to collect your data, usually the size of image doesn’t matter. It’ll better to be fit in (48, 48) ~ (1280, 720)</li><li>When you finished your own dataset, you should label your images.<ul><li>tools : <a class="link"   href="https://github.com/tzutalin/labelImg" >labelImage<i class="fas fa-external-link-alt"></i></a> </li><li>Usage : refer to the github</li></ul></li></ol><hr><h3 id="Install-Darknet"><a href="#Install-Darknet" class="headerlink" title="Install Darknet"></a>Install Darknet</h3><ol><li><a class="link"   href="https://pjreddie.com/darknet/install/" >Darknet Installation<i class="fas fa-external-link-alt"></i></a> , compile with GPU and Opencv if it’s necessary</li></ol><hr><h3 id="Create-VOC-format-dataset"><a href="#Create-VOC-format-dataset" class="headerlink" title="Create VOC format dataset"></a>Create VOC format dataset</h3><ul><li><p>(1)  In the root of darknet, create a folder names ‘VOCdevkit’, and create a folder names what you want to name your dataset. like ‘VOC2019_oppo’, which has to start with ‘VOC’.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /path/darknet</span><br><span class="line">mkdir VOCdevkit</span><br><span class="line">cd VOCdevkit</span><br><span class="line">mkdir VOC2019_oppo</span><br></pre></td></tr></table></figure></li><li><p>(2) Directory like this :</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">└── VOCdevkit</span><br><span class="line">    └── VOC2019_oppo</span><br><span class="line">        ├── Annotations</span><br><span class="line">        ├── ImageSets</span><br><span class="line">        │   └── Main</span><br><span class="line">        └── JPEGImages</span><br></pre></td></tr></table></figure></li><li><p>(3) Move the images into <strong>JPEGImages</strong> and xml files into <strong>Annotations</strong>.</p></li><li><p>(4) Split the train, val and test, create a py script like belows</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## split_train_val.py</span></span><br><span class="line"><span class="keyword">import</span> os,random</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the filenames from a file</span></span><br><span class="line">dirname = <span class="string">&#x27;./Annotations&#x27;</span></span><br><span class="line">files = [f[:-<span class="number">4</span>] <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(dirname) <span class="keyword">if</span> f[-<span class="number">4</span>:].lower() == <span class="string">&#x27;.xml&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># random divide  </span></span><br><span class="line">trainval = random.sample(files, <span class="built_in">len</span>(files)//<span class="number">2</span>)</span><br><span class="line">test = [f <span class="keyword">for</span> f <span class="keyword">in</span> files <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> trainval]</span><br><span class="line"></span><br><span class="line"><span class="comment"># random divide </span></span><br><span class="line">train = random.sample(trainval, <span class="built_in">len</span>(trainval)//<span class="number">2</span>)</span><br><span class="line">val = [f <span class="keyword">for</span> f <span class="keyword">in</span> trainval <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> train]</span><br><span class="line"></span><br><span class="line"><span class="comment"># save to txt file</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">list2txt</span>(<span class="params">arr, fname</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname+<span class="string">&#x27;.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> arr:</span><br><span class="line">            f.write(a+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">list2txt(trainval, <span class="string">&#x27;trainval&#x27;</span>)</span><br><span class="line">list2txt(test, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">list2txt(train, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">list2txt(val, <span class="string">&#x27;val&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>then run the script, you will get four files, then move them into the <strong>ImageSets&#x2F;Main&#x2F;</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python split_train_val.py</span><br><span class="line"></span><br><span class="line">mv test.txt ImageSets/Main/</span><br><span class="line">mv train.txt ImageSets/Main/</span><br><span class="line">mv trainval.txt ImageSets/Main/</span><br><span class="line">mv val.txt ImageSets/Main/</span><br></pre></td></tr></table></figure></li><li><p>(5) Now you have the directory like this</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">└── VOCdevkit</span><br><span class="line">    └── VOC2019_oppo</span><br><span class="line">        ├── Annotations</span><br><span class="line">        ├── ImageSets</span><br><span class="line">        │   └── Main</span><br><span class="line">        │       ├── test.txt</span><br><span class="line">        │       ├── train.txt</span><br><span class="line">        │       ├── trainval.txt</span><br><span class="line">        │       └── val.txt</span><br><span class="line">        ├── JPEGImages</span><br><span class="line">        └── split_train_val.py</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Use-the-voc-labe-to-generate-Image-path-list"><a href="#Use-the-voc-labe-to-generate-Image-path-list" class="headerlink" title="Use the voc_labe to generate Image path list"></a>Use the voc_labe to generate Image path list</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /path/darknet</span><br><span class="line">touch voc_label.py</span><br><span class="line">vim voc_label.py</span><br></pre></td></tr></table></figure><ul><li>(1) create a python script<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## name voc_label.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir, getcwd</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. change to your labels</span></span><br><span class="line"><span class="comment"># oppo for example</span></span><br><span class="line"><span class="comment"># 4 classes, A5s, A7, reno, reno10x</span></span><br><span class="line">sets = [(<span class="string">&#x27;2019_oppo&#x27;</span>, <span class="string">&#x27;train&#x27;</span>), (<span class="string">&#x27;2019_oppo&#x27;</span>, <span class="string">&#x27;val&#x27;</span>), (<span class="string">&#x27;2019_oppo&#x27;</span>, <span class="string">&#x27;test&#x27;</span>)]</span><br><span class="line">classes = [<span class="string">&#x27;A5s&#x27;</span>, <span class="string">&#x27;A7&#x27;</span>, <span class="string">&#x27;reno&#x27;</span>, <span class="string">&#x27;reno10x&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert</span>(<span class="params">size, box</span>):</span><br><span class="line">    dw = <span class="number">1.</span>/(size[<span class="number">0</span>])</span><br><span class="line">    dh = <span class="number">1.</span>/(size[<span class="number">1</span>])</span><br><span class="line">    x = (box[<span class="number">0</span>] + box[<span class="number">1</span>])/<span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    y = (box[<span class="number">2</span>] + box[<span class="number">3</span>])/<span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    w = box[<span class="number">1</span>] - box[<span class="number">0</span>]</span><br><span class="line">    h = box[<span class="number">3</span>] - box[<span class="number">2</span>]</span><br><span class="line">    x = x*dw</span><br><span class="line">    w = w*dw</span><br><span class="line">    y = y*dh</span><br><span class="line">    h = h*dh</span><br><span class="line">    <span class="keyword">return</span> (x,y,w,h)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_annotation</span>(<span class="params">year, image_id</span>):</span><br><span class="line">    <span class="comment"># 2. change to your path </span></span><br><span class="line">    in_file = <span class="built_in">open</span>(<span class="string">&#x27;/home/ares2/darknet/VOCdevkit/VOC%s/Annotations/%s.xml&#x27;</span>%(year, image_id))</span><br><span class="line">    out_file = <span class="built_in">open</span>(<span class="string">&#x27;/home/ares2/darknet/VOCdevkit/VOC%s/labels/%s.txt&#x27;</span>%(year, image_id), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    tree=ET.parse(in_file)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line">    size = root.find(<span class="string">&#x27;size&#x27;</span>)</span><br><span class="line">    w = <span class="built_in">int</span>(size.find(<span class="string">&#x27;width&#x27;</span>).text)</span><br><span class="line">    h = <span class="built_in">int</span>(size.find(<span class="string">&#x27;height&#x27;</span>).text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> root.<span class="built_in">iter</span>(<span class="string">&#x27;object&#x27;</span>):</span><br><span class="line">        difficult = obj.find(<span class="string">&#x27;difficult&#x27;</span>).text</span><br><span class="line">        cls = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> classes <span class="keyword">or</span> <span class="built_in">int</span>(difficult)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        cls_id = classes.index(cls)</span><br><span class="line">        xmlbox = obj.find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">        b = (<span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmin&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmax&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymin&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymax&#x27;</span>).text))</span><br><span class="line">        bb = convert((w,h), b)</span><br><span class="line">        out_file.write(<span class="built_in">str</span>(cls_id) + <span class="string">&quot; &quot;</span> + <span class="string">&quot; &quot;</span>.join([<span class="built_in">str</span>(a) <span class="keyword">for</span> a <span class="keyword">in</span> bb]) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">wd = getcwd()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> year, image_set <span class="keyword">in</span> sets:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;VOCdevkit/VOC%s/labels/&#x27;</span>%(year)):</span><br><span class="line">        os.makedirs(<span class="string">&#x27;VOCdevkit/VOC%s/labels/&#x27;</span>%(year))</span><br><span class="line">    <span class="comment"># 3. change to your path</span></span><br><span class="line">    image_ids = <span class="built_in">open</span>(<span class="string">&#x27;/home/ares2/darknet/VOCdevkit/VOC%s/ImageSets/Main/%s.txt&#x27;</span>%(year, image_set)).read().strip().split()</span><br><span class="line">    list_file = <span class="built_in">open</span>(<span class="string">&#x27;%s_%s.txt&#x27;</span>%(year, image_set), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> image_id <span class="keyword">in</span> image_ids:</span><br><span class="line">        list_file.write(<span class="string">&#x27;%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\n&#x27;</span>%(wd, year, image_id))</span><br><span class="line">        convert_annotation(year, image_id)</span><br><span class="line">    list_file.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>remember that there are 3 places you need to change</li><li>this will generate 3 files: <ul><li><strong>2019_oppo_train.txt</strong></li><li><strong>2019_oppo_val.txt</strong></li><li><strong>2019_oppo_test.txt</strong></li></ul></li><li>Usually I merge <strong>2019_oppo_train.txt</strong> and <strong>2019_oppo_test.txt</strong> as <strong>2019_oppo_train.txt</strong> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /path/darknet</span><br><span class="line">mkdir oppo_od_bak</span><br><span class="line">cd oppo_od_bak</span><br><span class="line">mkdir cfg</span><br></pre></td></tr></table></figure></li><li>from the <strong>darknet&#x2F;cfg&#x2F;</strong> you can find the <strong>yolo-voc.cfg</strong> and the <strong>yolo-tiny.cfg</strong> and from the <a class="link"   href="https://pjreddie.com/darknet/yolo/" >official website<i class="fas fa-external-link-alt"></i></a> you can download the <strong>pretrained models</strong>, like for the <strong>yolo-voc</strong> is <a class="link"   href="https://pjreddie.com/media/files/darknet53.conv.74" >darknet53.conv.74<i class="fas fa-external-link-alt"></i></a>.</li></ul><hr><h3 id="Prepare-your-cfg-file"><a href="#Prepare-your-cfg-file" class="headerlink" title="Prepare your cfg file"></a>Prepare your cfg file</h3><ul><li>the 3 files you use to train the yolo is <ul><li><strong>yourdata.names</strong></li><li><strong>yourdata.data</strong></li><li><strong>yourcfg.cfg</strong></li></ul></li><li>(1) <strong>yourdata.names</strong> contains the labels of your dataset, each label for a line</li><li>(2) <strong>yourdata.data</strong> example<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classes= #classes #类别数目</span><br><span class="line">train  = /path/yourfilename_train.txt # 训练数据</span><br><span class="line">valid  = /path/yourfilenane_val.txt # 验证数据</span><br><span class="line">names = data/yourname.names # class labels</span><br><span class="line">backup = /backup/ # 权重保存所在文件</span><br></pre></td></tr></table></figure></li><li>remember to delete the comments</li><li>(3) <strong>yourcfg.cfg</strong><ul><li>you can use the <strong>yolo-voc.cfg</strong> or the <strong>yolo-tiny.cfg</strong></li><li>remember to change these places</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim yolo-voc.cfg</span><br><span class="line"></span><br><span class="line">## Remember to comment the testing and uncomment the training</span><br><span class="line">[net]</span><br><span class="line"># Testing</span><br><span class="line"># batch=1</span><br><span class="line"># subdivisions=1</span><br><span class="line"># Training</span><br><span class="line">batch=64</span><br><span class="line">subdivisions=16</span><br></pre></td></tr></table></figure><ul><li>YOU should change every [yolo] layer.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">size=1</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=27 ## YOU SHOULD CHANGE THE # OF FILTERS</span><br><span class="line">## filters = (classes + 5) * 3</span><br><span class="line">activation=linear</span><br><span class="line"></span><br><span class="line">[yolo]</span><br><span class="line">mask = 6,7,8</span><br><span class="line">anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326</span><br><span class="line">classes=4  ## CHANGE TO THE NUMBER OF YOUR LABELS</span><br><span class="line">num=9</span><br><span class="line">jitter=.3</span><br><span class="line">ignore_thresh = .5</span><br><span class="line">truth_thresh = 1</span><br><span class="line">random=1</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Start-Training"><a href="#Start-Training" class="headerlink" title="Start Training"></a>Start Training</h3><ul><li>First time you train, use the pretrained classification model<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd path/darknet/</span><br><span class="line"># yolo-tiny</span><br><span class="line">./darknet detector train cfg/yourdata.data cfg/yourcfg.cfg backup/bo_can_tiny_176.weights</span><br><span class="line"># yolo-voc</span><br><span class="line">./darknet detector train cfg/yourdata.data cfg/yourcfg.cfg backup/darknet53.conv.74</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Know-your-log"><a href="#Know-your-log" class="headerlink" title="Know your log"></a>Know your log</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Region <span class="number">82</span> Avg IOU: <span class="number">0.801934</span>, Class: <span class="number">0.737764</span>, Obj: <span class="number">0.782024</span>, No Obj: <span class="number">0.006216</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">1.000000</span>, count: <span class="number">5</span> </span><br><span class="line">Region <span class="number">94</span> Avg IOU: <span class="number">0.706899</span>, Class: <span class="number">0.073915</span>, Obj: <span class="number">0.544467</span>, No Obj: <span class="number">0.000506</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">0.000000</span>, count: <span class="number">1</span> </span><br><span class="line">Region <span class="number">106</span> Avg IOU: <span class="number">0.831056</span>, Class: <span class="number">0.037965</span>, Obj: <span class="number">0.026004</span>, No Obj: <span class="number">0.000057</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">1.000000</span>, count: <span class="number">1</span> </span><br><span class="line">Region <span class="number">82</span> Avg IOU: <span class="number">0.731572</span>, Class: <span class="number">0.800899</span>, Obj: <span class="number">0.793200</span>, No Obj: <span class="number">0.005694</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">0.333333</span>, count: <span class="number">3</span> </span><br><span class="line">Region <span class="number">94</span> Avg IOU: <span class="number">0.607969</span>, Class: <span class="number">0.199724</span>, Obj: <span class="number">0.884315</span>, No Obj: <span class="number">0.000286</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">0.000000</span>, count: <span class="number">1</span> </span><br><span class="line">Region <span class="number">106</span> Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: <span class="number">0.000015</span>, <span class="number">.5</span>R: -nan, <span class="number">.75</span>R: -nan, count:</span><br></pre></td></tr></table></figure><ul><li>（1）以上输出显示了所有训练图片的一个批次（batch），批次大小的划分根据我们在 .cfg 文件中设置的subdivisions参数。在我使用的 .cfg 文件中 batch &#x3D; 64 ，subdivision &#x3D; 16，所以在训练输出中，训练迭代包含了16组，每组又包含了4张图片，跟设定的batch和subdivision的值一致。<br>但是此处有16*3条信息，每组包含三条信息，分别是：<br>Region 82 Avg IOU:<br>Region 94 Avg IOU:<br>Region 106 Avg IOU:<br>三个尺度上预测不同大小的框 82卷积层 为最大的预测尺度，使用较大的mask，但是可以预测出较小的物体 94卷积层 为中间的预测尺度，使用中等的mask， 106卷积层为最小的预测尺度，使用较小的mask，可以预测出较大的物体</li><li>（2）每个batch都会有这样一个输出：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2706: 1.350835, 1.386559 avg, 0.001000 rate, 3.323842 seconds, 173184 images</span><br></pre></td></tr></table></figure></li></ul><p>2706：batch是第几组。<br>1.350835：总损失<br>1.386559 avg ： 平均损失<br>0.001000 rate：当前的学习率<br>3.323842 seconds： 当前batch训练所花的时间<br>173184 images ： 目前为止参与训练的图片总数 &#x3D; 2706 * 64 </p><ul><li>（3）<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Region 82 Avg IOU: 0.798032, Class: 0.559781, Obj: 0.515851, No Obj: 0.006533, .5R: 1.000000, .75R: 1.000000,  count: 2</span><br></pre></td></tr></table></figure></li></ul><p>Region Avg IOU: 表示在当前subdivision内的图片的平均IOU，代表预测的矩形框和真实目标的交集与并集之比.<br>Class: 标注物体分类的正确率，期望该值趋近于1。<br>Obj: 越接近1越好。<br>No Obj: 期望该值越来越小，但不为零。<br>count: count后的值是所有的当前subdivision图片（本例中一共4张）中包含正样本的图片的数量。</p><ul><li>参考：<a class="link"   href="https://blog.csdn.net/qq_33444963/article/details/80842179" >https://blog.csdn.net/qq_33444963&#x2F;article&#x2F;details&#x2F;80842179<i class="fas fa-external-link-alt"></i></a></li></ul><hr><h3 id="Training-experience"><a href="#Training-experience" class="headerlink" title="Training experience"></a>Training experience</h3><ul><li><p><strong>YOLO-TINY</strong></p><ul><li>It’s a simple network for feature extraction, fit to the simple circumstances.</li><li>Each class should have more than 500 images</li><li>Training more than 1000 epoches</li><li>Fast but low accurate.</li></ul></li><li><p><strong>YOLO-VOC</strong></p><ul><li>It’s a complicated network training on the Imagenet</li><li>Each class should have more than 300 images</li><li>Traing more than 10000 epoches.</li><li>Slow but accurate</li></ul></li><li><p>Overall, more images, the model will be better. You can try to add images slowly.</p></li></ul><h3 id="How-to-run-your-own-yolov3-model-with-Opencv"><a href="#How-to-run-your-own-yolov3-model-with-Opencv" class="headerlink" title="How to run your own yolov3 model with Opencv"></a>How to run your own yolov3 model with Opencv</h3><ul><li>first you need to install the opencv</li><li>then, you just copy three files from what you have trained<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yourname.name</span><br><span class="line">yourcfg.cfg</span><br><span class="line">yourweights.weights</span><br></pre></td></tr></table></figure></li><li>then set them in the config file<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class FLAGS:</span><br><span class="line">    # Initialize the parameters</span><br><span class="line">    confThreshold = 0.65  # Confidence threshold</span><br><span class="line">    nmsThreshold = 0.3  # Non-maximum suppression threshold</span><br><span class="line">    inpWidth = 416  # Width of network&#x27;s input image</span><br><span class="line">    inpHeight = 416  # Height of network&#x27;s input image</span><br><span class="line"></span><br><span class="line">    camera_id = 0</span><br><span class="line"></span><br><span class="line">    # Load names of classes</span><br><span class="line">    classesFile = &quot;./shelves_od_300/shelves_od.names&quot;</span><br><span class="line">    classes = None</span><br><span class="line">    with open(classesFile, &#x27;rt&#x27;) as f:</span><br><span class="line">        classes = f.read().rstrip(&#x27;\n&#x27;).split(&#x27;\n&#x27;)</span><br><span class="line"></span><br><span class="line">    # Give the configuration and weight files for the model and load the network using them</span><br><span class="line">    modelConfiguration = &quot;./shelves_od_300/yolov3-voc.cfg&quot;</span><br><span class="line">    modelWeights = &quot;./shelves_od_300/yolov3-voc_latest.weights&quot;</span><br></pre></td></tr></table></figure></li><li>Finally, run the script below<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"># This code is written at BigVision LLC. It is based on the OpenCV project. It is subject to the license terms in the LICENSE file found in this distribution and at http://opencv.org/license.html</span><br><span class="line"></span><br><span class="line"># Usage example:  python3 object_detection_yolo.py --video=run.mp4</span><br><span class="line">#                 python3 object_detection_yolo.py --image=bird.jpg</span><br><span class="line"></span><br><span class="line">import cv2 as cv</span><br><span class="line">import argparse</span><br><span class="line">import sys</span><br><span class="line">import numpy as np</span><br><span class="line">import os.path</span><br><span class="line">import uuid</span><br><span class="line"></span><br><span class="line">from config import FLAGS</span><br><span class="line"></span><br><span class="line"># Initialize the parameters</span><br><span class="line">confThreshold = FLAGS.confThreshold  #Confidence threshold</span><br><span class="line">nmsThreshold = FLAGS.nmsThreshold   #Non-maximum suppression threshold</span><br><span class="line">inpWidth = FLAGS.inpWidth       #Width of network&#x27;s input image</span><br><span class="line">inpHeight = FLAGS.inpHeight      #Height of network&#x27;s input image</span><br><span class="line"></span><br><span class="line">classes = FLAGS.classes</span><br><span class="line">global _i</span><br><span class="line">_i = 1000</span><br><span class="line"># Get the =-.l2 of the output layers</span><br><span class="line">def getOutputsNames(net):</span><br><span class="line">    # Get the names of all the layers in the network</span><br><span class="line">    layersNames = net.getLayerNames()</span><br><span class="line">    # Get the names of the output layers, i.e. the layers with unconnected outputs</span><br><span class="line">    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]</span><br><span class="line"></span><br><span class="line"># Draw the predicted bounding box</span><br><span class="line">def drawPred(frame, classId, conf, left, top, right, bottom):</span><br><span class="line">    # Draw a bounding box.</span><br><span class="line">    cv.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)</span><br><span class="line">    </span><br><span class="line">    label = &#x27;%.2f&#x27; % conf</span><br><span class="line">        </span><br><span class="line">    # Get the label for the class name and its confidence</span><br><span class="line">    if classes:</span><br><span class="line">        assert(classId &lt; len(classes))</span><br><span class="line">        label = &#x27;%s:%s&#x27; % (classes[classId], label)</span><br><span class="line"></span><br><span class="line">    #Display the label at the top of the bounding box</span><br><span class="line">    labelSize, baseLine = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)</span><br><span class="line">    top = max(top, labelSize[1])</span><br><span class="line">    cv.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (255, 255, 255), cv.FILLED)</span><br><span class="line">    cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)</span><br><span class="line"></span><br><span class="line"># Remove the bounding boxes with low confidence using non-maxima suppression</span><br><span class="line">def postprocess(frame, outs):</span><br><span class="line">    frameHeight = frame.shape[0]</span><br><span class="line">    frameWidth = frame.shape[1]</span><br><span class="line"></span><br><span class="line">    # Scan through all the bounding boxes output from the network and keep only the</span><br><span class="line">    # ones with high confidence scores. Assign the box&#x27;s class label as the class with the highest score.</span><br><span class="line">    classIds = []</span><br><span class="line">    confidences = []</span><br><span class="line">    boxes = []</span><br><span class="line">    for out in outs:</span><br><span class="line">        for detection in out:</span><br><span class="line">            scores = detection[5:]</span><br><span class="line">            classId = np.argmax(scores)</span><br><span class="line">            confidence = scores[classId]</span><br><span class="line">            if confidence &gt; confThreshold:</span><br><span class="line">                center_x = int(detection[0] * frameWidth)</span><br><span class="line">                center_y = int(detection[1] * frameHeight)</span><br><span class="line">                width = int(detection[2] * frameWidth)</span><br><span class="line">                height = int(detection[3] * frameHeight)</span><br><span class="line">                left = int(center_x - width / 2)</span><br><span class="line">                top = int(center_y - height / 2)</span><br><span class="line">                classIds.append(classId)</span><br><span class="line">                confidences.append(float(confidence))</span><br><span class="line">                boxes.append([left, top, width, height])</span><br><span class="line">    global _i</span><br><span class="line">    # Perform non maximum suppression to eliminate redundant overlapping boxes with</span><br><span class="line">    # lower confidences.</span><br><span class="line">    indices = cv.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)</span><br><span class="line">    for i in indices:</span><br><span class="line">        i = i[0]</span><br><span class="line">        box = boxes[i]</span><br><span class="line">        left = box[0]</span><br><span class="line">        top = box[1]</span><br><span class="line">        width = box[2]</span><br><span class="line">        height = box[3]</span><br><span class="line"></span><br><span class="line">        ## save crop image</span><br><span class="line">        crop_img = frame[top:top+height, left:left+width, ]</span><br><span class="line">        #resized_img = cv.resize(crop_img, (100, 100))</span><br><span class="line">        #if _i % 5 == 0:</span><br><span class="line">        #cv.imwrite(&#x27;save_imgs/&#x27;+str(uuid.uuid1())+&#x27;.jpg&#x27;, crop_img)</span><br><span class="line">        _i = _i + 1</span><br><span class="line">        drawPred(frame, classIds[i], confidences[i], left, top, left + width, top + height)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def processing_yolov3(args):</span><br><span class="line"></span><br><span class="line">    net = cv.dnn.readNetFromDarknet(FLAGS.modelConfiguration, FLAGS.modelWeights)</span><br><span class="line">    net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)</span><br><span class="line">    net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)</span><br><span class="line"></span><br><span class="line">    # Process inputs</span><br><span class="line">    winName = &#x27;Deep learning object detection in OpenCV&#x27;</span><br><span class="line">    cv.namedWindow(winName, cv.WINDOW_NORMAL)</span><br><span class="line"></span><br><span class="line">    outputFile = &quot;yolo_out_py.avi&quot;</span><br><span class="line">    if (args.image):</span><br><span class="line">        # Open the image file</span><br><span class="line">        if not os.path.isfile(args.image):</span><br><span class="line">            print(&quot;Input image file &quot;, args.image, &quot; doesn&#x27;t exist&quot;)</span><br><span class="line">            sys.exit(1)</span><br><span class="line">        cap = cv.VideoCapture(args.image)</span><br><span class="line">        outputFile = args.image[:-4]+&#x27;_yolo_out_py.jpg&#x27;</span><br><span class="line">    elif (args.video):</span><br><span class="line">        # Open the video file</span><br><span class="line">        if not os.path.isfile(args.video):</span><br><span class="line">            print(&quot;Input video file &quot;, args.video, &quot; doesn&#x27;t exist&quot;)</span><br><span class="line">            sys.exit(1)</span><br><span class="line">        cap = cv.VideoCapture(args.video)</span><br><span class="line">        outputFile = args.video[:-4]+&#x27;_yolo_out_py.avi&#x27;</span><br><span class="line">    else:</span><br><span class="line">        # Webcam input</span><br><span class="line">        cap = cv.VideoCapture(FLAGS.camera_id)</span><br><span class="line"></span><br><span class="line">        cap.set(3, 720)</span><br><span class="line">        cap.set(4, 1280)</span><br><span class="line"></span><br><span class="line">    # Get the video writer initialized to save the output video</span><br><span class="line">    if (not args.image):</span><br><span class="line">        vid_writer = cv.VideoWriter(outputFile, cv.VideoWriter_fourcc(&#x27;M&#x27;,&#x27;J&#x27;,&#x27;P&#x27;,&#x27;G&#x27;), 30, (round(cap.get(cv.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv.CAP_PROP_FRAME_HEIGHT))))</span><br><span class="line"></span><br><span class="line">    while cv.waitKey(1) &lt; 0:</span><br><span class="line"></span><br><span class="line">        # get frame from the video</span><br><span class="line">        hasFrame, frame = cap.read()</span><br><span class="line"></span><br><span class="line">        # Stop the program if reached end of video</span><br><span class="line">        if not hasFrame:</span><br><span class="line">            print(&quot;Done processing !!!&quot;)</span><br><span class="line">            print(&quot;Output file is stored as &quot;, outputFile)</span><br><span class="line">            cv.waitKey(3000)</span><br><span class="line">            # Release device</span><br><span class="line">            cap.release()</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">        # Create a 4D blob from a frame.</span><br><span class="line">        blob = cv.dnn.blobFromImage(frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)</span><br><span class="line"></span><br><span class="line">        # Sets the input to the network</span><br><span class="line">        net.setInput(blob)</span><br><span class="line"></span><br><span class="line">        # Runs the forward pass to get output of the output layers</span><br><span class="line">        outs = net.forward(getOutputsNames(net))</span><br><span class="line"></span><br><span class="line">        # Remove the bounding boxes with low confidence</span><br><span class="line">        postprocess(frame, outs)</span><br><span class="line"></span><br><span class="line">        # Put efficiency information. The function getPerfProfile returns the overall time for inference(t) and the timings for each of the layers(in layersTimes)</span><br><span class="line">        t, _ = net.getPerfProfile()</span><br><span class="line">        label = &#x27;Inference time: %.2f ms&#x27; % (t * 1000.0 / cv.getTickFrequency())</span><br><span class="line">        cv.putText(frame, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))</span><br><span class="line"></span><br><span class="line">        # Write the frame with the detection boxes</span><br><span class="line">        if (args.image):</span><br><span class="line">            cv.imwrite(outputFile, frame.astype(np.uint8))</span><br><span class="line">        else:</span><br><span class="line">            vid_writer.write(frame.astype(np.uint8))</span><br><span class="line"></span><br><span class="line">        cv.imshow(winName, frame)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    parser = argparse.ArgumentParser(description=&#x27;Object Detection using YOLO in OPENCV&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--image&#x27;, help=&#x27;Path to image file.&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--video&#x27;, help=&#x27;Path to video file.&#x27;)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    processing_yolov3(args)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;date, 2018-11-07 14:48:57&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Data-collection-amp-labeling&quot;&gt;&lt;a href=&quot;#Data-collection-amp-labeling&quot; class=&quot;headerlin</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolov3" scheme="http://example.com/tags/yolov3/"/>
    
    <category term="darknet" scheme="http://example.com/tags/darknet/"/>
    
  </entry>
  
  <entry>
    <title>Training your own data with TF object detection API</title>
    <link href="http://example.com/2021/02/03/Training-your-own-data-with-TF-object-detection-API/"/>
    <id>http://example.com/2021/02/03/Training-your-own-data-with-TF-object-detection-API/</id>
    <published>2021-02-03T06:28:07.000Z</published>
    <updated>2023-07-16T01:45:09.790Z</updated>
    
    <content type="html"><![CDATA[<h2 id="System-Info"><a href="#System-Info" class="headerlink" title="System Info"></a>System Info</h2><ul><li>Ubuntu 16.04</li><li>Git</li><li>TF 2.0</li><li>pillow</li><li>lxml</li><li>protobuf ( &gt; 3.3 , my version, 3.11.2)</li><li><a class="link"   href="https://www.cnblogs.com/gezhuangzhuang/p/10613468.html" >ref1-tensorflow+ssd_mobilenet实现目标检测的训练<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="https://blog.csdn.net/dy_guox/article/details/79111949" >ref2-（更新视频教程）Tensorflow object detection API 搭建属于自己的物体识别模型（2）——训练并使用自己的模型<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="https://blog.csdn.net/linolzhang/article/details/87121875" >ref3-Tensorflow object detection API训练自己的数据<i class="fas fa-external-link-alt"></i></a></li></ul><h2 id="To-dos"><a href="#To-dos" class="headerlink" title="To-dos"></a>To-dos</h2><ul><li><a class="link"   href="https://github.com/tensorflow/models/tree/master/research/object_detection" >TF object detection API<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="Env-prepare"><a href="#Env-prepare" class="headerlink" title="Env prepare"></a>Env prepare</h3><ul><li>Clone the <a class="link"   href="https://github.com/tensorflow/models" >model repository<i class="fas fa-external-link-alt"></i></a> into local</li><li><a class="link"   href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md" >Guid for installation<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="Make-your-own-dataset"><a href="#Make-your-own-dataset" class="headerlink" title="Make your own dataset"></a>Make your own dataset</h3><ul><li>For us, we have the yolo format annotaion files(txt files), but TFRecord format data is fit to the tensorlow.</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yolo-2-voc.py</span><br><span class="line">voc-2-csv.py</span><br><span class="line">csv-2-tfrecord.py</span><br></pre></td></tr></table></figure><h4 id="yolo-to-voc"><a href="#yolo-to-voc" class="headerlink" title="yolo to voc"></a>yolo to voc</h4><ul><li>Prepare two folders, one for <strong>annotation files</strong> and the other for the <strong>image files</strong>. VOC format(xml files) will save into the <strong>converted_lanbels</strong> folder.</li><li><strong>manual change your own data label-mappings</strong></li><li>Notice that the value of <strong>(x, y, width, height) are integers</strong> .<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"># Script to convert yolo annotations to voc format</span><br><span class="line"></span><br><span class="line"># Sample format</span><br><span class="line"># &lt;annotation&gt;</span><br><span class="line">#     &lt;folder&gt;_image_fashion&lt;/folder&gt;</span><br><span class="line">#     &lt;filename&gt;brooke-cagle-39574.jpg&lt;/filename&gt;</span><br><span class="line">#     &lt;size&gt;</span><br><span class="line">#         &lt;width&gt;1200&lt;/width&gt;</span><br><span class="line">#         &lt;height&gt;800&lt;/height&gt;</span><br><span class="line">#         &lt;depth&gt;3&lt;/depth&gt;</span><br><span class="line">#     &lt;/size&gt;</span><br><span class="line">#     &lt;segmented&gt;0&lt;/segmented&gt;</span><br><span class="line">#     &lt;object&gt;</span><br><span class="line">#         &lt;name&gt;head&lt;/name&gt;</span><br><span class="line">#         &lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">#         &lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">#         &lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">#         &lt;bndbox&gt;</span><br><span class="line">#             &lt;xmin&gt;549&lt;/xmin&gt;</span><br><span class="line">#             &lt;ymin&gt;251&lt;/ymin&gt;</span><br><span class="line">#             &lt;xmax&gt;625&lt;/xmax&gt;</span><br><span class="line">#             &lt;ymax&gt;335&lt;/ymax&gt;</span><br><span class="line">#         &lt;/bndbox&gt;</span><br><span class="line">#     &lt;/object&gt;</span><br><span class="line"># &lt;annotation&gt;</span><br><span class="line">import os</span><br><span class="line">import xml.etree.cElementTree as ET</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">ANNOTATIONS_DIR_PREFIX = &quot;annotations&quot;</span><br><span class="line"></span><br><span class="line">DESTINATION_DIR = &quot;converted_labels&quot;</span><br><span class="line"></span><br><span class="line">CLASS_MAPPING = &#123;</span><br><span class="line">    &#x27;0&#x27;: &#x27;cream_hazelnut&#x27;,</span><br><span class="line">    &#x27;1&#x27;: &#x27;cream_berry&#x27;,</span><br><span class="line">    &#x27;2&#x27;: &#x27;cream_cherry&#x27;,</span><br><span class="line">    &#x27;3&#x27;: &#x27;yida_cool_lemon&#x27;,</span><br><span class="line">    &#x27;4&#x27;: &#x27;box_yogurt_mango&#x27;,</span><br><span class="line">    &#x27;5&#x27;: &#x27;white_strawberry&#x27;,</span><br><span class="line">    &#x27;6&#x27;: &#x27;cookies_lemon&#x27;,</span><br><span class="line">    &#x27;7&#x27;: &#x27;yogurt_cranberry&#x27;,</span><br><span class="line">    &#x27;8&#x27;: &#x27;box_cookies_matcha&#x27;,</span><br><span class="line">    &#x27;9&#x27;: &#x27;cookies_matcha&#x27;,</span><br><span class="line">    &#x27;10&#x27;: &#x27;yogurt_mango&#x27;,</span><br><span class="line">    &#x27;11&#x27;: &#x27;white_passionfruit&#x27;,</span><br><span class="line">    &#x27;12&#x27;: &#x27;yida_cool_litchi&#x27;,</span><br><span class="line">    &#x27;13&#x27;: &#x27;box_white_strawberry&#x27;</span><br><span class="line">    # Add your remaining classes here.</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_root(file_prefix, width, height):</span><br><span class="line">    root = ET.Element(&quot;annotations&quot;)</span><br><span class="line">    ET.SubElement(root, &quot;filename&quot;).text = &quot;&#123;&#125;.jpg&quot;.format(file_prefix)</span><br><span class="line">    ET.SubElement(root, &quot;folder&quot;).text = &quot;images&quot;</span><br><span class="line">    size = ET.SubElement(root, &quot;size&quot;)</span><br><span class="line">    ET.SubElement(size, &quot;width&quot;).text = str(width)</span><br><span class="line">    ET.SubElement(size, &quot;height&quot;).text = str(height)</span><br><span class="line">    ET.SubElement(size, &quot;depth&quot;).text = &quot;3&quot;</span><br><span class="line">    return root</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_object_annotation(root, voc_labels):</span><br><span class="line">    for voc_label in voc_labels:</span><br><span class="line">        obj = ET.SubElement(root, &quot;object&quot;)</span><br><span class="line">        ET.SubElement(obj, &quot;name&quot;).text = voc_label[0]</span><br><span class="line">        ET.SubElement(obj, &quot;pose&quot;).text = &quot;Unspecified&quot;</span><br><span class="line">        ET.SubElement(obj, &quot;truncated&quot;).text = str(0)</span><br><span class="line">        ET.SubElement(obj, &quot;difficult&quot;).text = str(0)</span><br><span class="line">        bbox = ET.SubElement(obj, &quot;bndbox&quot;)</span><br><span class="line">        ET.SubElement(bbox, &quot;xmin&quot;).text = str(voc_label[1])</span><br><span class="line">        ET.SubElement(bbox, &quot;ymin&quot;).text = str(voc_label[2])</span><br><span class="line">        ET.SubElement(bbox, &quot;xmax&quot;).text = str(voc_label[3])</span><br><span class="line">        ET.SubElement(bbox, &quot;ymax&quot;).text = str(voc_label[4])</span><br><span class="line">    return root</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_file(file_prefix, width, height, voc_labels):</span><br><span class="line">    root = create_root(file_prefix, width, height)</span><br><span class="line">    root = create_object_annotation(root, voc_labels)</span><br><span class="line">    tree = ET.ElementTree(root)</span><br><span class="line">    tree.write(&quot;&#123;&#125;/&#123;&#125;.xml&quot;.format(DESTINATION_DIR, file_prefix))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_file(file_path):</span><br><span class="line">    file_prefix = file_path.split(&quot;.txt&quot;)[0]</span><br><span class="line">    image_file_name = &quot;&#123;&#125;.jpg&quot;.format(file_prefix)</span><br><span class="line">    img = Image.open(&quot;&#123;&#125;/&#123;&#125;&quot;.format(&quot;images&quot;, image_file_name))</span><br><span class="line">    w, h = img.size</span><br><span class="line">    </span><br><span class="line">    with open(&quot;&#123;&#125;/&#123;&#125;&quot;.format(ANNOTATIONS_DIR_PREFIX, file_path), &#x27;r&#x27;) as file:</span><br><span class="line">        lines = file.readlines()</span><br><span class="line">        voc_labels = []</span><br><span class="line">        for line in lines:</span><br><span class="line">            voc = []</span><br><span class="line">            line = line.strip()</span><br><span class="line">            data = line.split()</span><br><span class="line">            voc.append(CLASS_MAPPING.get(data[0]))</span><br><span class="line">            bbox_width = float(data[3]) * w</span><br><span class="line">            bbox_height = float(data[4]) * h</span><br><span class="line">            center_x = float(data[1]) * w</span><br><span class="line">            center_y = float(data[2]) * h</span><br><span class="line">            voc.append(int(center_x - (bbox_width / 2)))</span><br><span class="line">            voc.append(int(center_y - (bbox_height / 2)))</span><br><span class="line">            voc.append(int(center_x + (bbox_width / 2)))</span><br><span class="line">            voc.append(int(center_y + (bbox_height / 2)))</span><br><span class="line">            voc_labels.append(voc)</span><br><span class="line">        create_file(file_prefix, w, h, voc_labels)</span><br><span class="line">    print(&quot;Processing complete for file: &#123;&#125;/&#123;&#125;&quot;.format(ANNOTATIONS_DIR_PREFIX, file_path))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def start():</span><br><span class="line">    if not os.path.exists(DESTINATION_DIR):</span><br><span class="line">        os.makedirs(DESTINATION_DIR)</span><br><span class="line">    for filename in os.listdir(ANNOTATIONS_DIR_PREFIX):</span><br><span class="line">        if filename.endswith(&#x27;txt&#x27;):</span><br><span class="line">            read_file(filename)</span><br><span class="line">        else:</span><br><span class="line">            print(&quot;Skipping file: &#123;&#125;&quot;.format(filename))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    start()</span><br></pre></td></tr></table></figure></li></ul><h4 id="train-test-split-on-xml-files"><a href="#train-test-split-on-xml-files" class="headerlink" title="train test split on xml files"></a>train test split on xml files</h4><ul><li>You can change the percentage to split the dataset manually.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import random</span><br><span class="line">import time</span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">xmlfilepath = r&#x27;./Annotations&#x27;</span><br><span class="line">saveBasePath = r&quot;./&quot;</span><br><span class="line"></span><br><span class="line">trainval_percent = 0.8</span><br><span class="line">train_percent = 0.8</span><br><span class="line">total_xml = os.listdir(xmlfilepath)</span><br><span class="line">num = len(total_xml)</span><br><span class="line">list = range(num)</span><br><span class="line">tv = int(num * trainval_percent)</span><br><span class="line">tr = int(tv * train_percent)</span><br><span class="line">trainval = random.sample(list, tv)</span><br><span class="line">train = random.sample(trainval, tr)</span><br><span class="line">print(&quot;train and val size&quot;, tv)</span><br><span class="line">print(&quot;train size&quot;, tr)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line">test_num = 0</span><br><span class="line">val_num = 0</span><br><span class="line">train_num = 0</span><br><span class="line">print(&#x27;total xml : &#123;&#125;&#x27;.format(total_xml))</span><br><span class="line"></span><br><span class="line">for i in list:</span><br><span class="line">    name = total_xml[i]</span><br><span class="line">    # print(&#x27;name : &#123;&#125;&#x27;.format(name))</span><br><span class="line">    if i in trainval:  # train and val set</span><br><span class="line">        if i in train:</span><br><span class="line">            directory = &quot;train&quot;</span><br><span class="line">            train_num += 1</span><br><span class="line">            xml_path = os.path.join(os.getcwd(), &#x27;&#123;&#125;&#x27;.format(directory))</span><br><span class="line">            if (not os.path.exists(xml_path)):</span><br><span class="line">                os.mkdir(xml_path)</span><br><span class="line">            filePath = os.path.join(xmlfilepath, name)</span><br><span class="line">            newfile = os.path.join(saveBasePath, os.path.join(directory, name))</span><br><span class="line">            # print(&#x27;newfile : &#123;&#125;&#x27;.format(newfile))</span><br><span class="line">            shutil.copyfile(filePath, newfile)</span><br><span class="line">        else:</span><br><span class="line">            directory = &quot;validation&quot;</span><br><span class="line">            xml_path = os.path.join(os.getcwd(), &#x27;&#123;&#125;&#x27;.format(directory))</span><br><span class="line">            if (not os.path.exists(xml_path)):</span><br><span class="line">                os.mkdir(xml_path)</span><br><span class="line">            val_num += 1</span><br><span class="line">            filePath = os.path.join(xmlfilepath, name)</span><br><span class="line">            newfile = os.path.join(saveBasePath, os.path.join(directory, name))</span><br><span class="line">            # print(&#x27;newfile : &#123;&#125;&#x27;.format(newfile))</span><br><span class="line">            shutil.copyfile(filePath, newfile)</span><br><span class="line">    else:</span><br><span class="line">        directory = &quot;test&quot;</span><br><span class="line">        xml_path = os.path.join(os.getcwd(), &#x27;&#123;&#125;&#x27;.format(directory))</span><br><span class="line">        if (not os.path.exists(xml_path)):</span><br><span class="line">            os.mkdir(xml_path)</span><br><span class="line">        test_num += 1</span><br><span class="line">        filePath = os.path.join(xmlfilepath, name)</span><br><span class="line">        newfile = os.path.join(saveBasePath, os.path.join(directory, name))</span><br><span class="line">        # print(&#x27;name : &#123;&#125;&#x27;.format(name))</span><br><span class="line">        shutil.copyfile(filePath, newfile)</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line">seconds = end - start</span><br><span class="line">print(&quot;train total : &quot; + str(train_num))</span><br><span class="line">print(&quot;validation total : &quot; + str(val_num))</span><br><span class="line">print(&quot;test total : &quot; + str(test_num))</span><br><span class="line">total_num = train_num + val_num + test_num</span><br><span class="line">print(&quot;total number : &quot; + str(total_num))</span><br><span class="line">print(&quot;Time taken : &#123;0&#125; seconds&quot;.format(seconds))</span><br></pre></td></tr></table></figure></li></ul><h4 id="voc-to-csv"><a href="#voc-to-csv" class="headerlink" title="voc to csv"></a>voc to csv</h4><ul><li>Transfer the xml files to csv for trian, test and validation folder individually.</li><li>You should change the save path for your own csv files.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">import pandas as pd</span><br><span class="line">import xml.etree.ElementTree as ET</span><br><span class="line"></span><br><span class="line">def xml_to_csv(path):</span><br><span class="line">    xml_list = []</span><br><span class="line">    for xml_file in glob.glob(path + &#x27;/*.xml&#x27;):</span><br><span class="line">        tree = ET.parse(xml_file)</span><br><span class="line">        root = tree.getroot()</span><br><span class="line"></span><br><span class="line">        print(root.find(&#x27;filename&#x27;).text)</span><br><span class="line">        for member in root.findall(&#x27;object&#x27;):</span><br><span class="line">            value = (root.find(&#x27;filename&#x27;).text,</span><br><span class="line">                int(root.find(&#x27;size&#x27;)[0].text),   #width</span><br><span class="line">                int(root.find(&#x27;size&#x27;)[1].text),   #height</span><br><span class="line">                member[0].text,</span><br><span class="line">                int(member[4][0].text),</span><br><span class="line">                int(float(member[4][1].text)),</span><br><span class="line">                int(member[4][2].text),</span><br><span class="line">                int(member[4][3].text)</span><br><span class="line">                )</span><br><span class="line">            xml_list.append(value)</span><br><span class="line">    column_name = [&#x27;filename&#x27;, &#x27;width&#x27;, &#x27;height&#x27;, &#x27;class&#x27;, &#x27;xmin&#x27;, &#x27;ymin&#x27;, &#x27;xmax&#x27;, &#x27;ymax&#x27;]</span><br><span class="line">    xml_df = pd.DataFrame(xml_list, columns=column_name)</span><br><span class="line">    return xml_df</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    for directory in [&#x27;train&#x27;,&#x27;test&#x27;,&#x27;validation&#x27;]:</span><br><span class="line">        xml_path = os.path.join(os.getcwd(), &#x27;./&#123;&#125;&#x27;.format(directory))</span><br><span class="line"></span><br><span class="line">        xml_df = xml_to_csv(xml_path)</span><br><span class="line">        # xml_df.to_csv(&#x27;whsyxt.csv&#x27;, index=None)</span><br><span class="line">        xml_df.to_csv(&#x27;/home/tim/workspace/models/research/object_detection/data/dove_cholo_&#123;&#125;_labels.csv&#x27;.format(directory), index=None)</span><br><span class="line">        print(&#x27;Successfully converted xml to csv.&#x27;)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure></li></ul><h4 id="csv-to-tfrecord"><a href="#csv-to-tfrecord" class="headerlink" title="csv to tfrecord"></a>csv to tfrecord</h4><ul><li>You should set your JPEGImage path.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on Tue Mar  5 15:28:55 2019</span><br><span class="line"></span><br><span class="line">@author: z</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Usage:</span><br><span class="line">  # From tensorflow/models/</span><br><span class="line">  # Create train data:</span><br><span class="line">  python generate_tfrecord.py --csv_input=data/tv_vehicle_labels.csv  --output_path=train.record</span><br><span class="line">  # Create test data:</span><br><span class="line">  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=test.record</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import io</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">from PIL import Image</span><br><span class="line">from object_detection.utils import dataset_util</span><br><span class="line">from collections import namedtuple, OrderedDict</span><br><span class="line"></span><br><span class="line">os.chdir(&#x27;/home/tim/workspace/models/research/&#x27;)</span><br><span class="line"></span><br><span class="line">flags = tf.app.flags</span><br><span class="line">flags.DEFINE_string(&#x27;csv_input&#x27;, &#x27;&#x27;, &#x27;Path to the CSV input&#x27;)</span><br><span class="line">flags.DEFINE_string(&#x27;output_path&#x27;, &#x27;&#x27;, &#x27;Path to output TFRecord&#x27;)</span><br><span class="line">FLAGS = flags.FLAGS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TO-DO replace this with label map</span><br><span class="line">def class_text_to_int(row_label):</span><br><span class="line">    # 你的所有类别, 必须从1开始，0被征用作为了背景。</span><br><span class="line">    if row_label == &#x27;cream_berry&#x27;:</span><br><span class="line">        return 1</span><br><span class="line">    elif row_label == &#x27;cream_cherry&#x27;:</span><br><span class="line">        return 2</span><br><span class="line">    elif row_label == &#x27;yida_cool_lemon&#x27;:</span><br><span class="line">        return 3</span><br><span class="line">    elif row_label == &#x27;box_yogurt_mango&#x27;:</span><br><span class="line">        return 4</span><br><span class="line">    elif row_label == &#x27;white_strawberry&#x27;:</span><br><span class="line">        return 5</span><br><span class="line">    elif row_label == &#x27;cookies_lemon&#x27;:</span><br><span class="line">        return 6</span><br><span class="line">    elif row_label == &#x27;yogurt_cranberry&#x27;:</span><br><span class="line">        return 7</span><br><span class="line">    elif row_label == &#x27;box_cookies_matcha&#x27;:</span><br><span class="line">        return 8</span><br><span class="line">    elif row_label == &#x27;cookies_matcha&#x27;:</span><br><span class="line">        return 9</span><br><span class="line">    elif row_label == &#x27;yogurt_mango&#x27;:</span><br><span class="line">        return 10</span><br><span class="line">    elif row_label == &#x27;white_passionfruit&#x27;:</span><br><span class="line">        return 11</span><br><span class="line">    elif row_label == &#x27;yida_cool_litchi&#x27;:</span><br><span class="line">        return 12</span><br><span class="line">    elif row_label == &#x27;box_white_strawberry&#x27;:</span><br><span class="line">        return 13</span><br><span class="line">    elif row_label == &#x27;cream_hazelnut&#x27;:</span><br><span class="line">        return 14</span><br><span class="line">    else:</span><br><span class="line">        return None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def split(df, group):</span><br><span class="line">    data = namedtuple(&#x27;data&#x27;, [&#x27;filename&#x27;, &#x27;object&#x27;])</span><br><span class="line">    gb = df.groupby(group)</span><br><span class="line">    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_tf_example(group, path):</span><br><span class="line">    with tf.gfile.GFile(os.path.join(path, &#x27;&#123;&#125;&#x27;.format(group.filename)), &#x27;rb&#x27;) as fid:</span><br><span class="line">        encoded_jpg = fid.read()</span><br><span class="line">    encoded_jpg_io = io.BytesIO(encoded_jpg)</span><br><span class="line">    image = Image.open(encoded_jpg_io)</span><br><span class="line">    width, height = image.size</span><br><span class="line"></span><br><span class="line">    filename = group.filename.encode(&#x27;utf8&#x27;)</span><br><span class="line">    image_format = b&#x27;jpg&#x27;</span><br><span class="line">    xmins = []</span><br><span class="line">    xmaxs = []</span><br><span class="line">    ymins = []</span><br><span class="line">    ymaxs = []</span><br><span class="line">    classes_text = []</span><br><span class="line">    classes = []</span><br><span class="line"></span><br><span class="line">    for index, row in group.object.iterrows():</span><br><span class="line">        xmins.append(row[&#x27;xmin&#x27;] / width)</span><br><span class="line">        xmaxs.append(row[&#x27;xmax&#x27;] / width)</span><br><span class="line">        ymins.append(row[&#x27;ymin&#x27;] / height)</span><br><span class="line">        ymaxs.append(row[&#x27;ymax&#x27;] / height)</span><br><span class="line">        classes_text.append(row[&#x27;class&#x27;].encode(&#x27;utf8&#x27;))</span><br><span class="line">        classes.append(class_text_to_int(row[&#x27;class&#x27;]))</span><br><span class="line"></span><br><span class="line">    tf_example = tf.train.Example(features=tf.train.Features(feature=&#123;</span><br><span class="line">        &#x27;image/height&#x27;: dataset_util.int64_feature(height),</span><br><span class="line">        &#x27;image/width&#x27;: dataset_util.int64_feature(width),</span><br><span class="line">        &#x27;image/filename&#x27;: dataset_util.bytes_feature(filename),</span><br><span class="line">        &#x27;image/source_id&#x27;: dataset_util.bytes_feature(filename),</span><br><span class="line">        &#x27;image/encoded&#x27;: dataset_util.bytes_feature(encoded_jpg),</span><br><span class="line">        &#x27;image/format&#x27;: dataset_util.bytes_feature(image_format),</span><br><span class="line">        &#x27;image/object/bbox/xmin&#x27;: dataset_util.float_list_feature(xmins),</span><br><span class="line">        &#x27;image/object/bbox/xmax&#x27;: dataset_util.float_list_feature(xmaxs),</span><br><span class="line">        &#x27;image/object/bbox/ymin&#x27;: dataset_util.float_list_feature(ymins),</span><br><span class="line">        &#x27;image/object/bbox/ymax&#x27;: dataset_util.float_list_feature(ymaxs),</span><br><span class="line">        &#x27;image/object/class/text&#x27;: dataset_util.bytes_list_feature(classes_text),</span><br><span class="line">        &#x27;image/object/class/label&#x27;: dataset_util.int64_list_feature(classes),</span><br><span class="line">    &#125;))</span><br><span class="line">    return tf_example</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(_):</span><br><span class="line">    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)</span><br><span class="line">    path = os.path.join(os.getcwd(), &#x27;object_detection/VOCdevkit/VOC2020_dove_cholo/JPEGImages/&#x27;)</span><br><span class="line">    examples = pd.read_csv(FLAGS.csv_input)</span><br><span class="line">    grouped = split(examples, &#x27;filename&#x27;)</span><br><span class="line">    num = 0</span><br><span class="line">    for group in grouped:</span><br><span class="line">        num += 1</span><br><span class="line">        tf_example = create_tf_example(group, path)</span><br><span class="line">        writer.write(tf_example.SerializeToString())</span><br><span class="line">        if (num % 100 == 0):  # 每完成100个转换，打印一次</span><br><span class="line">            print(num)</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line">    output_path = os.path.join(os.getcwd(), FLAGS.output_path)</span><br><span class="line">    print(&#x27;Successfully created the TFRecords: &#123;&#125;&#x27;.format(output_path))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure></li><li>command to generate tfrecord files<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd models/research/</span><br><span class="line"></span><br><span class="line">python generate_tfrecord.py --csv_input=object_detection/data/dove_cholo_test_labels.csv --output_path=dove_test.tfrecord</span><br><span class="line"></span><br><span class="line">python generate_tfrecord.py --csv_input=object_detection/data/dove_cholo_validation_labels.csv --output_path=dove_validation.tfrecord</span><br><span class="line"></span><br><span class="line">python generate_tfrecord.py --csv_input=object_detection/data/dove_cholo_train_labels.csv --output_path=dove_train.tfrecord</span><br></pre></td></tr></table></figure></li></ul><h3 id="Training-model"><a href="#Training-model" class="headerlink" title="Training model"></a>Training model</h3><ul><li>Things to prepare<ul><li>create your own label-map.pbtxt<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cd models/research/object_detection/data</span><br><span class="line">create label-map.pbtxt</span><br><span class="line">contents are belows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">item &#123;</span><br><span class="line">  id: 1    # id 从1开始编号</span><br><span class="line">  name: &#x27;red pedestrian&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">item &#123;</span><br><span class="line">  id: 2</span><br><span class="line">  name: &#x27;green pedestrian&#x27;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>model config file list<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">cd object_detection/samples/config/</span><br><span class="line"></span><br><span class="line">(base) tim@tim-System-Product-Name:~/workspace/models/research/object_detection/samples/configs$ tree</span><br><span class="line">.</span><br><span class="line">├── embedded_ssd_mobilenet_v1_coco.config</span><br><span class="line">├── facessd_mobilenet_v2_quantized_320x320_open_image_v4.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_coco.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_oid.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_oid_v4.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_pets.config</span><br><span class="line">├── faster_rcnn_inception_v2_coco.config</span><br><span class="line">├── faster_rcnn_inception_v2_pets.config</span><br><span class="line">├── faster_rcnn_nas_coco.config</span><br><span class="line">├── faster_rcnn_resnet101_atrous_coco.config</span><br><span class="line">├── faster_rcnn_resnet101_ava_v2.1.config</span><br><span class="line">├── faster_rcnn_resnet101_coco.config</span><br><span class="line">├── faster_rcnn_resnet101_fgvc.config</span><br><span class="line">├── faster_rcnn_resnet101_kitti.config</span><br><span class="line">├── faster_rcnn_resnet101_pets.config</span><br><span class="line">├── faster_rcnn_resnet101_voc07.config</span><br><span class="line">├── faster_rcnn_resnet152_coco.config</span><br><span class="line">├── faster_rcnn_resnet152_pets.config</span><br><span class="line">├── faster_rcnn_resnet50_coco.config</span><br><span class="line">├── faster_rcnn_resnet50_fgvc.config</span><br><span class="line">├── faster_rcnn_resnet50_pets.config</span><br><span class="line">├── mask_rcnn_inception_resnet_v2_atrous_coco.config</span><br><span class="line">├── mask_rcnn_inception_v2_coco.config</span><br><span class="line">├── mask_rcnn_resnet101_atrous_coco.config</span><br><span class="line">├── mask_rcnn_resnet101_pets.config</span><br><span class="line">├── mask_rcnn_resnet50_atrous_coco.config</span><br><span class="line">├── rfcn_resnet101_coco.config</span><br><span class="line">├── rfcn_resnet101_pets.config</span><br><span class="line">├── ssd_inception_v2_coco.config</span><br><span class="line">├── ssd_inception_v2_pets.config</span><br><span class="line">├── ssd_inception_v3_pets.config</span><br><span class="line">├── ssdlite_mobilenet_edgetpu_320x320_coco.config</span><br><span class="line">├── ssdlite_mobilenet_edgetpu_320x320_coco_quant.config</span><br><span class="line">├── ssdlite_mobilenet_v1_coco.config</span><br><span class="line">├── ssdlite_mobilenet_v2_coco.config</span><br><span class="line">├── ssdlite_mobilenet_v3_large_320x320_coco.config</span><br><span class="line">├── ssdlite_mobilenet_v3_small_320x320_coco.config</span><br><span class="line">├── ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_coco.config</span><br><span class="line">├── ssd_mobilenet_v1_focal_loss_pets.config</span><br><span class="line">├── ssd_mobilenet_v1_focal_loss_pets_inference.config</span><br><span class="line">├── ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_pets.config</span><br><span class="line">├── ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_quantized_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v2_coco.config</span><br><span class="line">├── ssd_mobilenet_v2_fpnlite_quantized_shared_box_predictor_256x256_depthmultiplier_75_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v2_fullyconv_coco.config</span><br><span class="line">├── ssd_mobilenet_v2_oid_v4.config</span><br><span class="line">├── ssd_mobilenet_v2_pets_keras.config</span><br><span class="line">├── ssd_mobilenet_v2_quantized_300x300_coco.config</span><br><span class="line">├── ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config</span><br><span class="line">└── ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config</span><br><span class="line"></span><br><span class="line">0 directories, 57 files</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>Custom your own model config, ssd_moblienet_v1_coco.config for example</li><li>Open it and change the code.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"># SSD with Mobilenet v1 configuration for MSCOCO Dataset.</span><br><span class="line"># Users should configure the fine_tune_checkpoint field in the train config as</span><br><span class="line"># well as the label_map_path and input_path fields in the train_input_reader and</span><br><span class="line"># eval_input_reader. Search for &quot;PATH_TO_BE_CONFIGURED&quot; to find the fields that</span><br><span class="line"># should be configured.</span><br><span class="line"></span><br><span class="line">model &#123;</span><br><span class="line">  ssd &#123;</span><br><span class="line">    num_classes: 14  ## change here</span><br><span class="line">    box_coder &#123;</span><br><span class="line">      faster_rcnn_box_coder &#123;</span><br><span class="line">        y_scale: 10.0</span><br><span class="line">        x_scale: 10.0</span><br><span class="line">        height_scale: 5.0</span><br><span class="line">        width_scale: 5.0</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    matcher &#123;</span><br><span class="line">      argmax_matcher &#123;</span><br><span class="line">        matched_threshold: 0.5</span><br><span class="line">        unmatched_threshold: 0.5</span><br><span class="line">        ignore_thresholds: false</span><br><span class="line">        negatives_lower_than_unmatched: true</span><br><span class="line">        force_match_for_each_row: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    similarity_calculator &#123;</span><br><span class="line">      iou_similarity &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    anchor_generator &#123;</span><br><span class="line">      ssd_anchor_generator &#123;</span><br><span class="line">        num_layers: 6</span><br><span class="line">        min_scale: 0.2</span><br><span class="line">        max_scale: 0.95</span><br><span class="line">        aspect_ratios: 1.0</span><br><span class="line">        aspect_ratios: 2.0</span><br><span class="line">        aspect_ratios: 0.5</span><br><span class="line">        aspect_ratios: 3.0</span><br><span class="line">        aspect_ratios: 0.3333</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    image_resizer &#123;</span><br><span class="line">      fixed_shape_resizer &#123;</span><br><span class="line">        height: 300</span><br><span class="line">        width: 300</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    box_predictor &#123;</span><br><span class="line">      convolutional_box_predictor &#123;</span><br><span class="line">        min_depth: 0</span><br><span class="line">        max_depth: 0</span><br><span class="line">        num_layers_before_predictor: 0</span><br><span class="line">        use_dropout: false</span><br><span class="line">        dropout_keep_probability: 0.8</span><br><span class="line">        kernel_size: 1</span><br><span class="line">        box_code_size: 4</span><br><span class="line">        apply_sigmoid_to_scores: false</span><br><span class="line">        conv_hyperparams &#123;</span><br><span class="line">          activation: RELU_6,</span><br><span class="line">          regularizer &#123;</span><br><span class="line">            l2_regularizer &#123;</span><br><span class="line">              weight: 0.00004</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          initializer &#123;</span><br><span class="line">            truncated_normal_initializer &#123;</span><br><span class="line">              stddev: 0.03</span><br><span class="line">              mean: 0.0</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          batch_norm &#123;</span><br><span class="line">            train: true,</span><br><span class="line">            scale: true,</span><br><span class="line">            center: true,</span><br><span class="line">            decay: 0.9997,</span><br><span class="line">            epsilon: 0.001,</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    feature_extractor &#123;</span><br><span class="line">      type: &#x27;ssd_mobilenet_v1&#x27;</span><br><span class="line">      min_depth: 16</span><br><span class="line">      depth_multiplier: 1.0</span><br><span class="line">      conv_hyperparams &#123;</span><br><span class="line">        activation: RELU_6,</span><br><span class="line">        regularizer &#123;</span><br><span class="line">          l2_regularizer &#123;</span><br><span class="line">            weight: 0.00004</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        initializer &#123;</span><br><span class="line">          truncated_normal_initializer &#123;</span><br><span class="line">            stddev: 0.03</span><br><span class="line">            mean: 0.0</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        batch_norm &#123;</span><br><span class="line">          train: true,</span><br><span class="line">          scale: true,</span><br><span class="line">          center: true,</span><br><span class="line">          decay: 0.9997,</span><br><span class="line">          epsilon: 0.001,</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    loss &#123;</span><br><span class="line">      classification_loss &#123;</span><br><span class="line">        weighted_sigmoid &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      localization_loss &#123;</span><br><span class="line">        weighted_smooth_l1 &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      hard_example_miner &#123;</span><br><span class="line">        num_hard_examples: 3000</span><br><span class="line">        iou_threshold: 0.99</span><br><span class="line">        loss_type: CLASSIFICATION</span><br><span class="line">        max_negatives_per_positive: 3</span><br><span class="line">        min_negatives_per_image: 0</span><br><span class="line">      &#125;</span><br><span class="line">      classification_weight: 1.0</span><br><span class="line">      localization_weight: 1.0</span><br><span class="line">    &#125;</span><br><span class="line">    normalize_loss_by_num_matches: true</span><br><span class="line">    post_processing &#123;</span><br><span class="line">      batch_non_max_suppression &#123;</span><br><span class="line">        score_threshold: 1e-8</span><br><span class="line">        iou_threshold: 0.6</span><br><span class="line">        max_detections_per_class: 100</span><br><span class="line">        max_total_detections: 100</span><br><span class="line">      &#125;</span><br><span class="line">      score_converter: SIGMOID</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">train_config: &#123;</span><br><span class="line">  batch_size: 24   ## change here</span><br><span class="line">  optimizer &#123;</span><br><span class="line">    rms_prop_optimizer: &#123;</span><br><span class="line">      learning_rate: &#123;</span><br><span class="line">        exponential_decay_learning_rate &#123;</span><br><span class="line">          initial_learning_rate: 0.0004</span><br><span class="line">          decay_steps: 800720</span><br><span class="line">          decay_factor: 0.95</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      momentum_optimizer_value: 0.9</span><br><span class="line">      decay: 0.9</span><br><span class="line">      epsilon: 1.0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  fine_tune_checkpoint: &quot;object_detection/finetune_cpkt/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt&quot;   ## change here</span><br><span class="line">  from_detection_checkpoint: true</span><br><span class="line">  # Note: The below line limits the training process to 200K steps, which we</span><br><span class="line">  # empirically found to be sufficient enough to train the pets dataset. This</span><br><span class="line">  # effectively bypasses the learning rate schedule (the learning rate will</span><br><span class="line">  # never decay). Remove the below line to train indefinitely.</span><br><span class="line">  num_steps: 10000    ## change here</span><br><span class="line">  data_augmentation_options &#123;</span><br><span class="line">    random_horizontal_flip &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  data_augmentation_options &#123;</span><br><span class="line">    ssd_random_crop &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">train_input_reader: &#123;</span><br><span class="line">  tf_record_input_reader &#123;</span><br><span class="line">    input_path: &quot;object_detection/data/dove_train.tfrecord&quot;   ## change here</span><br><span class="line">  &#125;</span><br><span class="line">  label_map_path: &quot;object_detection/data/dove_cholo_label_map.pbtxt&quot;   ## change here</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">eval_config: &#123;</span><br><span class="line">  num_examples: 3438    ## change here</span><br><span class="line">  # Note: The below line limits the evaluation process to 10 evaluations.</span><br><span class="line">  # Remove the below line to evaluate indefinitely.</span><br><span class="line">  max_evals: 10</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">eval_input_reader: &#123;</span><br><span class="line">  tf_record_input_reader &#123;</span><br><span class="line">    input_path: &quot;object_detection/data/dove_validation.tfrecord&quot;   ## change here</span><br><span class="line">  &#125;</span><br><span class="line">  label_map_path: &quot;object_detection/data/dove_cholo_label_map.pbtxt&quot;   ## change here</span><br><span class="line">  shuffle: false</span><br><span class="line">  num_readers: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>Download the pre-trained model<br>  <a class="link"   href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" >modle zoo<i class="fas fa-external-link-alt"></i></a></li><li>fine_tune_checkpoint: “object_detection&#x2F;finetune_cpkt&#x2F;ssd_mobilenet_v1_coco_2018_01_28&#x2F;model.ckpt”   ## change here</li></ul></li></ul><h4 id="legacy-training-同时跑train-py和eval-py"><a href="#legacy-training-同时跑train-py和eval-py" class="headerlink" title="legacy training (同时跑train.py和eval.py)"></a>legacy training (同时跑train.py和eval.py)</h4><ul><li><p>旧的训练方法，path,  &#x2F;models&#x2F;research&#x2F;object_detection&#x2F;legacy&#x2F;train.py</p></li><li><p>旧的训练方法，path,  &#x2F;models&#x2F;research&#x2F;object_detection&#x2F;legacy&#x2F;eval.py</p></li><li><p><a class="link"   href="https://blog.csdn.net/zong596568821xp/article/details/84842688" >ref-eval的使用<i class="fas fa-external-link-alt"></i></a></p></li><li><p>–logtostderr, 日志保存</p></li><li><p>–train_dir, 训练模型保存的位置</p></li><li><p>–pipeline_config_path, 模型配置文件的路径</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">## 可用GPU训练，但常常会cuda out of memory</span><br><span class="line">## 先在trian.py和eval.py中加入以下代码控制gpu的内存使用</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"> </span><br><span class="line">os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;</span><br><span class="line">config = tf.ConfigProto(allow_soft_placement = True)</span><br><span class="line">gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.35)</span><br><span class="line">config.gpu_options.allow_growth = True</span><br><span class="line"> </span><br><span class="line">sess0 = tf.InteractiveSession(config = config)</span><br><span class="line"></span><br><span class="line"># 原文链接：https://blog.csdn.net/baidu_33597755/article/details/102311000</span><br><span class="line"></span><br><span class="line">cd models/research/</span><br><span class="line"></span><br><span class="line">python object_detection/legacy/train.py \</span><br><span class="line">    --pipeline_config_path=object_detection/dove_cholo_od/config/ssd_mobilenet_v2_coco.config \</span><br><span class="line">    --train_dir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_train  \</span><br><span class="line">    --alsologtostderr</span><br><span class="line">    </span><br><span class="line"># 等train.py跑了一会之后，再运行eval.py</span><br><span class="line"></span><br><span class="line">python object_detection/legacy/eval.py \</span><br><span class="line">    --pipeline_config_path=object_detection/dove_cholo_od/config/ssd_mobilenet_v2_coco.config \</span><br><span class="line">    --checkpoint_dir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_train  \</span><br><span class="line">    --eval_dir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_eval  \</span><br><span class="line">    --logtostderr</span><br><span class="line">    </span><br></pre></td></tr></table></figure></li><li><p>Then open the tensorboard to watch the training and eval progress</p></li><li><p>Open two tensorboard at the same time</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_train --port=6005</span><br><span class="line"></span><br><span class="line">tensorboard --logdir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_eval</span><br></pre></td></tr></table></figure><h4 id="modern-training-暂不支持GPU"><a href="#modern-training-暂不支持GPU" class="headerlink" title="modern training(暂不支持GPU)"></a>modern training(暂不支持GPU)</h4><ul><li>新的训练方法，path,   &#x2F;models&#x2F;research&#x2F;object_detection&#x2F;model_main.py<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># From the tensorflow/models/research/ directory</span><br><span class="line">python object_detection/model_main.py \</span><br><span class="line">    --pipeline_config_path=object_detection/training/ssd_mobilenet_v1_coco.config \</span><br><span class="line">    --model_dir=object_detection/training \</span><br><span class="line">    --num_train_steps=50000 \</span><br><span class="line">    --num_eval_steps=2000 \</span><br><span class="line">    --alsologtostderr</span><br></pre></td></tr></table></figure></li></ul><h3 id="Model-evaluation"><a href="#Model-evaluation" class="headerlink" title="Model evaluation"></a>Model evaluation</h3><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. ImportError: cannot import name &#x27;input_reader_pb2&#x27; from &#x27;object_detection.protos&#x27;</span><br><span class="line">solution:</span><br><span class="line"># From tensorflow/models/research/</span><br><span class="line">protoc object_detection/protos/*.proto --python_out=.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2. from nets import inception_resnet_v2 ModuleNotFoundError: No module named &#x27;nets&#x27;</span><br><span class="line">solution:</span><br><span class="line">cd model/research/</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cd model/research/slim/</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3. Not found: PATH_TO_BE_CONFIGURED; No such file or directory</span><br><span class="line">solution:</span><br><span class="line">download pre-trained cpkt model</span><br><span class="line">go into the config file and Search for &quot;PATH_TO_BE_CONFIGURED&quot; to find the fields that should be configured.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4. No module named &#x27;pycocotools&#x27;</span><br><span class="line">pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;System-Info&quot;&gt;&lt;a href=&quot;#System-Info&quot; class=&quot;headerlink&quot; title=&quot;System Info&quot;&gt;&lt;/a&gt;System Info&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;Git&lt;/l</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="tensorflow" scheme="http://example.com/tags/tensorflow/"/>
    
    <category term="目标检测" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>可视化证明：神经网络可以计算任何函数</title>
    <link href="http://example.com/2017/06/14/visual-proof-of-neural-networks/"/>
    <id>http://example.com/2017/06/14/visual-proof-of-neural-networks/</id>
    <published>2017-06-14T09:49:20.000Z</published>
    <updated>2023-07-16T01:37:48.995Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><a class="link"   href="http://neuralnetworksanddeeplearning.com/chap4.html" >Original Post<i class="fas fa-external-link-alt"></i></a></p></li><li><p>神经网络当中一个最显著的事实是：神经网络能够计算任何的函数。也就是说，假如有人给你一个复杂的，波形弯弯曲曲的函数，$f(x):$</p><img src="/img/visual_proof_of_neural_networks/1_function.png" class="[class names]" title="[Function $f(x)$ []]"></li><li><p>无论是什么的函数，我们都可以保证能找到一个网络，对于所有可能的输入，$x$，$f(x)$的值(或者其他的逼近)就是网络的输出，譬如：</p><img src="/img/visual_proof_of_neural_networks/2_network.png" class="[class names]" title="[Neural Network []]"></li><li><p>这个网络即使是网络的输入很多的时候也是成立的。$f&#x3D;f(x_1,…,x_m)$，然后还有很多的输出。譬如，以下就是一个网络，计算的函数，输入$m&#x3D;3$和输出$n&#x3D;2$：</p><img src="/img/visual_proof_of_neural_networks/3_network.png" class="[class names]" title="[Neural Network []]"></li><li><p>以上的网络告诉我们神经网络是有一种泛化性的。无论我们想要计算任何的函数，肯定存在一个网络可以满足我们的需求。</p></li><li><p>另外，更进一步，即使这个网络限制到只有一层隐含层，这个泛化理论也是成立的。这个网络就是所谓的单层隐含层。所以，即使是很简单的网络也是很厉害的。</p></li><li><p>搞神经网络的人都应该熟知这个泛化性理论，可是事实上它并没有让很多人能理解。大多数的解释都是偏技术性的。譬如，其中一篇原始的论文利用$Hahn-Banach$理论，$Riesz Representation$理论和一些傅里叶分析来证明这个泛化性理论。如果你不是一个数学家的话，你很难读懂里面的解释。很遗憾，这样很多人就很难懂这个泛化性理论了。但是其实背后的原理是非常简单和美好的。</p></li><li><p>在本章中，我会给出一个关于泛化性理论的简单的和非常可视化的解释。我们会循序渐进，一步一步来理解这个idea。你将会明白为什么神经网络能够计算任意的函数。你也会理解到这个理论的一些限制的地方。你会理解到这个理论和深度学习之间的关系。</p></li><li><p>要理解这章的内容，你并不需要读前面的内容。反而这一章是一个结构乐见和完整的文章。我会给你们讲一些神经网络的基本知识，这样你们会更容易看懂解释。我会提供一些临时的链接，以防填补你某一部分知识的缺口。</p></li><li><p>泛化性理论是计算机科学中一个老生常谈的东西，以至于我们有时候都忘记了它到底有多厉害了。但是我们应该时刻提醒自己：有能力去计算任意的函数真的是一件很厉害的事情。几乎你可以想象得到的任何进程都可以被认为是一个函数的计算。譬如，根据一些短的音乐片段去给一段音乐命名。这也是看作计算一个函数。又或者把中文翻译成英文，又或者给计算机一个mp4视频资料，它会生成一个关于视频的描绘图还有这个视频的拍摄质量。泛化性意味着，原则上，神经网络可以做任何的事情而且更多。</p></li><li><p>当然，仅仅因为我们知道存在一个网络能够把中文翻译成英文，那不意味着我们就很容易构造甚至识别出这个网络。这样的约束对于模型的传统的泛化性理论，譬如布尔回路，都是适用的。但是，像这本书前面所说的，神经网络对于学习函数有非常强的算法。学习算法和泛化性是一个绝妙的结合。直到现在，这本书还是关注在学习算法。在这一章，我们关注在泛化性理论，和它所代表的意思。</p></li></ul><h2 id="两个说明"><a href="#两个说明" class="headerlink" title="两个说明"></a>两个说明</h2><ul><li><p>在我们解释为什么泛化性理论成立之前，我想要提两个关于“一个神经网络能够计算任何的函数”的说明。</p></li><li><p>首先，并不是说一个神经网络被用来准确地计算任何的函数。而是，我们能够得到一个关于这个函数的一个很好的逼近。通过增加隐层神经元的个数，我们可以不断改进逼近的程度。譬如，前面我们谈到的神经网络，包含一个隐层。对于很多函数，只有3个隐层神经元的神经网络是一个并不好的逼近。通过增加隐层的神经元(假如，增加到5)，我们可以得到更好的逼近：</p><img src="/img/visual_proof_of_neural_networks/4_network.png" class="[class names]" title="[Neural Network []]"></li><li><p>如果我们继续增加神经元的个数，这个逼近就能做得更好。</p></li><li><p>更精确的说，假如给定我们一个函数$f(x)$，我们想要计算它的误差$\epsilon &gt; 0$。只要我们增加足够的隐层神经元，我们就能找到一个神经网络，它的输出$g(x)$满足$g(x)-f(x) &lt; \epsilon$,对于所有的输入$x$。换句话说，这个逼近函数$g(x)$对于所有可能的输入都在可预计的准确度范围之内。</p></li><li><p>第二个说明是能够用逼近的方式的函数类型是连续函数。如果一个函数是不连续的，譬如，突然断开的，或者跳动的函数，然后是没法用神经网络来进行逼近的。这并没有什么奇怪的，因为我们的网络计算输入的连续函数。然而，即使我们想要计算不连续的函数，但是利用连续的逼近来的效果更好。既然这样，那么我们就可以用神经网络。实际上，这并不是一个很重要的限制。</p></li><li><p>总结一下，关于泛化性理论的一个精确的说法是一个单隐层的神经网络能够用来逼近任何的连续函数到任何想要的精度。在本章中，我们实际上证明了一个微弱版本的理论，用两个隐层的神经网络代替单隐层的神经网络。在以下的我们将要解释的问题中，利用一些小技巧，适合的给出了单隐层的神经网络的证明。</p></li></ul><h2 id="一个输入和一个输出的泛化理论"><a href="#一个输入和一个输出的泛化理论" class="headerlink" title="一个输入和一个输出的泛化理论"></a>一个输入和一个输出的泛化理论</h2><ul><li><p>要理解为什么泛化性理论是可行的，让我们从理解如何构造一个能够逼近函数的只包含一个输入和一个输出的神经网络说起：</p><img src="/img/visual_proof_of_neural_networks/5_network.png" class="[class names]" title="[Neural Network []]"></li><li><p>其实最简单的神经网络(包含一个输入和一个输出)就是泛化性理论的核心。一旦我们能够理解这个简单的网络，那么我们也很容易扩展到其他的(包含多个输入和输出)复杂网络了。</p></li><li><p>想要深入理解如果构造能够计算函数$f$的网络，我们先来构造一个简单的神经网络，包含单个输入，一个包含2个神经元的隐层，单个输出的神经网络：</p></li></ul><img src="/img/visual_proof_of_neural_networks/6_network.png" class="[class names]" title="[Neural Network []]"><ul><li>为了感受一下神经网络中的组件是如何工作的，让我们先看上面的隐层神经元。在下面的图中，用鼠标点击然后拖动来改变权重$w$的值。你就可以立刻看到右边的函数图是如何变化的(原网页才能操作)：<img src="/img/visual_proof_of_neural_networks/7_gif_pic.gif" class="[class names]" title="[Neural Network []]"></li><li>本书的前面讲过，$\sigma(wx+b)$是怎么计算的，其中$\sigma(z)&#x3D;1&#x2F;(1+e^{-z})$就是sigmoid函数。到现在，我们一直频繁的使用这种代数形式进行计算。但是要证明泛化性理论，我们应该忽视这种数学形式，然后通过通过操作和观察以上的图来获得更深的理解。</li><li>要开始证明这个理论，请试着用鼠标点击偏置$b$，然后往右拉来增加它的数值。你会看到增加偏置$b$只会把图像往左移动，而不会改变图像的形状。</li><li>接下来，你试着鼠标往左拉，偏置$b$的数值减小，你会看到图像是往右边移动的，同样的，图像的形状并没有改变。</li><li>然后，我们来改变权重$w$的数值，把它的值改变到2或者3。你会看到减小权重$w$，曲线变得更宽了。你可能需要同时改变一下偏置$b$，以免图像跑出了框内。</li><li>最后，把权重$w$的数值增加到100。你会发现，曲线变得更陡峭了，直到它看起来像一个step函数。可以观察一下下面的小视频：</li></ul><img src="/img/visual_proof_of_neural_networks/8_gif_pic.gif" class="[class names]" title="[Neural Network []]"><ul><li>我们可以通过增加权重$w$直到它变成了一个step函数(逼近的精度越来越高)来简化我们的分析。下面右边的图像是当$w&#x3D;999$时的函数图。</li></ul><img src="/img/visual_proof_of_neural_networks/9_network.png" class="[class names]" title="[Neural Network []]"><ul><li><p>其实step函数比一般的sigmoid函数表现更好。原因是输出层是前面所有的隐层的神经元计算来决定的。去分析一堆step函数的总和是件更容易的事，但是去分析一堆sigmoid函数的曲线计算后的结果就不是一件易事了。而且，把隐层的神经元输出step函数是一件更容易的事情。更确切的说，我们把权重$w$的值调得非常大，这样就可以得到了step函数了。然后通过调整偏置$b$来改变step(跃阶)的位置。当然，把输出当成一个step函数也是一种逼近，而且是一个很好的逼近，而现在我们正是这样做的。我稍后会再讨论这种逼近的求导的影响。</p></li><li><p>当$x$的数值是多少时达到跃阶(step)呢？换一种说法，step的位置是如何由权重$w$和偏置$b$来决定的？</p></li><li><p>要回答这个问题，我们再来试一下改变权重$w$和偏置$b$的数值。你可以弄清楚跃阶(step)和权重$w$与偏置$b$的关系吗？通过不断的观察图像的变化，你可能就会发现，跃阶(step)的位置是和偏置$b$成正比的，和权重$w$成反比的。</p></li><li><p>事实上，跃阶(step)的位置$s$满足$s&#x3D;-b&#x2F;w$.</p><img src="/img/visual_proof_of_neural_networks/10_gif_pic.gif" class="[class names]" title="[Neural Network []]"></li><li><p>这会大大简化了我们的生活，如果我们只是单单利用一个参数$s$来描述隐层的神经元的话。$s$就是跃阶(step)的位置，$s&#x3D;-b&#x2F;w$。</p><img src="/img/visual_proof_of_neural_networks/11_gif_pic.gif" class="[class names]" title="[Neural Network []]"></li><li><p>上面有提到，我们可以偷偷地把输入的权重$w$调到非常大，大到出来的step函数是一个很好的逼近。我们就可以很容易的在传统的模型当中把神经元参数按这样子来调整，偏置$b&#x3D;-ws$.</p></li><li><p>以上我们都是讨论上层神经元。现在我们来讨论一下整个网络。我们假设上层神经元通过step函数参数化跃阶(step)为$s_1$，下层神经元通过step函数参数化跃阶(step)为$s_2$.他们分别有输出权重$w_1$和$w_2$。以下就是网络：</p><img src="/img/visual_proof_of_neural_networks/12_gif_pic.gif" class="[class names]" title="[Neural Network []]"></li><li><p>右上方的图像是输出权重$(w_1a_1+w_2a_2)$的结果。其中$a_1$和$a_2$分别是上层神经元和下层神经元的输出。输出用$a$来表示是因为神经元通常包含一个激活函数(activation)。</p></li><li><p>让我们来看一下$s_1$和$s_2$相遇时会发生什么情况呢？看图：</p><img src="/img/visual_proof_of_neural_networks/13_gif_pic.gif" class="[class names]" title="[Neural Network []]"></li><li><p>可以看到，当我们操作$s_1$时，$S_1$和$s_2$相遇时，$S_1$就会带动$s_2$走。所以，图像的变动，我们也要分是$s_1$先动还是$s_2$先动。</p></li><li><p>$h_t$</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a class=&quot;link&quot;   href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html&quot; &gt;Original Post&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="可视化证明" scheme="http://example.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络 Part 1-简介</title>
    <link href="http://example.com/2017/03/29/RNN-tutorial-Part-1-Introduction/"/>
    <id>http://example.com/2017/03/29/RNN-tutorial-Part-1-Introduction/</id>
    <published>2017-03-29T13:02:30.000Z</published>
    <updated>2023-07-16T01:44:09.384Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>本文翻自<a class="link"   href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" >WILDML<i class="fas fa-external-link-alt"></i></a></p></li><li><p>循环神经网络(RNNs)是一个目前在许多的自然语言处理(NLP)任务当中表现了出色的性能的模型。但是，除了它最近的火热之外，我能找到的关于RNNs模型的工作原理和实现的资源非常的有限。所以我才着手写了这个tutorial。我分了几个部分来写RNN的tutorial：</p><ol><li>RNNs简介(本tutorial)</li><li>用Python和Theano实现RNN</li><li>理解定时后向传播算法和梯度消失的问题</li><li>实现一个GRU&#x2F;LSTM RNN</li></ol></li><li><p>在本tutorial中我们实现了一个基于语言模型的RNN。这个语言模型应用包括两个部分：第一，它允许我们对一个可能出现在现实当中的抽象句子做一个评分，这个分数可以用来评判句子的语法和语义的准确性。这样的模型是典型的机器翻译系统当中的一个部分。第二，一个语言模型允许我们生成一个新的文本(我认为这是一个非常cool的应用)。在Shakespeare(莎士比亚)文章上训练一个语言模型，它可以生成一个新的类莎士比亚的文本。<a class="link"   href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" >Andrej Karpathy写的贴子<i class="fas fa-external-link-alt"></i></a>很好地阐明了基于RNN的字符level的语言模型能够干什么。</p></li><li><p>我假设你们对于基本的神经网络都熟悉了。如果你们并不熟悉的话，你可以先去看一下这个贴子,<a class="link"   href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" >从零开始实现一个神经网络<i class="fas fa-external-link-alt"></i></a>，它会指导你非RNN背后神经网络的思想和实现。</p></li></ul><h2 id="什么是RNNs"><a href="#什么是RNNs" class="headerlink" title="什么是RNNs?"></a>什么是RNNs?</h2><ul><li><p>RNN的核心思想是利用序列信息。在传统的神经网络中，我们假设所有的inputs和outputs都是彼此独立的。但是在很多的任务当中，这是一个非常不合理的想法。如果你想预测一个句子当中的下一个单词，你最好能知道它前面跟着的是什么单词。RNNs当中的”recurrent”，递归，是因为它对于序列当中的每一个元素都执行了同样的任务，当前的output和之前的计算有依赖关系。RNNs的另一种理解就是它有一个“记忆体”记住了到目前为止所计算的信息。理论上RNNs能够利用任意长序列上的信息，但是实际上它们只是局限于能回看前面几步上的信息(以后会更多)。这就是一个典型的RNN模型图：</p></li><li><p><img src="/img/tf_tutorial/05_RNN/rnn.jpg" title="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg"></p></li><li><p>上面这张图就是RNN展开成全连接的网络图。展开的意思就是我们把所有的序列写出来。譬如，如果我们关注的序列是包含5个单词的，那么展开的网络图就是5层的网络，每一层对应一个单词。图中的公式参数意义如下：</p><ul><li>$x_t$是在时间$t$时的输入。譬如，$x_1$可以是序列中对应到第二个单词的one_hot向量。($X_0$是第一个)</li><li>$s_t$是在时间$t$时的隐层状态。它是网络的“记忆体”。$s_t$是基于前一个隐层状态和当前的输入计算出来的:</li><li><img src="/img/tf_tutorial/05_RNN/eq1.jpg"> 这个函数f通常是一个非线性函数，如tanh或者ReLU。$s_{-1}$是用来计算第一个隐层的，通常初始化为全0。</li><li>$o_t$是在步骤$t$时的输出。譬如，如果我们想去预测句子当中的下一个单词，结果会是一个概率向量，它对应着我们字典中的每一个单词。$o_t&#x3D;softmax(V*s_t)$</li></ul></li><li><p>以下是一些我们需要注意的地方：</p><ul><li>你可以把$s_t$看作是网络的记忆体。$s_t$捕获了前一步所发生的信息。输出$o_t$是单独基于时间$t$时的记忆算出来的。像以上简单提到的，实际上它有点复杂，因为$s_t$一般不能够捕获太多次之前的信息。</li><li>不想传统的深度学习网络，每一层都用了不同的参数，RNN当中的每一层是共享一组参数的($U,V,W$)。这也说明了我们每一层都是做了相同的操作的，只是每次的inputs不同了。这就大大减少了我们要学习的参数的数量。</li><li>上面的图当中每一层都有一个输出，但是有些任务当中这些输出并不是必须的。譬如，在做句子的情感分析当中，我们可能就是需要最后一个输出而已，而不是每一层的outputs。简单来说，我们不是每一层都需要输入。RNN的主要特点是它的的隐层状态，它捕获了序列当中的一些信息。</li></ul></li></ul><h2 id="RNN能够干什么？"><a href="#RNN能够干什么？" class="headerlink" title="RNN能够干什么？"></a>RNN能够干什么？</h2><ul><li>RNN在许多的自然语言处理(NLP)任务中当中获得非常好的结果。这里我不得不提到的一个常用到的RNN模型就是<a class="link"   href="https://en.wikipedia.org/wiki/Long_short_term_memory" >LSTMs<i class="fas fa-external-link-alt"></i></a>。相比普通的RNN模型，它能更好的捕获到长期的依赖信息。但是不要担心，LSTMs也是我们这个tutorial中的RNN差不多，只是它的隐层状态的计算会有所不同。我们会在下一个贴子当中详解LSTMs。这里我们只是列举一些RNN在NLP当中应用的例子。</li></ul><h2 id="语言建模和生成文本"><a href="#语言建模和生成文本" class="headerlink" title="语言建模和生成文本"></a>语言建模和生成文本</h2><ul><li>给定一个序列的单词，我们要预测当给定前一个单词时，下一个单词会出现的概率。语言模型会让我们评判一个什么序列的单词如何才可能是一个句子。这也是机器翻译当中的一个重要的输入(通常概率高的句子都是正确的)。预测下一个单词的另一个作用是我们会得到一个生成模型，这个模型可以通过从输出概率当中进行采样来生成新的文本。利用我们的训练数据，我们可以生成各种各样的单词序列。在语言模型当中，我们的输入通常是一个单词的序列(譬如加密成一个one-hot向量)，然后我们的输出就是预测的单词的序列。当我们训练网络时，我们把$o_t&#x3D;x_{t+1}$，因为我们希望时间$t$时的输出是真实的下一个单词。</li><li>语言模型和生成文本的相关论文：<ul><li><a class="link"   href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" >Recurrent neural network based language model<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf" >Extensions of Recurrent neural network based language model<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf" >Generating Text with Recurrent Neural Networks<i class="fas fa-external-link-alt"></i></a></li></ul></li></ul><h2 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h2><ul><li>机器模型和语言模型非常的相似，它的输入也是一种语种的单词序列(譬如德语)。我们想要输出的是目标语种的单词序列(譬如英语)。一个主要的不同就是我们的输出是当所有的输入都计算过了才开始的，因为我们翻译的句子中的第一个单词是需要所有的输入序列的信息才能确定的。</li><li><img src="/img/tf_tutorial/05_RNN/RNN_for_machine_translation.png" title="RNN for Machine Translation. Image Source: http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf"></li><li>机器翻译的相关论文：<ul><li><a class="link"   href="http://www.aclweb.org/anthology/P14-1140.pdf" >A Recursive Recurrent Neural Network for Statistical Machine Translation<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" >Sequence to Sequence Learning with Neural Networks<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="http://research.microsoft.com/en-us/um/people/gzweig/Pubs/EMNLP2013RNNMT.pdf" >Joint Language and Translation Modeling with Recurrent Neural Networks<i class="fas fa-external-link-alt"></i></a></li></ul></li></ul><h2 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h2><ul><li>给定一个从声波中得到的声学信号的输入序列，我们能够利用它们的概率来预测一个序列的语音片段。</li><li>语音识别的相关论文：<ul><li><a class="link"   href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" >Towards End-to-End Speech Recognition with Recurrent Neural Networks<i class="fas fa-external-link-alt"></i></a></li></ul></li></ul><h2 id="生成图片描述"><a href="#生成图片描述" class="headerlink" title="生成图片描述"></a>生成图片描述</h2><ul><li>结合卷积神经网络(CNN)和RNN的模型能够对没标签的图片生成描述。这是一个非常惊人的工作。这个组合模型还能把图片当中找到的特征和生成的单词一一对应。</li><li><img src="/img/tf_tutorial/05_RNN/cnn-rnn.png" title="Deep Visual-Semantic Alignments for Generating Image Descriptions. Source: http://cs.stanford.edu/people/karpathy/deepimagesent/"></li></ul><h2 id="训练RNNs"><a href="#训练RNNs" class="headerlink" title="训练RNNs"></a>训练RNNs</h2><ul><li>训练RNN和训练传统的神经网络很相似。我们也用到后向传播(backpropagtion)算法，但是有小小不同。因为参数在整个网络中的每一层是共享的，而每一层输出的梯度不仅仅依赖于当前这一步的计算，还依赖前一步的计算。譬如，为了计算$t&#x3D;4$时的梯度，我们需要往后传播3层，并且把它们的梯度加起来。这就是定时后向传播(BPTT)。如果这还是没有那么清晰的话，不要担心，我们之后还会有更多的详情。现在，我们要注意到用BPTT来训练普通的RNNs，因为梯度消失的问题，所以很难学习到长期的依赖信息(譬如，每一层之间的信息相差甚远)。但还是有一些模型来解决这个问题的，像特定的RNNs模型(LSTMs)就是特别设计用来解决这些问题的。</li></ul><h2 id="RNNs扩展"><a href="#RNNs扩展" class="headerlink" title="RNNs扩展"></a>RNNs扩展</h2><ul><li>经过研究者们那么多年的研究，他们已经发展了更为复杂的RNNs模型来解决普通的RNN模型的一些不足。我们接下来的贴子会讲到更多的细节，但是我想在本部分中做一个简单的总结，这样我们才能对RNNs模型的分类更为熟悉。</li><li>**双向RNNs(Bidirectional RNNs)**就是基于这样的思想：时间$t$的输出可能不仅仅依赖鱼序列当中的前面的元素，还包括了未来的元素。譬如，预测一个序列当中的缺失的单词，你可能要看到前面和后边的内容。双向RNNs非常容易。他们就是把两个RNNs堆叠在一起。输出是基于两个RNNs模型的隐层计算得到的。</li><li><img src="/img/tf_tutorial/05_RNN/bidirectional-rnn.png"></li><li><strong>深度(Bidirectional)RNNs</strong>和双向RNNs很相似，就是我们现在每一个时间点有多层。实际上这给了我们一个更高的学习容量(但是我们也需要大量的训练数据)。</li><li><img src="/img/tf_tutorial/05_RNN/deep-bi-rnn.png"></li><li><strong>LSTM网络</strong>现在非常流行。LSTMs和RNNs在架构上没什么大的不同，但是它们利用了不同的函数来计算隐层状态。LSTMs中的记忆体叫做<strong>cells</strong>，你可以把它们当成一个黑盒子，它吃进了前一个状态$h_{t-1}$和当前的输入$x_t$。这些cells内部决定哪些应该保持，哪些应该删除。然后它们就会合并前一个状态，当前状态和当前的输入。结果证明了这类型的units非常有效地捕获了长期的依赖信息。LSTMs一开始可能很疑惑，但是如果你感兴趣的话，可以读一下<a class="link"   href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" >这篇详细的解释<i class="fas fa-external-link-alt"></i></a>。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>目前还是理解的不错的。我希望你现在对RNNs已有了一个基本的理解。在下一个贴子中我们会利用Python和Theano实现我们RNN语言模型的第一个版本，请在留言区留下你的问题。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;本文翻自&lt;a class=&quot;link&quot;   href=&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&quot; &gt;WILD</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="循环神经网络" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>40 道ML/Data Science的初创公司(可能)的面试题</title>
    <link href="http://example.com/2016/10/03/40/"/>
    <id>http://example.com/2016/10/03/40/</id>
    <published>2016-10-03T08:19:38.000Z</published>
    <updated>2023-07-16T01:44:24.214Z</updated>
    
    <content type="html"><![CDATA[<ul><li>By Manish Saraswat, <a class="link"   href="https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/" >Original Link<i class="fas fa-external-link-alt"></i></a></li><li>09&#x2F;16&#x2F;2016</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p><b><font color="red">留心，这些问题你要三思！</font></b></p></blockquote><ul><li>在今天，机器学习和数据科学家被认为是下一代工业改革的驱动力。这也意味着有很多新进的初创公司在寻找数据科学家。那么，该如何给你振奋人心的职业生涯一个更好的开始呢？</li><li>当然，想进入这个行业并不容易。显然你必须要对公司的理念，团队和远景感兴趣。你可能在你的职业之路上遇到一写很棘手的技术问题。本题集和初创公司做的东西有紧密联系。他们会不会提供咨询？他们会不会创建ML产品？你在面试之前就该提前考虑这个问题？</li><li>为了帮助你准备下一场面试，我整理了40个看似可信但是其中暗藏端倪的问题，它们都有可能出现在你的面试当中。如果你能够轻松应对并且深刻理解问题，那么请放心，你可以在面试中打一场硬仗。</li><li>备注：轻松应对这些问题的核心是你对ML有实际操作经验和了解相关的统计概念。</li></ul><h2 id="机器学习的面试问题"><a href="#机器学习的面试问题" class="headerlink" title="机器学习的面试问题"></a>机器学习的面试问题</h2><ul><li><p><b><font color="blue"> Q1，给你一个1000列，100万行的数据集。这个数据集是一个分类问题。你经理要求你减少这个数据集的维度以减少模型计算所花的时间。你的机器有内存限制。你会怎么做？(你可以做出实际的假设)</font></b></p></li><li><p><b>答：</b>在一台内存有限的机器上处理高纬度的数据是一个很费力的任务，你的面试官肯定意识到这一点。以下是你可以拿来应对的方法：</p><ol><li>由于我们的内存有限，我们首先应该关闭所有不需要的程序，包括浏览器，这样我们才能把内存的利用最大化。</li><li>我们可以随机对数据集进行抽样。这就意味着我们可以创建一个更小的数据集，假如，1000个变量，30万行的数据集，然后做计算。</li><li>要减少维度，我们可以把数值型和分类型的变量分开，然后去掉相关的变量。对于数值型数据，我们利用相关性分析，对于分类型数据，我们利用卡方检验。</li><li>另外，我们可以做PCA，然后挑出数据集中能够解释最大方差的变量。</li><li>利用在线学习算法，像Vowpal Wabbit (Python提供)，也是一个选择。</li><li>利用随机梯度下降来创建一个线性模型也是有帮助的。</li><li>我们也可以把数据集的商业理解考虑进去，然后估计哪些predictors能够影响respone variable。但是这是一个凭直觉的方法，如果分析错误就会造成信息的损失。</li></ol></li><li><p><b>备注：</b>对于第4，5点，请务必了解在线算法和随机梯度下降算法。另外还有更高级的算法。</p></li><li><p><b><font color="blue"> Q2， PCA中的旋转是必须的吗？如果是，那么你没有旋转的话，会发生什么？</font></b></p></li><li><p><b>答：</b>是的，旋转(正交直线)是必须的，因为它能最大化捕捉到的变量之间的差异。这会使变量更容易解释。不要忘记，这确切是PCA的动机所在，我们的目标是选择更少的components，这些变量能够解释数据集的最大方差。通过旋转，components的相关位置不会改变，她仅仅改变这些点的实际坐标。</p></li><li><p>如果我们没有进行旋转，PCA的作用就会减少，而我们需要选择更多的components来解释数据集的方差。</p></li><li><p>了解更多：<a class="link"   href="https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/" >PCA<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q3，给你一个数据集，这个数据集包含这样的缺失值，它的分布是沿着中位数，标准差是1。那么，有百分之几的数据不受影响？为什么呢？</font></b></p></li><li><p><b>答：</b>这道题有足够的提示让你思考。由于数据是沿着中位数分布的，我们假设它是一个正态分布。我们知道在一个正态分布中，68%的数据包含在均值(或者众数，中位数)为中心1为标准差的范围内，那么就有32%的数据是不受影响的。所以，32%的数据将会不受缺失值的影响。</p></li><li><p><b><font color="blue"> Q4，给你一个癌症检测的数据，你建了一个分类模型并且模型的准确率达到了96%。为什么你不能对你模型的表现感到满意？你会怎么做？</font></b></p></li><li><p><b>答：</b>如果你处理了足够多的数据集，你可以推断癌症检测造成了不平衡的数据。在一个不平衡的数据集中，准确率不应该被当作表现的衡量标准，因为96%仅仅是预测对了大多数的类别，但是我们感兴趣的类别是小部分的4%，它恰恰是被用来用作癌症的诊断。所以，为了评估模型的表现，我们应该利用Sensitivity(True Positive Rate)，Specificity(True Negative Rate)，F measure用来诊断分类器的性能。如果小部分的分类性能是很无力的，我们可以采取一下的措施：</p><ol><li>我们可以利用欠采样，过采样或者SMOTE(一种采样技术)使得数据平衡。</li><li>我们可以通过概率校正来改变预测阈值，然后利用AUC-ROC曲线找到一个最优的阈值。</li><li>我们可以给类别赋予权重，让小部分的类别得到更大的权重。</li><li>我们可以做异常检测。</li></ol></li><li><p><b>了解更多：</b><a class="link"   href="https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/" >Imbalanced Classification<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q5，为什么朴素贝叶斯那么“朴素”？</font></b></p></li><li><p><b>答：</b>朴素贝叶斯“朴素”是因为它假设了数据集中的特征都是平等重要且相互独立的。我们知道，这些假设在现实生活中存在的几率是很小的。</p></li><li><p><b><font color="blue"> Q6，解释朴素贝叶斯中的概念：先验概率，似然和边缘似然。</font></b></p></li><li><p><b>答：</b>先验概率就是数据集中独立(二分类)变量的比重，就是一个最简单的分类。譬如，一个数据集中，独立变量是二分类的(0或者1)。1(垃圾邮件)的比重是70%，0(正常邮件)的比重是30%。所以，我们可以估计新邮件有70%的几率被分作垃圾邮件。</p></li><li><p>似然就是给定的观测量在其他变量的条件下被分作1的概率。譬如，在之前的垃圾邮件中“FREE”这个词的概率就是似然。边缘似然就是，“FREE”这个词被用在任何信息当中的概率。</p></li><li><p><b><font color="blue"> Q7，你正在处理一个时间序列的数据集。你经理要求你创建一个高准确率的模型。你一开始就用决策树算法，因为你知道它对任意类型的数据都处理得不错。然后，你试了一个时间序列的回归模型并且得到了更高的准确率。这会发生吗？为什么呢？</font></b></p></li><li><p><b>答：</b>时间序列数据是受线性限制的。另一方面，我们知道决策树算法是适用于检测非线性的交互。决策树不能很好地提供robust的预测是因为它没有像线性模型那样能很好地map到数据的线性关系。所以，我们知道，线性回归模型可以对给定的线性数据提供更robust的预测。</p></li><li><p><b><font color="blue"> Q8，你被分配了一个新的project，这个project是帮助某公司的食物派送减低成本的。问题是：公司的派送团队没能及时地派送食物，结果，顾客就不开心。然后为了让驳回顾客的芳心，公司最后决定免派送费。你认为哪一个机器学习方法能够拯救他们呢？</font></b></p></li><li><p><b>答：</b>你可能很快的在脑海中扫描了一遍机器学习算法。但是，请放松一下，这个问题考的是你的机器学习的基础。</p></li><li><p>这不是一个机器学习的问题。这是一个路径优化的问题。一个机器学习的算法包括三个基本要素：</p><ol><li>问题中存在一个模式</li><li>你不能通过数学计算来解决它(即使是写指数方程)</li><li>你要有数据</li></ol></li><li><p>我们通常找出这三个要素来决定是否能把机器学习当成一个解决实际问题的工具。</p></li><li><p><b><font color="blue"> Q9，你发现你的模型出现了低偏差高方差的问题，你应该怎么解决？为什么呢？</font></b></p></li><li><p><b>答：</b>低偏差说明模型的预测值非常接近真实值。换句话说，这个模型能够很灵活地模仿到训练数据分布。看似这是很成功，但是不要忘记，一个灵活的模型往往没有泛化能力。这意味着，当模型对新数据进行预测时，它可能会给出很差的结果。</p></li><li><p>这样的话，我们可以利用bagging算法(譬如随机森林)来解决高方差的问题。Bagging算法通过重复的随机采样把数据集分成很多个子集。然后，用这些样本来做不同的算法得到一个模型的集合。之后，最终的预测是对模型集中的模型进行不同的组合，分类的话就用投票的方式，回归的话就用求均值的方式。</p></li><li><p>另外，为了防止高方差，我们可以：</p><ol><li>利用正则化技术，对模型的高系数进行惩罚，从而降低模型的复杂度</li><li>对特征的重要性进行排序然后利用前n个。因为，如果全部的特征都用上，算法可能没办法很好地找到有意义的信号。</li></ol></li><li><p><b><font color="blue"> Q10，给你一个数据集，它包含了很多变量，但是你已经知道其中的一些有很高的关联性变量了。你的经理要求你利用PCA进行处理。你会先把有关联性的变量删掉吗？为什么呢？</font></b></p></li><li><p><b>答：</b>可能你会说NO，但是这是不对的。抛开有关联性的数据对PCA有实质的影响不说，如果没有删掉有关联性的变量，那么一个特定的成分的方差就会膨胀。</p></li><li><p>譬如，你有3个变量的数据集，其中2个是有关联性的。如果你对数据进行PCA处理，那么第一个主要的成分就会展示2倍的方差，比没有关联性变量存在的时候。另外，添加有关联性的变量会使得PCA把更多的重要性给以它们，这恰恰是误导的。</p></li><li><p><b><font color="blue"> Q11，运行了几个小时之后，你很焦急想要创建一个准确率高的模型。结果，你建了一个5GB的模型集合，考虑到一个boosting算法会产生奇迹。但是，很不幸的，没有一个模型的表现能比基准分(benchmark score)更好。最后，你决定组合这些模型。我们都知道ensemble模型都会得到很高的准确率，但是你的却没有。请问到底是哪里出了问题呢？</font></b></p></li><li><p><b>答：</b>总所周知，ensemble learners的核心思想是通过组合简单的弱模型来得到一个强模型。但是当模型之间是独立的时候，组合出来的模型才会表现得很好。所以，我们建立了5GB的模型集合，但是准确率却没有提升，这就暗示着模型之间是有关联性的。有关联性的模型的问题是它们都提供了相同的信息。</p></li><li><p>譬如，如果模型1，2，3都是关联的，那么当模型1把User122分类为1时，模型2和模型3也会得到同样的分类结果，即使它的真实值是0。所以，ensemble learners是建立在没有关联性的弱模型集合的基础上，这样的组合才能得到更好的predictions。</p></li><li><p><b><font color="blue"> Q12，kNN和kmeans聚类有什么不同？</font></b></p></li><li><p><b>答：</b>不要给名字中的k误导了。你应该知道这两个算法之间最基本的不同是：kmeans是一个无监督学习方法而kNN是一个有监督的学习方法。kNN是一个分类(或者回归)的方法。</p></li><li><p>kmeans算法是对一个数据集进行划分以至于组成有同质性族群，其中的点与点之间的距离是相近的。这个算法尽量保持这些族群之间的可划分性。而无监督的学习方法中的族群是没有标签的。</p></li><li><p>kNN算法试着对没有标签的数据按临近的距离进行k(k可以是任何小于sample的数)分类。它又被称作懒惰学习方法因为它涉及到模型的最小训练集。所以，它不会用训练数据对新的数据进行泛化。</p></li><li><p><b><font color="blue"> Q13，True Positive Rate和Recall是什么关系？写出公式。</font></b></p></li><li><p><b>答：</b>True Positive Rate &#x3D; Recall。是的，它们有相同的公式(TP&#x2F;(TP+FN))。</p></li><li><p><b>了解更多：</b><a class="link"   href="https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/" >Evaluation Metrics<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q14，你建立了一个多重回归模型。模型的R方并没有你预期的那么好。为了改进，你去掉了截距，模型的R方由原来的0.3升到了0.8。请问这是可能的吗？是如何做到的？</font></b></p></li><li><p><b>答：</b>是的，是有可能的。我们需要理解截距在回归模型中的重要性。截距说明模型的预测是没有任何的独立变量的，如预测的均值。公式：R² &#x3D; 1 – ∑(y – y´)²&#x2F;∑(y – ymean)²，其中y´ 是预测值。</p></li><li><p>当截距存在的时候，R方值把模型的wrt评估到均值模型当中。当截距不存在的时候，模型就不会这样做。巨大的分母∑(y - y´)²&#x2F;∑(y)²就把等式的值变得比实际的要小，然后造成了高的R方。</p></li><li><p><b><font color="blue"> Q15，分析了模型之后，你的经理得知你的模型有多重共线性。你会怎么验证模型？没有信息的流失，你可以建立一个更好的模型吗？</font></b></p></li><li><p><b>答：</b>要检查多重共线性，我们可以创建一个相关系数矩阵来辨识或者删除相关系数达到75%的变量(这个阈值的设置是主观的)。另外，我们可以计算方差膨胀因子(Variance Inflation Factor)来检查多重共线性。 VIF的值&lt;&#x3D;4意味着没有多重共线性，VIF的值&gt;&#x3D;10预示着有严重的多重共线性。最后，我们还可以利用公差来判断多重共线性的出现与否。</p></li><li><p>但是删除相关变量可能导致信息流失。为了保持变量，我们可以用惩罚性回归模型，像lasso回归或者ridge回归。还有，我们可以加入一些随机噪声变量到相关变量当中，这样变量间就会变得不同。但是，加入噪声可能影响预测准确率。所以，这个方法也要小心使用。</p></li><li><p><b>了解更多：</b><a class="link"   href="https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/" >Regression<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q16，什么时候Ridge回归比Lasso回归更好用？</font></b></p></li><li><p><b>答：</b>你可以引用ISLR这本书的作者Hastie Tibshirani的话，他说：In presence of few variables with medium&#x2F; large sized effect, use lasso regression. In presence of many variables with small&#x2F; medium size effect, use ridge regression.</p></li><li><p>理论上说，lasso回归(L1)既做了变量选择也做了参数的收缩，而Ridge回归(L2)只是做了参数的收缩，最后把所有的系数都会算进了模型中。当存在相关变量时，ridge回归回事更好的选择。而且，ridge回归在最小二乘因子有比较高的方差时表现最好。所以，她取决于模型的客观性。</p></li><li><p><b>了解更多：</b><a class="link"   href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/" >Ridge and Lasso Regression<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q17，全球平均气温的提升导致了海盗数量的减少。那么我们可以说海盗数量的减少造成了全球气温的变化吗？</font></b></p></li><li><p><b>答：</b>读完题目后，你应该这就是典型的“因果与相关”的问题。不，我们不能得出海盗数量的减少造成了气温的改变，因为可能是其他因素(潜伏或者混淆的变量)影响着气候。</p></li><li><p>所以，或许全球平均气温和海盗数量有一定的关系，但是基于这样的信息我们不能说海盗减少是因为全球平均气温的升高。</p></li><li><p><b>了解更多：</b><a class="link"   href="https://www.analyticsvidhya.com/blog/2015/06/establish-causality-events/" >Causation and Correlation<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q18，在处理数据的时候，如何选择重要的变量？请解释你的方法。</font></b></p></li><li><p><b>答：</b>以下是一些常用的变量选择方法：</p><ol><li>在选择重要变量之前，先把有相关性的变量删掉</li><li>利用线性回归基于P值选择变量</li><li>利用前向选择，后向选择和逐步选择法</li><li>利用随机森林，Xgboost或者对变量的重要性列表</li><li>利用Lasso回归</li><li>对所有变量做信息增益，然后选择前n个特征</li></ol></li><li><p><b><font color="blue"> Q19，相关性系数和方差有什么不同？</font></b></p></li><li><p><b>答：</b>相关性系数是协方差的标准形式。</p></li><li><p>协方差不容易进行比较。譬如，如果我们计算了工资和年龄的协方差，我们不能把他们进行比较，因为他们之间的scales不一样。为了解决这个情况，我们计算相关性系数，得到一个在-1和1之间的数值，就不用考虑它们之间不同的scales了。</p></li><li><p><b><font color="blue"> Q20，有没有可能求连续变量和分类变量之间的相关性系数吗？如果可以，怎么做？</font></b></p></li><li><p><b>答：</b>是的，我们可以利用ANCOVA(协方差分析)技术来获得连续性和分类变量之间的相关性系数。</p></li><li><p><b><font color="blue"> Q21，同样是基于树的算法，随机森林和梯度提升算法(GBM)之间有什么不同？</font></b></p></li><li><p><b>答：</b>最基本的不同是，随机森林利用bagging技术来做预测，GBM是用boosting技术来做预测。</p></li><li><p>在bagging技术当中，一个额数据集被随机抽样法分成了n个样本。之后利用单个学习方法对所有的样本进行建模。之后，最终的预测结果是通过对多个模型的预测值进行投票或者求均值的方法得到的。bagging是并行化的。在boosting当中，在第一轮的模型进行预测之后，这个算法就会把误分类模型的权值加大，这样就可以在随后的建模过程中对模型进行修正。这种顺序性的过程直到最后的预测值满足停止标准值为止。</p></li><li><p>随机森林通过减低方差改善了模型的准确率。树的生长和最大化方差的降幅没有关系。另外，GBM同时减低了bias和方差来改善模型的准确率。</p></li><li><p><b>了解更多：</b><a class="link"   href="https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/" >Tree based modeling<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q22，运行一个二分类树算法并不难。那你知道树是如何分割的吗？譬如，树是如何决定哪一个结点作为根节点，哪一个结点作为后继节点的吗？</font></b></p></li><li><p><b>答：</b>一个分类树的生成是基于基尼系数和结点的熵。简单的说，树算法会找到最适合把数据集划分成子节点的变量。</p></li><li><p>基尼系数是说，如果我们随机选择了两个群体，那么它们必须是属于同一类的，然后概率和为1。我们可以这样来计算基尼系数：</p><ol><li>计算子节点的基尼系数，用概率的成功和失败的平方和公式： (p^2+q^2)。</li><li>利用每个分割节点的权重基尼得分计算它们的基尼系数</li></ol></li><li><p>熵是衡量混乱程度的标准，即是否更适合分割：</p></li><li><p>这里的p和q分别代表节点成功和失败的概率。熵是0当一个节点是同质的时候。它最大的时候是当一个节点的两个分类各占50%。我们要求小的熵。</p></li><li><p><b><font color="blue"> Q23，你利用1000棵树建立了一个随机森林。你非常高兴因为你得到了训练错误率是0。但是，验证错误率是34.23。发生了什么事了？你训练的模型完美吗？</font></b></p></li><li><p><b>答：</b>这个模型是过拟合了。训练错误率是0意味着分类器在某程度上很好的拟合了你的训练数据，但是它对未知数据的预测性很差。所以，当我们用这个分类器来预测新数据时，它找不到新数据的模式并且返回了很高的错误率。在随机森林中，当我们用了超过了我们所需要的树时就会发生这种情况。所以，为了避免这种问题，我们就要用交叉验证来调节树的数量。</p></li><li><p><b><font color="blue"> Q24，你得到了一个数据集，他的变量数p&gt;样本数n。为什么OLS是一个不好的方法？你应该用什么技术来解决，为什么呢？</font></b></p></li><li><p><b>答：</b>在如此高纬度的数据集中，我们不可以利用经典的回归技术，因为假设都会失败。当p&gt;n时，我们不能计算一个最小二乘系数了，变量是非常大的时候，OSL就不可行了。</p></li><li><p>为了解决这种情况，我们可以利用惩罚性回归方法，像lasso，LARS，ridge这些能够收缩系数的方法来减少变量。更准确的说，ridge回归表现最好，当最小二乘因子有高方差的时候。</p></li><li><p>其它的方法还有取子集回归，前向逐步回归。</p></li><li><p><b><font color="blue"> Q25，什么是凸多边形(convex hull)？(思考一下SVM)</font></b></p></li><li><p><b>答：</b>在可分割的数据中，convex hull就是两组数据点的边界部分。一旦convex hull创建了，我们就可以得到最大边界超平面(MMH)，它是两个convex hulls之间的垂直平分线。MMH是一条能最大化的分割两组数据的直线。</p></li><li><p><b><font color="blue"> Q26，我们知道一位热编码会增加数据集的维度。但是，标签编码却不会，为什么呢？</font></b></p></li><li><p><b>答：</b>不要给这个问题给搞混了。它只是问你两个编码之间的差别。</p></li><li><p>利用一位热编码，数据集的维度(变量)就会增加因为它为分类变量的每一个level表现创建了一个新的变量。譬如，一个变量叫“颜色”。这个变量有3个level，红，蓝和绿。一位热编码就会产生3个新的变量<b>Color.Red, Color.Blue, Color.Green</b>，然后它们的值包含0和1。</p></li><li><p>在类标签编码中，分类变量被编成0和1，所以没有产生新的变量。类标签编码通常用在二分类变量当中。</p></li><li><p><b><font color="blue"> Q27，在时间序列的数据集中，你会用哪一种交叉验证方法，k折叠还是留一验证？</font></b></p></li><li><p><b>答：</b>都不是。</p></li><li><p>在时间序列问题中，k折叠会产生问题，因为可能有些模式在第4年和第5年中，但是没有在第3年中。重采样会分离这些，然后我们可以用去年来做验证，这是不对的。但是我们可以用5-fold的正向推理策略：</p><ul><li>fold 1: training [1], test[2]</li><li>fold 2: training[1,2], test[3]</li><li>fold 3: training[1,2,3], test[4]</li><li>fold 4: training[1,2,3,4], test[5]</li><li>fold 5: training[1,2,3,4,5], test[6]</li></ul></li><li><p>1,2,3,4,5,6代表年。</p></li><li><p><b><font color="blue"> Q28，给你一个数据集，但是它包含了缺失值的变量，且缺失值占了超过30%，譬如，50个变量，有8个变量的缺失值超过了30%。你会怎么处理？</font></b></p></li><li><p><b>答：</b>我们可以做以下处理：</p><ol><li>给缺失值赋一个唯一分类值，谁知道缺失值会不会破译一些趋势呢</li><li>我们可以直接删掉它们</li><li>或者，我们可以根据目标变量检查它们的分布，如果我们能找到模式，那么我们就赋于缺失值一个新的分类，否则就删掉它们。</li></ol></li><li><p><b><font color="blue"> Q29，亚马逊上的“浏览此商品的顾客也同时浏览。。。”这个推荐系统是什么算法的结果？</font></b></p></li><li><p><b>答：</b>推荐系统的核心思想是协同过滤。</p></li><li><p>协同过滤算法是通过用户行为来推荐物品。它是通过物品的交易记录，评价，选择以及购买信息来挖掘其他用户的行为。其他用户对物品的行为和爱好被用来当作给新用户推荐的依据。这个例子中，物品的特征是不知道的。</p></li><li><p><b>了解更多：</b><a class="link"   href="https://www.analyticsvidhya.com/blog/2015/10/recommendation-engines/" >Recommender System<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q30，你怎么理解Type one and Type two error ?</font></b></p></li><li><p><b>答：</b>Type I error 是指统计学中的一类错误，意思是本来是错误的结论却被接受了。TypeII error 是指统计学中的二类错误，也就是本来是正确的错误却被拒绝了。简而言之，就是存伪和弃真。</p></li><li><p>在混淆矩阵当中，我们可以说Type I error就是当我们把实际的0预测分类为1，Type II error就是当我们把实际的1预测分类为0.</p></li><li><p><b><font color="blue"> Q31，你正在做一个分类的问题。为了达到验证的目标，你随机的从训练集抽样分为训练集和验证集。你对你的模型的泛化能力很有信心因为它的验证错误率非常高。但是，结果却是十分失望，你的测试准确率很低。到底是怎么了？</font></b></p></li><li><p><b>答：</b>在分类问题当中，我们应该常常用分层抽样而不是随机抽样。因为随机抽样并没有考虑到目标类别的比例。相反，分层抽样就会保证目标变量的分布也会保证样本的分布。</p></li><li><p><b><font color="blue"> Q32，你被要求利用R², adjusted R² 和tolerance来对回归模型进行评价。你会用哪一个作为标准？</font></b></p></li><li><p><b>答：</b>Tolerance(1 &#x2F; VIF)是用作多重共线性的预示。它是衡量一个预测中的变量的百分比不能给另一个预测占据的程度。Tolerance越大越好。</p></li><li><p>我们认为adjusted R² 和R² 来评估模型是截然不同的，因为当我们增加变量的时候，无论预测准确率有没有改进，R²都会增加 。但是adjusted R²仅仅是在增加变量而提高了模型的准确率的时候才会增加。很难去确定adjusted R²的通常值，因为它会随着数据的不同而不同。譬如，基因变异数据集中，低的adjusted R²值的模型依然会有比较不错的预测能力，但是对比于股票数据，低的adjusted R²值就会得到不好的模型。</p></li><li><p><b><font color="blue"> Q33，在k-means或者kNN中，我们计算相近点之间的距离是用欧几里得距离，为什么我们不用曼哈顿距离呢？</font></b></p></li><li><p><b>答：</b>我们不用曼哈顿距离是因为它真能垂直计算或者平行计算距离，它有维度限制。另外，欧几里得距离是用在任意空间当中计算的。因为，数据是可以表示在任何的维度空间当中的，所以欧几里得距离是更好的选择。</p></li><li><p>譬如，在一个棋盘上，象和车的移动就是通过曼哈顿距离来计算的，因为他们只能垂直做或者平行的移动。</p></li><li><p><b><font color="blue"> Q34，像一个5岁的孩子来介绍一下机器学习。</font></b></p></li><li><p><b>答：</b>非常简单。就像宝宝学走路一样。每一次的跌倒，他们都会无意识地学习并且意识到他们下一次就应该挺直的走而不是弯下来走。当下一次他们跌倒，他们会感到疼，他们会哭，但是，他们就不会再那样走了。为了避免疼痛，他们会更努力尝试。为了成功，他们会借助门后或者墙的力量或者任何接近他们的东西，这样他们就会站的更稳。</p></li><li><p>这是机器如何从它的周围环境学习和发展直觉的过程。</p></li><li><p>注：这道面试题就是考你能不能很好把复杂的文件简单化解释一下。</p></li><li><p><b><font color="blue"> Q35，我知道一个线性回归通常是用adjusted R²或者F值来评估的。那你如何评估一个罗吉斯特回归模型呢？</font></b></p></li><li><p><b>答：</b>我们可以用以下方法：</p><ol><li>因为逻辑斯特回归是用来预测概率的，我们可以用混淆矩阵的AUC-ROC曲线来评估它的性能。</li><li>另外，逻辑斯特回归当中类似adjusted R²的评估标准是AIC。AIC是通过模型系数的数量来惩罚模型的拟合标准。所以，我们要的是有最小AIC值的模型。</li><li>空异常(Null Deviance)说明模型仅仅通过截距来预测。数值越小，模型越好。残差(Residual deviance)说明模型添加了独立变量来进行预测的。数值越小，模型越好。</li></ol></li><li><p><b>了解更多：</b><a class="link"   href="https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/" >Logistic Regression<i class="fas fa-external-link-alt"></i></a></p></li><li><p><b><font color="blue"> Q36，有那么多的机器学习方法，给你一个数据集，你会决定用什么方法呢？</font></b></p></li><li><p><b>答：</b>你应该说，选择用哪一种机器学习方法取决于数据的类型。如果给你的数据集是一个线性的，那么线性回归算法最适合。如果你处理的是图像，语音数据，那么神经网络可以建一个更稳固的模型。</p></li><li><p>如果数据包含一些非线性的关系，那么boosting或者bagging算法就是一个选择。如果商业需求是建一个能够发布的模型，那么我们会用回归或者决策树模型(容易解释)而不是一些黑箱的方法，像SVM，GBM等等。</p></li><li><p>简单来说，没有绝对的方法，我们应该要认真的理解我们要用的算法。</p></li><li><p><b><font color="blue"> Q37，你认为把分类变量当成连续的变量来处理，会使得预测模型更好吗？</font></b></p></li><li><p><b>答：</b>为了得到更准确的预测，分类变量只有在它是有序的时候才被当成连续的变量，这样才合理。</p></li><li><p><b><font color="blue"> Q38，在机器学习当中，什么时候用到规则化技术(regularization)？</font></b></p></li><li><p><b>答：</b>当模型变得过拟合或者欠拟合时，规则化(Regularization)变得越来越重要了。这个技术在很多特征的模型中加入了一个惩罚项。所以，它试着把很多的变量的系数变成0以至于减少成本。这样可以帮助降低模型的复杂度从而可以提高模型的泛化能力。</p></li><li><p><b><font color="blue"> Q39，你怎么理解bias variance权衡？</font></b></p></li><li><p><b>答：</b>当模型能在数学上表示成3个成分的时候，这个错误就会出现。以下就是这些成分：</p></li><li><p>偏差错误是将预测的均值和真实值的差异程度量化了。一个高的偏差错误意味着我们得到的是一个欠拟合模型，它总是偏离了真实的趋势。而方差则是模型预测值和真实值的分散程度。高方差说明模型在训练数据上过拟合了，然后在新数据上预测很差。</p></li><li><p><b><font color="blue"> Q40，OLS(最小二乘法)对应线性回归，最大似然对应逻辑斯特回归。请解释这句话。</font></b></p></li><li><p><b>答：</b>简单来说，最小二乘法和最大似然都是对回归方法进行未知参数的预估的方法。</p></li><li><p>最小二乘法(Ordinary least square)用在线性回归中估计参数，目的是要真实值和预测值之间的距离最小。最大似然是帮助选择一个参数值，使得模型能够最大化产生观测数据。</p></li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ul><li>你可能很轻松地回答了所有问题，但是我们的目标是要理解它们并且可以举一反三，理解透相关的问题。如果你没能很好应对这些问题，也不用担心，从现在开始学习，从现在开始关注学习的问题。</li><li>这些问题是为大家提供了初创公司的面试问题的概况。我相信这些问题引起了你深入学习机器学习的欲望，现在开始计划吧。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;By Manish Saraswat, &lt;a class=&quot;link&quot;   href=&quot;https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-i</summary>
      
    
    
    
    <category term="技能-修行-进步" scheme="http://example.com/categories/%E6%8A%80%E8%83%BD-%E4%BF%AE%E8%A1%8C-%E8%BF%9B%E6%AD%A5/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="面试" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
</feed>
