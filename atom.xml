<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2020-06-23T01:07:56.000Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Tim Chan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Training your own datasets with Darknet</title>
    <link href="http://example.com/2021/11/07/Training-your-own-datasets-with-Darknet/"/>
    <id>http://example.com/2021/11/07/Training-your-own-datasets-with-Darknet/</id>
    <published>2021-11-07T06:48:57.000Z</published>
    <updated>2020-06-23T01:07:56.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>date, 2018-11-07 14:48:57</li></ul><h3 id="Data-collection-amp-labeling"><a href="#Data-collection-amp-labeling" class="headerlink" title="Data collection &amp; labeling"></a>Data collection &amp; labeling</h3><ol><li>Use your own way to collect your data, usually the size of image doesn’t matter. It’ll better to be fit in (48, 48) ~ (1280, 720)</li><li>When you finished your own dataset, you should label your images.<ul><li>tools : <a class="link"   href="https://github.com/tzutalin/labelImg" >labelImage<i class="fas fa-external-link-alt"></i></a> </li><li>Usage : refer to the github</li></ul></li></ol><hr><h3 id="Install-Darknet"><a href="#Install-Darknet" class="headerlink" title="Install Darknet"></a>Install Darknet</h3><ol><li><a class="link"   href="https://pjreddie.com/darknet/install/" >Darknet Installation<i class="fas fa-external-link-alt"></i></a> , compile with GPU and Opencv if it’s necessary</li></ol><hr><h3 id="Create-VOC-format-dataset"><a href="#Create-VOC-format-dataset" class="headerlink" title="Create VOC format dataset"></a>Create VOC format dataset</h3><ul><li><p>(1)  In the root of darknet, create a folder names ‘VOCdevkit’, and create a folder names what you want to name your dataset. like ‘VOC2019_oppo’, which has to start with ‘VOC’.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /path/darknet</span><br><span class="line">mkdir VOCdevkit</span><br><span class="line">cd VOCdevkit</span><br><span class="line">mkdir VOC2019_oppo</span><br></pre></td></tr></table></figure></li><li><p>(2) Directory like this :</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">└── VOCdevkit</span><br><span class="line">    └── VOC2019_oppo</span><br><span class="line">        ├── Annotations</span><br><span class="line">        ├── ImageSets</span><br><span class="line">        │   └── Main</span><br><span class="line">        └── JPEGImages</span><br></pre></td></tr></table></figure></li><li><p>(3) Move the images into <strong>JPEGImages</strong> and xml files into <strong>Annotations</strong>.</p></li><li><p>(4) Split the train, val and test, create a py script like belows</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## split_train_val.py</span></span><br><span class="line"><span class="keyword">import</span> os,random</span><br><span class="line"></span><br><span class="line"><span class="comment"># read the filenames from a file</span></span><br><span class="line">dirname = <span class="string">&#x27;./Annotations&#x27;</span></span><br><span class="line">files = [f[:-<span class="number">4</span>] <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(dirname) <span class="keyword">if</span> f[-<span class="number">4</span>:].lower() == <span class="string">&#x27;.xml&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># random divide  </span></span><br><span class="line">trainval = random.sample(files, <span class="built_in">len</span>(files)//<span class="number">2</span>)</span><br><span class="line">test = [f <span class="keyword">for</span> f <span class="keyword">in</span> files <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> trainval]</span><br><span class="line"></span><br><span class="line"><span class="comment"># random divide </span></span><br><span class="line">train = random.sample(trainval, <span class="built_in">len</span>(trainval)//<span class="number">2</span>)</span><br><span class="line">val = [f <span class="keyword">for</span> f <span class="keyword">in</span> trainval <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> train]</span><br><span class="line"></span><br><span class="line"><span class="comment"># save to txt file</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">list2txt</span>(<span class="params">arr, fname</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname+<span class="string">&#x27;.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> arr:</span><br><span class="line">            f.write(a+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">list2txt(trainval, <span class="string">&#x27;trainval&#x27;</span>)</span><br><span class="line">list2txt(test, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">list2txt(train, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">list2txt(val, <span class="string">&#x27;val&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>then run the script, you will get four files, then move them into the <strong>ImageSets&#x2F;Main&#x2F;</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python split_train_val.py</span><br><span class="line"></span><br><span class="line">mv test.txt ImageSets/Main/</span><br><span class="line">mv train.txt ImageSets/Main/</span><br><span class="line">mv trainval.txt ImageSets/Main/</span><br><span class="line">mv val.txt ImageSets/Main/</span><br></pre></td></tr></table></figure></li><li><p>(5) Now you have the directory like this</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">└── VOCdevkit</span><br><span class="line">    └── VOC2019_oppo</span><br><span class="line">        ├── Annotations</span><br><span class="line">        ├── ImageSets</span><br><span class="line">        │   └── Main</span><br><span class="line">        │       ├── test.txt</span><br><span class="line">        │       ├── train.txt</span><br><span class="line">        │       ├── trainval.txt</span><br><span class="line">        │       └── val.txt</span><br><span class="line">        ├── JPEGImages</span><br><span class="line">        └── split_train_val.py</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Use-the-voc-labe-to-generate-Image-path-list"><a href="#Use-the-voc-labe-to-generate-Image-path-list" class="headerlink" title="Use the voc_labe to generate Image path list"></a>Use the voc_labe to generate Image path list</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /path/darknet</span><br><span class="line">touch voc_label.py</span><br><span class="line">vim voc_label.py</span><br></pre></td></tr></table></figure><ul><li>(1) create a python script<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## name voc_label.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir, getcwd</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. change to your labels</span></span><br><span class="line"><span class="comment"># oppo for example</span></span><br><span class="line"><span class="comment"># 4 classes, A5s, A7, reno, reno10x</span></span><br><span class="line">sets = [(<span class="string">&#x27;2019_oppo&#x27;</span>, <span class="string">&#x27;train&#x27;</span>), (<span class="string">&#x27;2019_oppo&#x27;</span>, <span class="string">&#x27;val&#x27;</span>), (<span class="string">&#x27;2019_oppo&#x27;</span>, <span class="string">&#x27;test&#x27;</span>)]</span><br><span class="line">classes = [<span class="string">&#x27;A5s&#x27;</span>, <span class="string">&#x27;A7&#x27;</span>, <span class="string">&#x27;reno&#x27;</span>, <span class="string">&#x27;reno10x&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert</span>(<span class="params">size, box</span>):</span><br><span class="line">    dw = <span class="number">1.</span>/(size[<span class="number">0</span>])</span><br><span class="line">    dh = <span class="number">1.</span>/(size[<span class="number">1</span>])</span><br><span class="line">    x = (box[<span class="number">0</span>] + box[<span class="number">1</span>])/<span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    y = (box[<span class="number">2</span>] + box[<span class="number">3</span>])/<span class="number">2.0</span> - <span class="number">1</span></span><br><span class="line">    w = box[<span class="number">1</span>] - box[<span class="number">0</span>]</span><br><span class="line">    h = box[<span class="number">3</span>] - box[<span class="number">2</span>]</span><br><span class="line">    x = x*dw</span><br><span class="line">    w = w*dw</span><br><span class="line">    y = y*dh</span><br><span class="line">    h = h*dh</span><br><span class="line">    <span class="keyword">return</span> (x,y,w,h)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_annotation</span>(<span class="params">year, image_id</span>):</span><br><span class="line">    <span class="comment"># 2. change to your path </span></span><br><span class="line">    in_file = <span class="built_in">open</span>(<span class="string">&#x27;/home/ares2/darknet/VOCdevkit/VOC%s/Annotations/%s.xml&#x27;</span>%(year, image_id))</span><br><span class="line">    out_file = <span class="built_in">open</span>(<span class="string">&#x27;/home/ares2/darknet/VOCdevkit/VOC%s/labels/%s.txt&#x27;</span>%(year, image_id), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    tree=ET.parse(in_file)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line">    size = root.find(<span class="string">&#x27;size&#x27;</span>)</span><br><span class="line">    w = <span class="built_in">int</span>(size.find(<span class="string">&#x27;width&#x27;</span>).text)</span><br><span class="line">    h = <span class="built_in">int</span>(size.find(<span class="string">&#x27;height&#x27;</span>).text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> root.<span class="built_in">iter</span>(<span class="string">&#x27;object&#x27;</span>):</span><br><span class="line">        difficult = obj.find(<span class="string">&#x27;difficult&#x27;</span>).text</span><br><span class="line">        cls = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> classes <span class="keyword">or</span> <span class="built_in">int</span>(difficult)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        cls_id = classes.index(cls)</span><br><span class="line">        xmlbox = obj.find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">        b = (<span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmin&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmax&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymin&#x27;</span>).text), <span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymax&#x27;</span>).text))</span><br><span class="line">        bb = convert((w,h), b)</span><br><span class="line">        out_file.write(<span class="built_in">str</span>(cls_id) + <span class="string">&quot; &quot;</span> + <span class="string">&quot; &quot;</span>.join([<span class="built_in">str</span>(a) <span class="keyword">for</span> a <span class="keyword">in</span> bb]) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">wd = getcwd()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> year, image_set <span class="keyword">in</span> sets:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;VOCdevkit/VOC%s/labels/&#x27;</span>%(year)):</span><br><span class="line">        os.makedirs(<span class="string">&#x27;VOCdevkit/VOC%s/labels/&#x27;</span>%(year))</span><br><span class="line">    <span class="comment"># 3. change to your path</span></span><br><span class="line">    image_ids = <span class="built_in">open</span>(<span class="string">&#x27;/home/ares2/darknet/VOCdevkit/VOC%s/ImageSets/Main/%s.txt&#x27;</span>%(year, image_set)).read().strip().split()</span><br><span class="line">    list_file = <span class="built_in">open</span>(<span class="string">&#x27;%s_%s.txt&#x27;</span>%(year, image_set), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> image_id <span class="keyword">in</span> image_ids:</span><br><span class="line">        list_file.write(<span class="string">&#x27;%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\n&#x27;</span>%(wd, year, image_id))</span><br><span class="line">        convert_annotation(year, image_id)</span><br><span class="line">    list_file.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>remember that there are 3 places you need to change</li><li>this will generate 3 files: <ul><li><strong>2019_oppo_train.txt</strong></li><li><strong>2019_oppo_val.txt</strong></li><li><strong>2019_oppo_test.txt</strong></li></ul></li><li>Usually I merge <strong>2019_oppo_train.txt</strong> and <strong>2019_oppo_test.txt</strong> as <strong>2019_oppo_train.txt</strong> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /path/darknet</span><br><span class="line">mkdir oppo_od_bak</span><br><span class="line">cd oppo_od_bak</span><br><span class="line">mkdir cfg</span><br></pre></td></tr></table></figure></li><li>from the <strong>darknet&#x2F;cfg&#x2F;</strong> you can find the <strong>yolo-voc.cfg</strong> and the <strong>yolo-tiny.cfg</strong> and from the <a class="link"   href="https://pjreddie.com/darknet/yolo/" >official website<i class="fas fa-external-link-alt"></i></a> you can download the <strong>pretrained models</strong>, like for the <strong>yolo-voc</strong> is <a class="link"   href="https://pjreddie.com/media/files/darknet53.conv.74" >darknet53.conv.74<i class="fas fa-external-link-alt"></i></a>.</li></ul><hr><h3 id="Prepare-your-cfg-file"><a href="#Prepare-your-cfg-file" class="headerlink" title="Prepare your cfg file"></a>Prepare your cfg file</h3><ul><li>the 3 files you use to train the yolo is <ul><li><strong>yourdata.names</strong></li><li><strong>yourdata.data</strong></li><li><strong>yourcfg.cfg</strong></li></ul></li><li>(1) <strong>yourdata.names</strong> contains the labels of your dataset, each label for a line</li><li>(2) <strong>yourdata.data</strong> example<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classes= #classes #类别数目</span><br><span class="line">train  = /path/yourfilename_train.txt # 训练数据</span><br><span class="line">valid  = /path/yourfilenane_val.txt # 验证数据</span><br><span class="line">names = data/yourname.names # class labels</span><br><span class="line">backup = /backup/ # 权重保存所在文件</span><br></pre></td></tr></table></figure></li><li>remember to delete the comments</li><li>(3) <strong>yourcfg.cfg</strong><ul><li>you can use the <strong>yolo-voc.cfg</strong> or the <strong>yolo-tiny.cfg</strong></li><li>remember to change these places</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim yolo-voc.cfg</span><br><span class="line"></span><br><span class="line">## Remember to comment the testing and uncomment the training</span><br><span class="line">[net]</span><br><span class="line"># Testing</span><br><span class="line"># batch=1</span><br><span class="line"># subdivisions=1</span><br><span class="line"># Training</span><br><span class="line">batch=64</span><br><span class="line">subdivisions=16</span><br></pre></td></tr></table></figure><ul><li>YOU should change every [yolo] layer.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">size=1</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=27 ## YOU SHOULD CHANGE THE # OF FILTERS</span><br><span class="line">## filters = (classes + 5) * 3</span><br><span class="line">activation=linear</span><br><span class="line"></span><br><span class="line">[yolo]</span><br><span class="line">mask = 6,7,8</span><br><span class="line">anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326</span><br><span class="line">classes=4  ## CHANGE TO THE NUMBER OF YOUR LABELS</span><br><span class="line">num=9</span><br><span class="line">jitter=.3</span><br><span class="line">ignore_thresh = .5</span><br><span class="line">truth_thresh = 1</span><br><span class="line">random=1</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Start-Training"><a href="#Start-Training" class="headerlink" title="Start Training"></a>Start Training</h3><ul><li>First time you train, use the pretrained classification model<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd path/darknet/</span><br><span class="line"># yolo-tiny</span><br><span class="line">./darknet detector train cfg/yourdata.data cfg/yourcfg.cfg backup/bo_can_tiny_176.weights</span><br><span class="line"># yolo-voc</span><br><span class="line">./darknet detector train cfg/yourdata.data cfg/yourcfg.cfg backup/darknet53.conv.74</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Know-your-log"><a href="#Know-your-log" class="headerlink" title="Know your log"></a>Know your log</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Region <span class="number">82</span> Avg IOU: <span class="number">0.801934</span>, Class: <span class="number">0.737764</span>, Obj: <span class="number">0.782024</span>, No Obj: <span class="number">0.006216</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">1.000000</span>, count: <span class="number">5</span> </span><br><span class="line">Region <span class="number">94</span> Avg IOU: <span class="number">0.706899</span>, Class: <span class="number">0.073915</span>, Obj: <span class="number">0.544467</span>, No Obj: <span class="number">0.000506</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">0.000000</span>, count: <span class="number">1</span> </span><br><span class="line">Region <span class="number">106</span> Avg IOU: <span class="number">0.831056</span>, Class: <span class="number">0.037965</span>, Obj: <span class="number">0.026004</span>, No Obj: <span class="number">0.000057</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">1.000000</span>, count: <span class="number">1</span> </span><br><span class="line">Region <span class="number">82</span> Avg IOU: <span class="number">0.731572</span>, Class: <span class="number">0.800899</span>, Obj: <span class="number">0.793200</span>, No Obj: <span class="number">0.005694</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">0.333333</span>, count: <span class="number">3</span> </span><br><span class="line">Region <span class="number">94</span> Avg IOU: <span class="number">0.607969</span>, Class: <span class="number">0.199724</span>, Obj: <span class="number">0.884315</span>, No Obj: <span class="number">0.000286</span>, <span class="number">.5</span>R: <span class="number">1.000000</span>, <span class="number">.75</span>R: <span class="number">0.000000</span>, count: <span class="number">1</span> </span><br><span class="line">Region <span class="number">106</span> Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: <span class="number">0.000015</span>, <span class="number">.5</span>R: -nan, <span class="number">.75</span>R: -nan, count:</span><br></pre></td></tr></table></figure><ul><li>（1）以上输出显示了所有训练图片的一个批次（batch），批次大小的划分根据我们在 .cfg 文件中设置的subdivisions参数。在我使用的 .cfg 文件中 batch &#x3D; 64 ，subdivision &#x3D; 16，所以在训练输出中，训练迭代包含了16组，每组又包含了4张图片，跟设定的batch和subdivision的值一致。<br>但是此处有16*3条信息，每组包含三条信息，分别是：<br>Region 82 Avg IOU:<br>Region 94 Avg IOU:<br>Region 106 Avg IOU:<br>三个尺度上预测不同大小的框 82卷积层 为最大的预测尺度，使用较大的mask，但是可以预测出较小的物体 94卷积层 为中间的预测尺度，使用中等的mask， 106卷积层为最小的预测尺度，使用较小的mask，可以预测出较大的物体</li><li>（2）每个batch都会有这样一个输出：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2706: 1.350835, 1.386559 avg, 0.001000 rate, 3.323842 seconds, 173184 images</span><br></pre></td></tr></table></figure></li></ul><p>2706：batch是第几组。<br>1.350835：总损失<br>1.386559 avg ： 平均损失<br>0.001000 rate：当前的学习率<br>3.323842 seconds： 当前batch训练所花的时间<br>173184 images ： 目前为止参与训练的图片总数 &#x3D; 2706 * 64 </p><ul><li>（3）<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Region 82 Avg IOU: 0.798032, Class: 0.559781, Obj: 0.515851, No Obj: 0.006533, .5R: 1.000000, .75R: 1.000000,  count: 2</span><br></pre></td></tr></table></figure></li></ul><p>Region Avg IOU: 表示在当前subdivision内的图片的平均IOU，代表预测的矩形框和真实目标的交集与并集之比.<br>Class: 标注物体分类的正确率，期望该值趋近于1。<br>Obj: 越接近1越好。<br>No Obj: 期望该值越来越小，但不为零。<br>count: count后的值是所有的当前subdivision图片（本例中一共4张）中包含正样本的图片的数量。</p><ul><li>参考：<a class="link"   href="https://blog.csdn.net/qq_33444963/article/details/80842179" >https://blog.csdn.net/qq_33444963&#x2F;article&#x2F;details&#x2F;80842179<i class="fas fa-external-link-alt"></i></a></li></ul><hr><h3 id="Training-experience"><a href="#Training-experience" class="headerlink" title="Training experience"></a>Training experience</h3><ul><li><p><strong>YOLO-TINY</strong></p><ul><li>It’s a simple network for feature extraction, fit to the simple circumstances.</li><li>Each class should have more than 500 images</li><li>Training more than 1000 epoches</li><li>Fast but low accurate.</li></ul></li><li><p><strong>YOLO-VOC</strong></p><ul><li>It’s a complicated network training on the Imagenet</li><li>Each class should have more than 300 images</li><li>Traing more than 10000 epoches.</li><li>Slow but accurate</li></ul></li><li><p>Overall, more images, the model will be better. You can try to add images slowly.</p></li></ul><h3 id="How-to-run-your-own-yolov3-model-with-Opencv"><a href="#How-to-run-your-own-yolov3-model-with-Opencv" class="headerlink" title="How to run your own yolov3 model with Opencv"></a>How to run your own yolov3 model with Opencv</h3><ul><li>first you need to install the opencv</li><li>then, you just copy three files from what you have trained<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yourname.name</span><br><span class="line">yourcfg.cfg</span><br><span class="line">yourweights.weights</span><br></pre></td></tr></table></figure></li><li>then set them in the config file<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class FLAGS:</span><br><span class="line">    # Initialize the parameters</span><br><span class="line">    confThreshold = 0.65  # Confidence threshold</span><br><span class="line">    nmsThreshold = 0.3  # Non-maximum suppression threshold</span><br><span class="line">    inpWidth = 416  # Width of network&#x27;s input image</span><br><span class="line">    inpHeight = 416  # Height of network&#x27;s input image</span><br><span class="line"></span><br><span class="line">    camera_id = 0</span><br><span class="line"></span><br><span class="line">    # Load names of classes</span><br><span class="line">    classesFile = &quot;./shelves_od_300/shelves_od.names&quot;</span><br><span class="line">    classes = None</span><br><span class="line">    with open(classesFile, &#x27;rt&#x27;) as f:</span><br><span class="line">        classes = f.read().rstrip(&#x27;\n&#x27;).split(&#x27;\n&#x27;)</span><br><span class="line"></span><br><span class="line">    # Give the configuration and weight files for the model and load the network using them</span><br><span class="line">    modelConfiguration = &quot;./shelves_od_300/yolov3-voc.cfg&quot;</span><br><span class="line">    modelWeights = &quot;./shelves_od_300/yolov3-voc_latest.weights&quot;</span><br></pre></td></tr></table></figure></li><li>Finally, run the script below<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"># This code is written at BigVision LLC. It is based on the OpenCV project. It is subject to the license terms in the LICENSE file found in this distribution and at http://opencv.org/license.html</span><br><span class="line"></span><br><span class="line"># Usage example:  python3 object_detection_yolo.py --video=run.mp4</span><br><span class="line">#                 python3 object_detection_yolo.py --image=bird.jpg</span><br><span class="line"></span><br><span class="line">import cv2 as cv</span><br><span class="line">import argparse</span><br><span class="line">import sys</span><br><span class="line">import numpy as np</span><br><span class="line">import os.path</span><br><span class="line">import uuid</span><br><span class="line"></span><br><span class="line">from config import FLAGS</span><br><span class="line"></span><br><span class="line"># Initialize the parameters</span><br><span class="line">confThreshold = FLAGS.confThreshold  #Confidence threshold</span><br><span class="line">nmsThreshold = FLAGS.nmsThreshold   #Non-maximum suppression threshold</span><br><span class="line">inpWidth = FLAGS.inpWidth       #Width of network&#x27;s input image</span><br><span class="line">inpHeight = FLAGS.inpHeight      #Height of network&#x27;s input image</span><br><span class="line"></span><br><span class="line">classes = FLAGS.classes</span><br><span class="line">global _i</span><br><span class="line">_i = 1000</span><br><span class="line"># Get the =-.l2 of the output layers</span><br><span class="line">def getOutputsNames(net):</span><br><span class="line">    # Get the names of all the layers in the network</span><br><span class="line">    layersNames = net.getLayerNames()</span><br><span class="line">    # Get the names of the output layers, i.e. the layers with unconnected outputs</span><br><span class="line">    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]</span><br><span class="line"></span><br><span class="line"># Draw the predicted bounding box</span><br><span class="line">def drawPred(frame, classId, conf, left, top, right, bottom):</span><br><span class="line">    # Draw a bounding box.</span><br><span class="line">    cv.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)</span><br><span class="line">    </span><br><span class="line">    label = &#x27;%.2f&#x27; % conf</span><br><span class="line">        </span><br><span class="line">    # Get the label for the class name and its confidence</span><br><span class="line">    if classes:</span><br><span class="line">        assert(classId &lt; len(classes))</span><br><span class="line">        label = &#x27;%s:%s&#x27; % (classes[classId], label)</span><br><span class="line"></span><br><span class="line">    #Display the label at the top of the bounding box</span><br><span class="line">    labelSize, baseLine = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)</span><br><span class="line">    top = max(top, labelSize[1])</span><br><span class="line">    cv.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (255, 255, 255), cv.FILLED)</span><br><span class="line">    cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)</span><br><span class="line"></span><br><span class="line"># Remove the bounding boxes with low confidence using non-maxima suppression</span><br><span class="line">def postprocess(frame, outs):</span><br><span class="line">    frameHeight = frame.shape[0]</span><br><span class="line">    frameWidth = frame.shape[1]</span><br><span class="line"></span><br><span class="line">    # Scan through all the bounding boxes output from the network and keep only the</span><br><span class="line">    # ones with high confidence scores. Assign the box&#x27;s class label as the class with the highest score.</span><br><span class="line">    classIds = []</span><br><span class="line">    confidences = []</span><br><span class="line">    boxes = []</span><br><span class="line">    for out in outs:</span><br><span class="line">        for detection in out:</span><br><span class="line">            scores = detection[5:]</span><br><span class="line">            classId = np.argmax(scores)</span><br><span class="line">            confidence = scores[classId]</span><br><span class="line">            if confidence &gt; confThreshold:</span><br><span class="line">                center_x = int(detection[0] * frameWidth)</span><br><span class="line">                center_y = int(detection[1] * frameHeight)</span><br><span class="line">                width = int(detection[2] * frameWidth)</span><br><span class="line">                height = int(detection[3] * frameHeight)</span><br><span class="line">                left = int(center_x - width / 2)</span><br><span class="line">                top = int(center_y - height / 2)</span><br><span class="line">                classIds.append(classId)</span><br><span class="line">                confidences.append(float(confidence))</span><br><span class="line">                boxes.append([left, top, width, height])</span><br><span class="line">    global _i</span><br><span class="line">    # Perform non maximum suppression to eliminate redundant overlapping boxes with</span><br><span class="line">    # lower confidences.</span><br><span class="line">    indices = cv.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)</span><br><span class="line">    for i in indices:</span><br><span class="line">        i = i[0]</span><br><span class="line">        box = boxes[i]</span><br><span class="line">        left = box[0]</span><br><span class="line">        top = box[1]</span><br><span class="line">        width = box[2]</span><br><span class="line">        height = box[3]</span><br><span class="line"></span><br><span class="line">        ## save crop image</span><br><span class="line">        crop_img = frame[top:top+height, left:left+width, ]</span><br><span class="line">        #resized_img = cv.resize(crop_img, (100, 100))</span><br><span class="line">        #if _i % 5 == 0:</span><br><span class="line">        #cv.imwrite(&#x27;save_imgs/&#x27;+str(uuid.uuid1())+&#x27;.jpg&#x27;, crop_img)</span><br><span class="line">        _i = _i + 1</span><br><span class="line">        drawPred(frame, classIds[i], confidences[i], left, top, left + width, top + height)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def processing_yolov3(args):</span><br><span class="line"></span><br><span class="line">    net = cv.dnn.readNetFromDarknet(FLAGS.modelConfiguration, FLAGS.modelWeights)</span><br><span class="line">    net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)</span><br><span class="line">    net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)</span><br><span class="line"></span><br><span class="line">    # Process inputs</span><br><span class="line">    winName = &#x27;Deep learning object detection in OpenCV&#x27;</span><br><span class="line">    cv.namedWindow(winName, cv.WINDOW_NORMAL)</span><br><span class="line"></span><br><span class="line">    outputFile = &quot;yolo_out_py.avi&quot;</span><br><span class="line">    if (args.image):</span><br><span class="line">        # Open the image file</span><br><span class="line">        if not os.path.isfile(args.image):</span><br><span class="line">            print(&quot;Input image file &quot;, args.image, &quot; doesn&#x27;t exist&quot;)</span><br><span class="line">            sys.exit(1)</span><br><span class="line">        cap = cv.VideoCapture(args.image)</span><br><span class="line">        outputFile = args.image[:-4]+&#x27;_yolo_out_py.jpg&#x27;</span><br><span class="line">    elif (args.video):</span><br><span class="line">        # Open the video file</span><br><span class="line">        if not os.path.isfile(args.video):</span><br><span class="line">            print(&quot;Input video file &quot;, args.video, &quot; doesn&#x27;t exist&quot;)</span><br><span class="line">            sys.exit(1)</span><br><span class="line">        cap = cv.VideoCapture(args.video)</span><br><span class="line">        outputFile = args.video[:-4]+&#x27;_yolo_out_py.avi&#x27;</span><br><span class="line">    else:</span><br><span class="line">        # Webcam input</span><br><span class="line">        cap = cv.VideoCapture(FLAGS.camera_id)</span><br><span class="line"></span><br><span class="line">        cap.set(3, 720)</span><br><span class="line">        cap.set(4, 1280)</span><br><span class="line"></span><br><span class="line">    # Get the video writer initialized to save the output video</span><br><span class="line">    if (not args.image):</span><br><span class="line">        vid_writer = cv.VideoWriter(outputFile, cv.VideoWriter_fourcc(&#x27;M&#x27;,&#x27;J&#x27;,&#x27;P&#x27;,&#x27;G&#x27;), 30, (round(cap.get(cv.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv.CAP_PROP_FRAME_HEIGHT))))</span><br><span class="line"></span><br><span class="line">    while cv.waitKey(1) &lt; 0:</span><br><span class="line"></span><br><span class="line">        # get frame from the video</span><br><span class="line">        hasFrame, frame = cap.read()</span><br><span class="line"></span><br><span class="line">        # Stop the program if reached end of video</span><br><span class="line">        if not hasFrame:</span><br><span class="line">            print(&quot;Done processing !!!&quot;)</span><br><span class="line">            print(&quot;Output file is stored as &quot;, outputFile)</span><br><span class="line">            cv.waitKey(3000)</span><br><span class="line">            # Release device</span><br><span class="line">            cap.release()</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">        # Create a 4D blob from a frame.</span><br><span class="line">        blob = cv.dnn.blobFromImage(frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)</span><br><span class="line"></span><br><span class="line">        # Sets the input to the network</span><br><span class="line">        net.setInput(blob)</span><br><span class="line"></span><br><span class="line">        # Runs the forward pass to get output of the output layers</span><br><span class="line">        outs = net.forward(getOutputsNames(net))</span><br><span class="line"></span><br><span class="line">        # Remove the bounding boxes with low confidence</span><br><span class="line">        postprocess(frame, outs)</span><br><span class="line"></span><br><span class="line">        # Put efficiency information. The function getPerfProfile returns the overall time for inference(t) and the timings for each of the layers(in layersTimes)</span><br><span class="line">        t, _ = net.getPerfProfile()</span><br><span class="line">        label = &#x27;Inference time: %.2f ms&#x27; % (t * 1000.0 / cv.getTickFrequency())</span><br><span class="line">        cv.putText(frame, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))</span><br><span class="line"></span><br><span class="line">        # Write the frame with the detection boxes</span><br><span class="line">        if (args.image):</span><br><span class="line">            cv.imwrite(outputFile, frame.astype(np.uint8))</span><br><span class="line">        else:</span><br><span class="line">            vid_writer.write(frame.astype(np.uint8))</span><br><span class="line"></span><br><span class="line">        cv.imshow(winName, frame)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    parser = argparse.ArgumentParser(description=&#x27;Object Detection using YOLO in OPENCV&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--image&#x27;, help=&#x27;Path to image file.&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--video&#x27;, help=&#x27;Path to video file.&#x27;)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    processing_yolov3(args)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;date, 2018-11-07 14:48:57&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Data-collection-amp-labeling&quot;&gt;&lt;a href=&quot;#Data-collection-amp-labeling&quot; class=&quot;headerlin</summary>
      
    
    
    
    
    <category term="yolov3" scheme="http://example.com/tags/yolov3/"/>
    
    <category term="darknet" scheme="http://example.com/tags/darknet/"/>
    
  </entry>
  
  <entry>
    <title>Simple REST api to implement the age_gender_pred model</title>
    <link href="http://example.com/2021/08/08/Simple-REST-api/"/>
    <id>http://example.com/2021/08/08/Simple-REST-api/</id>
    <published>2021-08-08T06:51:32.000Z</published>
    <updated>2020-06-23T01:07:12.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>As we can see that the <strong>simeple REST api</strong> like this</p></li><li><p><img src="/img/simple-rest-api/rest-api.png"></p></li></ul><h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">def age_gender_predict(image_np):</span><br><span class="line">    </span><br><span class="line">    data = &#123;&quot;success&quot;: False&#125;</span><br><span class="line">    </span><br><span class="line">    orig_height, orig_width, _ = image_np.shape</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    # 1. Compose and send the request to db</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    #image_np = preprocess_image_pd(image_np)</span><br><span class="line">    #image_np = image_np.copy(order=&quot;C&quot;)</span><br><span class="line">    image_id = str(uuid.uuid4())</span><br><span class="line">    input_data = &#123;&#x27;id&#x27;: image_id, &#x27;image&#x27;: helpers.base64_encode_image(image_np)&#125;</span><br><span class="line">    </span><br><span class="line">    db.rpush(settings.IMAGE_QUEUE, json.dumps(input_data))</span><br><span class="line">    </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    # 2. Loop the db to get the response</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    while True:</span><br><span class="line">        output_data = db.get(image_id)</span><br><span class="line">        if output_data is not None:</span><br><span class="line">            output_data = output_data.decode(&quot;utf-8&quot;)</span><br><span class="line">            output_data = json.loads(output_data)</span><br><span class="line">            output_data[&#x27;out_boxes&#x27;] = np.array(output_data[&#x27;out_boxes&#x27;]).astype(&#x27;float32&#x27;)</span><br><span class="line">            output_data[&#x27;out_ages&#x27;] = np.array(output_data[&#x27;out_ages&#x27;]).astype(&#x27;int64&#x27;)</span><br><span class="line">            output_data[&#x27;out_genders&#x27;] = np.array(output_data[&#x27;out_genders&#x27;])</span><br><span class="line">            data[&quot;predictions&quot;] = output_data</span><br><span class="line">            </span><br><span class="line">            db.delete(image_id)</span><br><span class="line">            break</span><br><span class="line">        </span><br><span class="line">        time.sleep(settings.CLIENT_SLEEP)</span><br><span class="line">        </span><br><span class="line">    data[&quot;success&quot;] = True</span><br><span class="line">    </span><br><span class="line">    return data</span><br></pre></td></tr></table></figure><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">### Par-2. 模型服务主要函数</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">def detect_process():</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    #1.  加载模型</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    logfile.addinfolog(&quot;Loading model...&quot;)</span><br><span class="line">    model = FaceCV(depth=settings.NETWORK_DEPTH, width=settings.NETWORK_WIDTH)</span><br><span class="line">    logfile.addinfolog(&quot;Model Loaded&quot;)</span><br><span class="line"></span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    #2.  循环，从Redis获取请求数据，模型预测，返回结果到Redis</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    while True:</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        # 从Redis数据库，获取一张图片</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        age_gender_request_list = db.lrange(settings.IMAGE_QUEUE, 0, 0)</span><br><span class="line">        </span><br><span class="line">        imageID = None</span><br><span class="line">        imageNP = None</span><br><span class="line">        for age_gender_request in age_gender_request_list:</span><br><span class="line">            age_gender_request = json.loads(age_gender_request.decode(&quot;utf-8&quot;))</span><br><span class="line">            imageID = age_gender_request[&#x27;id&#x27;]</span><br><span class="line">            imageNP = base64_decode_image(age_gender_request[&#x27;image&#x27;], settings.IMAGE_DTYPE,</span><br><span class="line">                                          (settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH,</span><br><span class="line">                                           settings.IMAGE_CHANS))</span><br><span class="line">            print(imageNP.shape)</span><br><span class="line"></span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        # 模型处理</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        if imageNP is not None:</span><br><span class="line">            logfile.addinfolog(&quot;Start Process Request&quot;)</span><br><span class="line">            out_boxes, out_ages, out_genders = model.detect_face_one_image(imageNP)</span><br><span class="line">            logfile.addinfolog(&quot;End Process Request&quot;)</span><br><span class="line">            predicted_result = &#123;</span><br><span class="line">                &#x27;out_boxes&#x27;: np.array(out_boxes).astype(str).tolist(),</span><br><span class="line">                &#x27;out_ages&#x27;: np.array(out_ages).astype(str).tolist(),</span><br><span class="line">                &#x27;out_genders&#x27;: out_genders</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            db.set(imageID, json.dumps(predicted_result))</span><br><span class="line">            db.ltrim(settings.IMAGE_QUEUE, 1, -1)</span><br><span class="line"></span><br><span class="line">        # 睡眠</span><br><span class="line">        time.sleep(settings.SERVER_SLEEP)</span><br></pre></td></tr></table></figure><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /path/age_gender_pred</span><br><span class="line">python run_age_gender_rf_service.py</span><br><span class="line"></span><br><span class="line">python age_gender_predict_client.py</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;As we can see that the &lt;strong&gt;simeple REST api&lt;/strong&gt; like this&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;img src=&quot;/img/simple-rest-api/rest-api.png&quot;</summary>
      
    
    
    
    
    <category term="keras" scheme="http://example.com/tags/keras/"/>
    
    <category term="REST api" scheme="http://example.com/tags/REST-api/"/>
    
    <category term="redis" scheme="http://example.com/tags/redis/"/>
    
    <category term="deep learning" scheme="http://example.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>comparison of MTCNN on RK3399</title>
    <link href="http://example.com/2021/03/08/comparison-of-MTCNN-on-RK3399/"/>
    <id>http://example.com/2021/03/08/comparison-of-MTCNN-on-RK3399/</id>
    <published>2021-03-08T04:04:23.000Z</published>
    <updated>2020-06-23T01:08:20.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>date, 2019-03-08 12:04:23</li></ul><h2 id="在RK板上运行mtcnn的方案"><a href="#在RK板上运行mtcnn的方案" class="headerlink" title="在RK板上运行mtcnn的方案"></a>在RK板上运行mtcnn的方案</h2><h3 id="1-运行caffe版本的mtcnn"><a href="#1-运行caffe版本的mtcnn" class="headerlink" title="1. 运行caffe版本的mtcnn"></a>1. 运行caffe版本的mtcnn</h3><ul><li>在RK上安装OPENCL的Caffe(环境配置)<ul><li>先装OPENBLAS，后装Caffe</li><li>中间遇到ViennaCL库没找到，但是安装出错</li></ul></li><li>在RK板上运行python版的mtcnn-caffe</li><li>在RK板上运行C++版的mtcnn-caffe</li></ul><h3 id="2-运行tensorflow-lite版本的mtcnn"><a href="#2-运行tensorflow-lite版本的mtcnn" class="headerlink" title="2. 运行tensorflow-lite版本的mtcnn"></a>2. 运行tensorflow-lite版本的mtcnn</h3><ul><li>在RK板上安装tensorflow-lite(环境配置)<ul><li>这一部分的教程较少，安装出错</li></ul></li><li>把mtcnn的模型转换成lite模型</li><li>运行mtcnn</li></ul><h3 id="3-测试mtcnn-tensorflow-在不同机器上的fps"><a href="#3-测试mtcnn-tensorflow-在不同机器上的fps" class="headerlink" title="3. 测试mtcnn(tensorflow)在不同机器上的fps"></a>3. 测试mtcnn(tensorflow)在不同机器上的fps</h3><p>星期五, 08. 三月 2019 11:32上午 </p><ul><li><p>test_video : outpy4.avi</p></li><li><p>image size : 480 x 640</p></li></ul><hr><table><thead><tr><th>fps</th><th>CPU</th><th>GPU</th></tr></thead><tbody><tr><td>HP_Zhan(MX150)2G</td><td>5.05</td><td>12.04</td></tr><tr><td>JTx2 (Pascal GPU) 8G</td><td>4.00</td><td>6.23</td></tr><tr><td>RK3399   (No GPU)</td><td>1.52</td><td>None</td></tr></tbody></table><hr><ul><li>image size : 216 x 512</li></ul><hr><table><thead><tr><th>fps</th><th>CPU</th><th>GPU</th></tr></thead><tbody><tr><td>HP_Zhan(MX150)&#x2F;2G</td><td>10.45</td><td>22.96</td></tr><tr><td>JTx2 (Pascal GPU) &#x2F;8G</td><td>8.64</td><td>11.89</td></tr><tr><td>RK3399  (No GPU)</td><td>2.81</td><td>None</td></tr></tbody></table><h3 id="4-优化mtcnn的思路"><a href="#4-优化mtcnn的思路" class="headerlink" title="4. 优化mtcnn的思路"></a>4. 优化mtcnn的思路</h3><p>reference : <a class="link"   href="https://blog.csdn.net/Relocy/article/details/84075570" >MTCNN优化和另类用法<i class="fas fa-external-link-alt"></i></a></p><ul><li>MTCNN速度的瓶颈<ul><li>图片越大Pnet耗时也就越大。</li><li>人脸越多Onet和Rnet耗时越大。</li><li>噪点比较多的夜晚图像会导致Pnet误检测增多。</li></ul></li></ul><ul><li><p>从input_size入手，缩小input_size可以加快速度，如：480x640 -&gt; 216 x 512，可以提高40%左右的速率</p></li><li><p>换caffe框架，想办法把RK板上的GPU利用起来实现加速</p></li></ul><h3 id="5-caffe2-VS-tensorflow-MTCNN"><a href="#5-caffe2-VS-tensorflow-MTCNN" class="headerlink" title="5. caffe2 VS tensorflow (MTCNN)"></a>5. caffe2 VS tensorflow (MTCNN)</h3><ul><li>在HP-Zhan-Tim上跑GPU，每个流程跑3次</li></ul><table><thead><tr><th>Framwork</th><th>tensorflow</th><th>caffe2</th></tr></thead><tbody><tr><td>FPS</td><td>GPU-1&#x2F;GPU-2&#x2F;GPU-3</td><td>GPU-1&#x2F;GPU-2&#x2F;GPU-3</td></tr><tr><td>480x640</td><td><strong>13.41</strong>&#x2F;13.08&#x2F;12.75</td><td>13.69&#x2F;<strong>13.73</strong>&#x2F;13.58</td></tr><tr><td>216X512</td><td>22.43&#x2F;22.62&#x2F;<strong>23.09</strong></td><td><strong>30.01</strong>&#x2F;29.25&#x2F;29.82</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;date, 2019-03-08 12:04:23&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;在RK板上运行mtcnn的方案&quot;&gt;&lt;a href=&quot;#在RK板上运行mtcnn的方案&quot; class=&quot;headerlink&quot; title=&quot;在RK板上运行mtcnn的方案&quot;</summary>
      
    
    
    
    
    <category term="mtcnn" scheme="http://example.com/tags/mtcnn/"/>
    
    <category term="RK3399" scheme="http://example.com/tags/RK3399/"/>
    
  </entry>
  
  <entry>
    <title>Training your own data with TF object detection API</title>
    <link href="http://example.com/2021/02/03/Training-your-own-data-with-TF-object-detection-API/"/>
    <id>http://example.com/2021/02/03/Training-your-own-data-with-TF-object-detection-API/</id>
    <published>2021-02-03T06:28:07.000Z</published>
    <updated>2020-06-23T01:06:15.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="System-Info"><a href="#System-Info" class="headerlink" title="System Info"></a>System Info</h2><ul><li>Ubuntu 16.04</li><li>Git</li><li>TF 2.0</li><li>pillow</li><li>lxml</li><li>protobuf ( &gt; 3.3 , my version, 3.11.2)</li><li><a class="link"   href="https://www.cnblogs.com/gezhuangzhuang/p/10613468.html" >ref1-tensorflow+ssd_mobilenet实现目标检测的训练<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="https://blog.csdn.net/dy_guox/article/details/79111949" >ref2-（更新视频教程）Tensorflow object detection API 搭建属于自己的物体识别模型（2）——训练并使用自己的模型<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="https://blog.csdn.net/linolzhang/article/details/87121875" >ref3-Tensorflow object detection API训练自己的数据<i class="fas fa-external-link-alt"></i></a></li></ul><h2 id="To-dos"><a href="#To-dos" class="headerlink" title="To-dos"></a>To-dos</h2><ul><li><a class="link"   href="https://github.com/tensorflow/models/tree/master/research/object_detection" >TF object detection API<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="Env-prepare"><a href="#Env-prepare" class="headerlink" title="Env prepare"></a>Env prepare</h3><ul><li>Clone the <a class="link"   href="https://github.com/tensorflow/models" >model repository<i class="fas fa-external-link-alt"></i></a> into local</li><li><a class="link"   href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md" >Guid for installation<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="Make-your-own-dataset"><a href="#Make-your-own-dataset" class="headerlink" title="Make your own dataset"></a>Make your own dataset</h3><ul><li>For us, we have the yolo format annotaion files(txt files), but TFRecord format data is fit to the tensorlow.</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yolo-2-voc.py</span><br><span class="line">voc-2-csv.py</span><br><span class="line">csv-2-tfrecord.py</span><br></pre></td></tr></table></figure><h4 id="yolo-to-voc"><a href="#yolo-to-voc" class="headerlink" title="yolo to voc"></a>yolo to voc</h4><ul><li>Prepare two folders, one for <strong>annotation files</strong> and the other for the <strong>image files</strong>. VOC format(xml files) will save into the <strong>converted_lanbels</strong> folder.</li><li><strong>manual change your own data label-mappings</strong></li><li>Notice that the value of <strong>(x, y, width, height) are integers</strong> .<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"># Script to convert yolo annotations to voc format</span><br><span class="line"></span><br><span class="line"># Sample format</span><br><span class="line"># &lt;annotation&gt;</span><br><span class="line">#     &lt;folder&gt;_image_fashion&lt;/folder&gt;</span><br><span class="line">#     &lt;filename&gt;brooke-cagle-39574.jpg&lt;/filename&gt;</span><br><span class="line">#     &lt;size&gt;</span><br><span class="line">#         &lt;width&gt;1200&lt;/width&gt;</span><br><span class="line">#         &lt;height&gt;800&lt;/height&gt;</span><br><span class="line">#         &lt;depth&gt;3&lt;/depth&gt;</span><br><span class="line">#     &lt;/size&gt;</span><br><span class="line">#     &lt;segmented&gt;0&lt;/segmented&gt;</span><br><span class="line">#     &lt;object&gt;</span><br><span class="line">#         &lt;name&gt;head&lt;/name&gt;</span><br><span class="line">#         &lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">#         &lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">#         &lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">#         &lt;bndbox&gt;</span><br><span class="line">#             &lt;xmin&gt;549&lt;/xmin&gt;</span><br><span class="line">#             &lt;ymin&gt;251&lt;/ymin&gt;</span><br><span class="line">#             &lt;xmax&gt;625&lt;/xmax&gt;</span><br><span class="line">#             &lt;ymax&gt;335&lt;/ymax&gt;</span><br><span class="line">#         &lt;/bndbox&gt;</span><br><span class="line">#     &lt;/object&gt;</span><br><span class="line"># &lt;annotation&gt;</span><br><span class="line">import os</span><br><span class="line">import xml.etree.cElementTree as ET</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">ANNOTATIONS_DIR_PREFIX = &quot;annotations&quot;</span><br><span class="line"></span><br><span class="line">DESTINATION_DIR = &quot;converted_labels&quot;</span><br><span class="line"></span><br><span class="line">CLASS_MAPPING = &#123;</span><br><span class="line">    &#x27;0&#x27;: &#x27;cream_hazelnut&#x27;,</span><br><span class="line">    &#x27;1&#x27;: &#x27;cream_berry&#x27;,</span><br><span class="line">    &#x27;2&#x27;: &#x27;cream_cherry&#x27;,</span><br><span class="line">    &#x27;3&#x27;: &#x27;yida_cool_lemon&#x27;,</span><br><span class="line">    &#x27;4&#x27;: &#x27;box_yogurt_mango&#x27;,</span><br><span class="line">    &#x27;5&#x27;: &#x27;white_strawberry&#x27;,</span><br><span class="line">    &#x27;6&#x27;: &#x27;cookies_lemon&#x27;,</span><br><span class="line">    &#x27;7&#x27;: &#x27;yogurt_cranberry&#x27;,</span><br><span class="line">    &#x27;8&#x27;: &#x27;box_cookies_matcha&#x27;,</span><br><span class="line">    &#x27;9&#x27;: &#x27;cookies_matcha&#x27;,</span><br><span class="line">    &#x27;10&#x27;: &#x27;yogurt_mango&#x27;,</span><br><span class="line">    &#x27;11&#x27;: &#x27;white_passionfruit&#x27;,</span><br><span class="line">    &#x27;12&#x27;: &#x27;yida_cool_litchi&#x27;,</span><br><span class="line">    &#x27;13&#x27;: &#x27;box_white_strawberry&#x27;</span><br><span class="line">    # Add your remaining classes here.</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_root(file_prefix, width, height):</span><br><span class="line">    root = ET.Element(&quot;annotations&quot;)</span><br><span class="line">    ET.SubElement(root, &quot;filename&quot;).text = &quot;&#123;&#125;.jpg&quot;.format(file_prefix)</span><br><span class="line">    ET.SubElement(root, &quot;folder&quot;).text = &quot;images&quot;</span><br><span class="line">    size = ET.SubElement(root, &quot;size&quot;)</span><br><span class="line">    ET.SubElement(size, &quot;width&quot;).text = str(width)</span><br><span class="line">    ET.SubElement(size, &quot;height&quot;).text = str(height)</span><br><span class="line">    ET.SubElement(size, &quot;depth&quot;).text = &quot;3&quot;</span><br><span class="line">    return root</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_object_annotation(root, voc_labels):</span><br><span class="line">    for voc_label in voc_labels:</span><br><span class="line">        obj = ET.SubElement(root, &quot;object&quot;)</span><br><span class="line">        ET.SubElement(obj, &quot;name&quot;).text = voc_label[0]</span><br><span class="line">        ET.SubElement(obj, &quot;pose&quot;).text = &quot;Unspecified&quot;</span><br><span class="line">        ET.SubElement(obj, &quot;truncated&quot;).text = str(0)</span><br><span class="line">        ET.SubElement(obj, &quot;difficult&quot;).text = str(0)</span><br><span class="line">        bbox = ET.SubElement(obj, &quot;bndbox&quot;)</span><br><span class="line">        ET.SubElement(bbox, &quot;xmin&quot;).text = str(voc_label[1])</span><br><span class="line">        ET.SubElement(bbox, &quot;ymin&quot;).text = str(voc_label[2])</span><br><span class="line">        ET.SubElement(bbox, &quot;xmax&quot;).text = str(voc_label[3])</span><br><span class="line">        ET.SubElement(bbox, &quot;ymax&quot;).text = str(voc_label[4])</span><br><span class="line">    return root</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_file(file_prefix, width, height, voc_labels):</span><br><span class="line">    root = create_root(file_prefix, width, height)</span><br><span class="line">    root = create_object_annotation(root, voc_labels)</span><br><span class="line">    tree = ET.ElementTree(root)</span><br><span class="line">    tree.write(&quot;&#123;&#125;/&#123;&#125;.xml&quot;.format(DESTINATION_DIR, file_prefix))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_file(file_path):</span><br><span class="line">    file_prefix = file_path.split(&quot;.txt&quot;)[0]</span><br><span class="line">    image_file_name = &quot;&#123;&#125;.jpg&quot;.format(file_prefix)</span><br><span class="line">    img = Image.open(&quot;&#123;&#125;/&#123;&#125;&quot;.format(&quot;images&quot;, image_file_name))</span><br><span class="line">    w, h = img.size</span><br><span class="line">    </span><br><span class="line">    with open(&quot;&#123;&#125;/&#123;&#125;&quot;.format(ANNOTATIONS_DIR_PREFIX, file_path), &#x27;r&#x27;) as file:</span><br><span class="line">        lines = file.readlines()</span><br><span class="line">        voc_labels = []</span><br><span class="line">        for line in lines:</span><br><span class="line">            voc = []</span><br><span class="line">            line = line.strip()</span><br><span class="line">            data = line.split()</span><br><span class="line">            voc.append(CLASS_MAPPING.get(data[0]))</span><br><span class="line">            bbox_width = float(data[3]) * w</span><br><span class="line">            bbox_height = float(data[4]) * h</span><br><span class="line">            center_x = float(data[1]) * w</span><br><span class="line">            center_y = float(data[2]) * h</span><br><span class="line">            voc.append(int(center_x - (bbox_width / 2)))</span><br><span class="line">            voc.append(int(center_y - (bbox_height / 2)))</span><br><span class="line">            voc.append(int(center_x + (bbox_width / 2)))</span><br><span class="line">            voc.append(int(center_y + (bbox_height / 2)))</span><br><span class="line">            voc_labels.append(voc)</span><br><span class="line">        create_file(file_prefix, w, h, voc_labels)</span><br><span class="line">    print(&quot;Processing complete for file: &#123;&#125;/&#123;&#125;&quot;.format(ANNOTATIONS_DIR_PREFIX, file_path))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def start():</span><br><span class="line">    if not os.path.exists(DESTINATION_DIR):</span><br><span class="line">        os.makedirs(DESTINATION_DIR)</span><br><span class="line">    for filename in os.listdir(ANNOTATIONS_DIR_PREFIX):</span><br><span class="line">        if filename.endswith(&#x27;txt&#x27;):</span><br><span class="line">            read_file(filename)</span><br><span class="line">        else:</span><br><span class="line">            print(&quot;Skipping file: &#123;&#125;&quot;.format(filename))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    start()</span><br></pre></td></tr></table></figure></li></ul><h4 id="train-test-split-on-xml-files"><a href="#train-test-split-on-xml-files" class="headerlink" title="train test split on xml files"></a>train test split on xml files</h4><ul><li>You can change the percentage to split the dataset manually.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import random</span><br><span class="line">import time</span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">xmlfilepath = r&#x27;./Annotations&#x27;</span><br><span class="line">saveBasePath = r&quot;./&quot;</span><br><span class="line"></span><br><span class="line">trainval_percent = 0.8</span><br><span class="line">train_percent = 0.8</span><br><span class="line">total_xml = os.listdir(xmlfilepath)</span><br><span class="line">num = len(total_xml)</span><br><span class="line">list = range(num)</span><br><span class="line">tv = int(num * trainval_percent)</span><br><span class="line">tr = int(tv * train_percent)</span><br><span class="line">trainval = random.sample(list, tv)</span><br><span class="line">train = random.sample(trainval, tr)</span><br><span class="line">print(&quot;train and val size&quot;, tv)</span><br><span class="line">print(&quot;train size&quot;, tr)</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line">test_num = 0</span><br><span class="line">val_num = 0</span><br><span class="line">train_num = 0</span><br><span class="line">print(&#x27;total xml : &#123;&#125;&#x27;.format(total_xml))</span><br><span class="line"></span><br><span class="line">for i in list:</span><br><span class="line">    name = total_xml[i]</span><br><span class="line">    # print(&#x27;name : &#123;&#125;&#x27;.format(name))</span><br><span class="line">    if i in trainval:  # train and val set</span><br><span class="line">        if i in train:</span><br><span class="line">            directory = &quot;train&quot;</span><br><span class="line">            train_num += 1</span><br><span class="line">            xml_path = os.path.join(os.getcwd(), &#x27;&#123;&#125;&#x27;.format(directory))</span><br><span class="line">            if (not os.path.exists(xml_path)):</span><br><span class="line">                os.mkdir(xml_path)</span><br><span class="line">            filePath = os.path.join(xmlfilepath, name)</span><br><span class="line">            newfile = os.path.join(saveBasePath, os.path.join(directory, name))</span><br><span class="line">            # print(&#x27;newfile : &#123;&#125;&#x27;.format(newfile))</span><br><span class="line">            shutil.copyfile(filePath, newfile)</span><br><span class="line">        else:</span><br><span class="line">            directory = &quot;validation&quot;</span><br><span class="line">            xml_path = os.path.join(os.getcwd(), &#x27;&#123;&#125;&#x27;.format(directory))</span><br><span class="line">            if (not os.path.exists(xml_path)):</span><br><span class="line">                os.mkdir(xml_path)</span><br><span class="line">            val_num += 1</span><br><span class="line">            filePath = os.path.join(xmlfilepath, name)</span><br><span class="line">            newfile = os.path.join(saveBasePath, os.path.join(directory, name))</span><br><span class="line">            # print(&#x27;newfile : &#123;&#125;&#x27;.format(newfile))</span><br><span class="line">            shutil.copyfile(filePath, newfile)</span><br><span class="line">    else:</span><br><span class="line">        directory = &quot;test&quot;</span><br><span class="line">        xml_path = os.path.join(os.getcwd(), &#x27;&#123;&#125;&#x27;.format(directory))</span><br><span class="line">        if (not os.path.exists(xml_path)):</span><br><span class="line">            os.mkdir(xml_path)</span><br><span class="line">        test_num += 1</span><br><span class="line">        filePath = os.path.join(xmlfilepath, name)</span><br><span class="line">        newfile = os.path.join(saveBasePath, os.path.join(directory, name))</span><br><span class="line">        # print(&#x27;name : &#123;&#125;&#x27;.format(name))</span><br><span class="line">        shutil.copyfile(filePath, newfile)</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line">seconds = end - start</span><br><span class="line">print(&quot;train total : &quot; + str(train_num))</span><br><span class="line">print(&quot;validation total : &quot; + str(val_num))</span><br><span class="line">print(&quot;test total : &quot; + str(test_num))</span><br><span class="line">total_num = train_num + val_num + test_num</span><br><span class="line">print(&quot;total number : &quot; + str(total_num))</span><br><span class="line">print(&quot;Time taken : &#123;0&#125; seconds&quot;.format(seconds))</span><br></pre></td></tr></table></figure></li></ul><h4 id="voc-to-csv"><a href="#voc-to-csv" class="headerlink" title="voc to csv"></a>voc to csv</h4><ul><li>Transfer the xml files to csv for trian, test and validation folder individually.</li><li>You should change the save path for your own csv files.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">import pandas as pd</span><br><span class="line">import xml.etree.ElementTree as ET</span><br><span class="line"></span><br><span class="line">def xml_to_csv(path):</span><br><span class="line">    xml_list = []</span><br><span class="line">    for xml_file in glob.glob(path + &#x27;/*.xml&#x27;):</span><br><span class="line">        tree = ET.parse(xml_file)</span><br><span class="line">        root = tree.getroot()</span><br><span class="line"></span><br><span class="line">        print(root.find(&#x27;filename&#x27;).text)</span><br><span class="line">        for member in root.findall(&#x27;object&#x27;):</span><br><span class="line">            value = (root.find(&#x27;filename&#x27;).text,</span><br><span class="line">                int(root.find(&#x27;size&#x27;)[0].text),   #width</span><br><span class="line">                int(root.find(&#x27;size&#x27;)[1].text),   #height</span><br><span class="line">                member[0].text,</span><br><span class="line">                int(member[4][0].text),</span><br><span class="line">                int(float(member[4][1].text)),</span><br><span class="line">                int(member[4][2].text),</span><br><span class="line">                int(member[4][3].text)</span><br><span class="line">                )</span><br><span class="line">            xml_list.append(value)</span><br><span class="line">    column_name = [&#x27;filename&#x27;, &#x27;width&#x27;, &#x27;height&#x27;, &#x27;class&#x27;, &#x27;xmin&#x27;, &#x27;ymin&#x27;, &#x27;xmax&#x27;, &#x27;ymax&#x27;]</span><br><span class="line">    xml_df = pd.DataFrame(xml_list, columns=column_name)</span><br><span class="line">    return xml_df</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    for directory in [&#x27;train&#x27;,&#x27;test&#x27;,&#x27;validation&#x27;]:</span><br><span class="line">        xml_path = os.path.join(os.getcwd(), &#x27;./&#123;&#125;&#x27;.format(directory))</span><br><span class="line"></span><br><span class="line">        xml_df = xml_to_csv(xml_path)</span><br><span class="line">        # xml_df.to_csv(&#x27;whsyxt.csv&#x27;, index=None)</span><br><span class="line">        xml_df.to_csv(&#x27;/home/tim/workspace/models/research/object_detection/data/dove_cholo_&#123;&#125;_labels.csv&#x27;.format(directory), index=None)</span><br><span class="line">        print(&#x27;Successfully converted xml to csv.&#x27;)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure></li></ul><h4 id="csv-to-tfrecord"><a href="#csv-to-tfrecord" class="headerlink" title="csv to tfrecord"></a>csv to tfrecord</h4><ul><li>You should set your JPEGImage path.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on Tue Mar  5 15:28:55 2019</span><br><span class="line"></span><br><span class="line">@author: z</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Usage:</span><br><span class="line">  # From tensorflow/models/</span><br><span class="line">  # Create train data:</span><br><span class="line">  python generate_tfrecord.py --csv_input=data/tv_vehicle_labels.csv  --output_path=train.record</span><br><span class="line">  # Create test data:</span><br><span class="line">  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=test.record</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import io</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">from PIL import Image</span><br><span class="line">from object_detection.utils import dataset_util</span><br><span class="line">from collections import namedtuple, OrderedDict</span><br><span class="line"></span><br><span class="line">os.chdir(&#x27;/home/tim/workspace/models/research/&#x27;)</span><br><span class="line"></span><br><span class="line">flags = tf.app.flags</span><br><span class="line">flags.DEFINE_string(&#x27;csv_input&#x27;, &#x27;&#x27;, &#x27;Path to the CSV input&#x27;)</span><br><span class="line">flags.DEFINE_string(&#x27;output_path&#x27;, &#x27;&#x27;, &#x27;Path to output TFRecord&#x27;)</span><br><span class="line">FLAGS = flags.FLAGS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TO-DO replace this with label map</span><br><span class="line">def class_text_to_int(row_label):</span><br><span class="line">    # 你的所有类别, 必须从1开始，0被征用作为了背景。</span><br><span class="line">    if row_label == &#x27;cream_berry&#x27;:</span><br><span class="line">        return 1</span><br><span class="line">    elif row_label == &#x27;cream_cherry&#x27;:</span><br><span class="line">        return 2</span><br><span class="line">    elif row_label == &#x27;yida_cool_lemon&#x27;:</span><br><span class="line">        return 3</span><br><span class="line">    elif row_label == &#x27;box_yogurt_mango&#x27;:</span><br><span class="line">        return 4</span><br><span class="line">    elif row_label == &#x27;white_strawberry&#x27;:</span><br><span class="line">        return 5</span><br><span class="line">    elif row_label == &#x27;cookies_lemon&#x27;:</span><br><span class="line">        return 6</span><br><span class="line">    elif row_label == &#x27;yogurt_cranberry&#x27;:</span><br><span class="line">        return 7</span><br><span class="line">    elif row_label == &#x27;box_cookies_matcha&#x27;:</span><br><span class="line">        return 8</span><br><span class="line">    elif row_label == &#x27;cookies_matcha&#x27;:</span><br><span class="line">        return 9</span><br><span class="line">    elif row_label == &#x27;yogurt_mango&#x27;:</span><br><span class="line">        return 10</span><br><span class="line">    elif row_label == &#x27;white_passionfruit&#x27;:</span><br><span class="line">        return 11</span><br><span class="line">    elif row_label == &#x27;yida_cool_litchi&#x27;:</span><br><span class="line">        return 12</span><br><span class="line">    elif row_label == &#x27;box_white_strawberry&#x27;:</span><br><span class="line">        return 13</span><br><span class="line">    elif row_label == &#x27;cream_hazelnut&#x27;:</span><br><span class="line">        return 14</span><br><span class="line">    else:</span><br><span class="line">        return None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def split(df, group):</span><br><span class="line">    data = namedtuple(&#x27;data&#x27;, [&#x27;filename&#x27;, &#x27;object&#x27;])</span><br><span class="line">    gb = df.groupby(group)</span><br><span class="line">    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_tf_example(group, path):</span><br><span class="line">    with tf.gfile.GFile(os.path.join(path, &#x27;&#123;&#125;&#x27;.format(group.filename)), &#x27;rb&#x27;) as fid:</span><br><span class="line">        encoded_jpg = fid.read()</span><br><span class="line">    encoded_jpg_io = io.BytesIO(encoded_jpg)</span><br><span class="line">    image = Image.open(encoded_jpg_io)</span><br><span class="line">    width, height = image.size</span><br><span class="line"></span><br><span class="line">    filename = group.filename.encode(&#x27;utf8&#x27;)</span><br><span class="line">    image_format = b&#x27;jpg&#x27;</span><br><span class="line">    xmins = []</span><br><span class="line">    xmaxs = []</span><br><span class="line">    ymins = []</span><br><span class="line">    ymaxs = []</span><br><span class="line">    classes_text = []</span><br><span class="line">    classes = []</span><br><span class="line"></span><br><span class="line">    for index, row in group.object.iterrows():</span><br><span class="line">        xmins.append(row[&#x27;xmin&#x27;] / width)</span><br><span class="line">        xmaxs.append(row[&#x27;xmax&#x27;] / width)</span><br><span class="line">        ymins.append(row[&#x27;ymin&#x27;] / height)</span><br><span class="line">        ymaxs.append(row[&#x27;ymax&#x27;] / height)</span><br><span class="line">        classes_text.append(row[&#x27;class&#x27;].encode(&#x27;utf8&#x27;))</span><br><span class="line">        classes.append(class_text_to_int(row[&#x27;class&#x27;]))</span><br><span class="line"></span><br><span class="line">    tf_example = tf.train.Example(features=tf.train.Features(feature=&#123;</span><br><span class="line">        &#x27;image/height&#x27;: dataset_util.int64_feature(height),</span><br><span class="line">        &#x27;image/width&#x27;: dataset_util.int64_feature(width),</span><br><span class="line">        &#x27;image/filename&#x27;: dataset_util.bytes_feature(filename),</span><br><span class="line">        &#x27;image/source_id&#x27;: dataset_util.bytes_feature(filename),</span><br><span class="line">        &#x27;image/encoded&#x27;: dataset_util.bytes_feature(encoded_jpg),</span><br><span class="line">        &#x27;image/format&#x27;: dataset_util.bytes_feature(image_format),</span><br><span class="line">        &#x27;image/object/bbox/xmin&#x27;: dataset_util.float_list_feature(xmins),</span><br><span class="line">        &#x27;image/object/bbox/xmax&#x27;: dataset_util.float_list_feature(xmaxs),</span><br><span class="line">        &#x27;image/object/bbox/ymin&#x27;: dataset_util.float_list_feature(ymins),</span><br><span class="line">        &#x27;image/object/bbox/ymax&#x27;: dataset_util.float_list_feature(ymaxs),</span><br><span class="line">        &#x27;image/object/class/text&#x27;: dataset_util.bytes_list_feature(classes_text),</span><br><span class="line">        &#x27;image/object/class/label&#x27;: dataset_util.int64_list_feature(classes),</span><br><span class="line">    &#125;))</span><br><span class="line">    return tf_example</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(_):</span><br><span class="line">    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)</span><br><span class="line">    path = os.path.join(os.getcwd(), &#x27;object_detection/VOCdevkit/VOC2020_dove_cholo/JPEGImages/&#x27;)</span><br><span class="line">    examples = pd.read_csv(FLAGS.csv_input)</span><br><span class="line">    grouped = split(examples, &#x27;filename&#x27;)</span><br><span class="line">    num = 0</span><br><span class="line">    for group in grouped:</span><br><span class="line">        num += 1</span><br><span class="line">        tf_example = create_tf_example(group, path)</span><br><span class="line">        writer.write(tf_example.SerializeToString())</span><br><span class="line">        if (num % 100 == 0):  # 每完成100个转换，打印一次</span><br><span class="line">            print(num)</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line">    output_path = os.path.join(os.getcwd(), FLAGS.output_path)</span><br><span class="line">    print(&#x27;Successfully created the TFRecords: &#123;&#125;&#x27;.format(output_path))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure></li><li>command to generate tfrecord files<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd models/research/</span><br><span class="line"></span><br><span class="line">python generate_tfrecord.py --csv_input=object_detection/data/dove_cholo_test_labels.csv --output_path=dove_test.tfrecord</span><br><span class="line"></span><br><span class="line">python generate_tfrecord.py --csv_input=object_detection/data/dove_cholo_validation_labels.csv --output_path=dove_validation.tfrecord</span><br><span class="line"></span><br><span class="line">python generate_tfrecord.py --csv_input=object_detection/data/dove_cholo_train_labels.csv --output_path=dove_train.tfrecord</span><br></pre></td></tr></table></figure></li></ul><h3 id="Training-model"><a href="#Training-model" class="headerlink" title="Training model"></a>Training model</h3><ul><li>Things to prepare<ul><li>create your own label-map.pbtxt<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cd models/research/object_detection/data</span><br><span class="line">create label-map.pbtxt</span><br><span class="line">contents are belows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">item &#123;</span><br><span class="line">  id: 1    # id 从1开始编号</span><br><span class="line">  name: &#x27;red pedestrian&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">item &#123;</span><br><span class="line">  id: 2</span><br><span class="line">  name: &#x27;green pedestrian&#x27;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>model config file list<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">cd object_detection/samples/config/</span><br><span class="line"></span><br><span class="line">(base) tim@tim-System-Product-Name:~/workspace/models/research/object_detection/samples/configs$ tree</span><br><span class="line">.</span><br><span class="line">├── embedded_ssd_mobilenet_v1_coco.config</span><br><span class="line">├── facessd_mobilenet_v2_quantized_320x320_open_image_v4.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_coco.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_oid.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_oid_v4.config</span><br><span class="line">├── faster_rcnn_inception_resnet_v2_atrous_pets.config</span><br><span class="line">├── faster_rcnn_inception_v2_coco.config</span><br><span class="line">├── faster_rcnn_inception_v2_pets.config</span><br><span class="line">├── faster_rcnn_nas_coco.config</span><br><span class="line">├── faster_rcnn_resnet101_atrous_coco.config</span><br><span class="line">├── faster_rcnn_resnet101_ava_v2.1.config</span><br><span class="line">├── faster_rcnn_resnet101_coco.config</span><br><span class="line">├── faster_rcnn_resnet101_fgvc.config</span><br><span class="line">├── faster_rcnn_resnet101_kitti.config</span><br><span class="line">├── faster_rcnn_resnet101_pets.config</span><br><span class="line">├── faster_rcnn_resnet101_voc07.config</span><br><span class="line">├── faster_rcnn_resnet152_coco.config</span><br><span class="line">├── faster_rcnn_resnet152_pets.config</span><br><span class="line">├── faster_rcnn_resnet50_coco.config</span><br><span class="line">├── faster_rcnn_resnet50_fgvc.config</span><br><span class="line">├── faster_rcnn_resnet50_pets.config</span><br><span class="line">├── mask_rcnn_inception_resnet_v2_atrous_coco.config</span><br><span class="line">├── mask_rcnn_inception_v2_coco.config</span><br><span class="line">├── mask_rcnn_resnet101_atrous_coco.config</span><br><span class="line">├── mask_rcnn_resnet101_pets.config</span><br><span class="line">├── mask_rcnn_resnet50_atrous_coco.config</span><br><span class="line">├── rfcn_resnet101_coco.config</span><br><span class="line">├── rfcn_resnet101_pets.config</span><br><span class="line">├── ssd_inception_v2_coco.config</span><br><span class="line">├── ssd_inception_v2_pets.config</span><br><span class="line">├── ssd_inception_v3_pets.config</span><br><span class="line">├── ssdlite_mobilenet_edgetpu_320x320_coco.config</span><br><span class="line">├── ssdlite_mobilenet_edgetpu_320x320_coco_quant.config</span><br><span class="line">├── ssdlite_mobilenet_v1_coco.config</span><br><span class="line">├── ssdlite_mobilenet_v2_coco.config</span><br><span class="line">├── ssdlite_mobilenet_v3_large_320x320_coco.config</span><br><span class="line">├── ssdlite_mobilenet_v3_small_320x320_coco.config</span><br><span class="line">├── ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_coco.config</span><br><span class="line">├── ssd_mobilenet_v1_focal_loss_pets.config</span><br><span class="line">├── ssd_mobilenet_v1_focal_loss_pets_inference.config</span><br><span class="line">├── ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_pets.config</span><br><span class="line">├── ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v1_quantized_300x300_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v2_coco.config</span><br><span class="line">├── ssd_mobilenet_v2_fpnlite_quantized_shared_box_predictor_256x256_depthmultiplier_75_coco14_sync.config</span><br><span class="line">├── ssd_mobilenet_v2_fullyconv_coco.config</span><br><span class="line">├── ssd_mobilenet_v2_oid_v4.config</span><br><span class="line">├── ssd_mobilenet_v2_pets_keras.config</span><br><span class="line">├── ssd_mobilenet_v2_quantized_300x300_coco.config</span><br><span class="line">├── ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config</span><br><span class="line">└── ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config</span><br><span class="line"></span><br><span class="line">0 directories, 57 files</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>Custom your own model config, ssd_moblienet_v1_coco.config for example</li><li>Open it and change the code.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"># SSD with Mobilenet v1 configuration for MSCOCO Dataset.</span><br><span class="line"># Users should configure the fine_tune_checkpoint field in the train config as</span><br><span class="line"># well as the label_map_path and input_path fields in the train_input_reader and</span><br><span class="line"># eval_input_reader. Search for &quot;PATH_TO_BE_CONFIGURED&quot; to find the fields that</span><br><span class="line"># should be configured.</span><br><span class="line"></span><br><span class="line">model &#123;</span><br><span class="line">  ssd &#123;</span><br><span class="line">    num_classes: 14  ## change here</span><br><span class="line">    box_coder &#123;</span><br><span class="line">      faster_rcnn_box_coder &#123;</span><br><span class="line">        y_scale: 10.0</span><br><span class="line">        x_scale: 10.0</span><br><span class="line">        height_scale: 5.0</span><br><span class="line">        width_scale: 5.0</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    matcher &#123;</span><br><span class="line">      argmax_matcher &#123;</span><br><span class="line">        matched_threshold: 0.5</span><br><span class="line">        unmatched_threshold: 0.5</span><br><span class="line">        ignore_thresholds: false</span><br><span class="line">        negatives_lower_than_unmatched: true</span><br><span class="line">        force_match_for_each_row: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    similarity_calculator &#123;</span><br><span class="line">      iou_similarity &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    anchor_generator &#123;</span><br><span class="line">      ssd_anchor_generator &#123;</span><br><span class="line">        num_layers: 6</span><br><span class="line">        min_scale: 0.2</span><br><span class="line">        max_scale: 0.95</span><br><span class="line">        aspect_ratios: 1.0</span><br><span class="line">        aspect_ratios: 2.0</span><br><span class="line">        aspect_ratios: 0.5</span><br><span class="line">        aspect_ratios: 3.0</span><br><span class="line">        aspect_ratios: 0.3333</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    image_resizer &#123;</span><br><span class="line">      fixed_shape_resizer &#123;</span><br><span class="line">        height: 300</span><br><span class="line">        width: 300</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    box_predictor &#123;</span><br><span class="line">      convolutional_box_predictor &#123;</span><br><span class="line">        min_depth: 0</span><br><span class="line">        max_depth: 0</span><br><span class="line">        num_layers_before_predictor: 0</span><br><span class="line">        use_dropout: false</span><br><span class="line">        dropout_keep_probability: 0.8</span><br><span class="line">        kernel_size: 1</span><br><span class="line">        box_code_size: 4</span><br><span class="line">        apply_sigmoid_to_scores: false</span><br><span class="line">        conv_hyperparams &#123;</span><br><span class="line">          activation: RELU_6,</span><br><span class="line">          regularizer &#123;</span><br><span class="line">            l2_regularizer &#123;</span><br><span class="line">              weight: 0.00004</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          initializer &#123;</span><br><span class="line">            truncated_normal_initializer &#123;</span><br><span class="line">              stddev: 0.03</span><br><span class="line">              mean: 0.0</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          batch_norm &#123;</span><br><span class="line">            train: true,</span><br><span class="line">            scale: true,</span><br><span class="line">            center: true,</span><br><span class="line">            decay: 0.9997,</span><br><span class="line">            epsilon: 0.001,</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    feature_extractor &#123;</span><br><span class="line">      type: &#x27;ssd_mobilenet_v1&#x27;</span><br><span class="line">      min_depth: 16</span><br><span class="line">      depth_multiplier: 1.0</span><br><span class="line">      conv_hyperparams &#123;</span><br><span class="line">        activation: RELU_6,</span><br><span class="line">        regularizer &#123;</span><br><span class="line">          l2_regularizer &#123;</span><br><span class="line">            weight: 0.00004</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        initializer &#123;</span><br><span class="line">          truncated_normal_initializer &#123;</span><br><span class="line">            stddev: 0.03</span><br><span class="line">            mean: 0.0</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        batch_norm &#123;</span><br><span class="line">          train: true,</span><br><span class="line">          scale: true,</span><br><span class="line">          center: true,</span><br><span class="line">          decay: 0.9997,</span><br><span class="line">          epsilon: 0.001,</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    loss &#123;</span><br><span class="line">      classification_loss &#123;</span><br><span class="line">        weighted_sigmoid &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      localization_loss &#123;</span><br><span class="line">        weighted_smooth_l1 &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      hard_example_miner &#123;</span><br><span class="line">        num_hard_examples: 3000</span><br><span class="line">        iou_threshold: 0.99</span><br><span class="line">        loss_type: CLASSIFICATION</span><br><span class="line">        max_negatives_per_positive: 3</span><br><span class="line">        min_negatives_per_image: 0</span><br><span class="line">      &#125;</span><br><span class="line">      classification_weight: 1.0</span><br><span class="line">      localization_weight: 1.0</span><br><span class="line">    &#125;</span><br><span class="line">    normalize_loss_by_num_matches: true</span><br><span class="line">    post_processing &#123;</span><br><span class="line">      batch_non_max_suppression &#123;</span><br><span class="line">        score_threshold: 1e-8</span><br><span class="line">        iou_threshold: 0.6</span><br><span class="line">        max_detections_per_class: 100</span><br><span class="line">        max_total_detections: 100</span><br><span class="line">      &#125;</span><br><span class="line">      score_converter: SIGMOID</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">train_config: &#123;</span><br><span class="line">  batch_size: 24   ## change here</span><br><span class="line">  optimizer &#123;</span><br><span class="line">    rms_prop_optimizer: &#123;</span><br><span class="line">      learning_rate: &#123;</span><br><span class="line">        exponential_decay_learning_rate &#123;</span><br><span class="line">          initial_learning_rate: 0.0004</span><br><span class="line">          decay_steps: 800720</span><br><span class="line">          decay_factor: 0.95</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      momentum_optimizer_value: 0.9</span><br><span class="line">      decay: 0.9</span><br><span class="line">      epsilon: 1.0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  fine_tune_checkpoint: &quot;object_detection/finetune_cpkt/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt&quot;   ## change here</span><br><span class="line">  from_detection_checkpoint: true</span><br><span class="line">  # Note: The below line limits the training process to 200K steps, which we</span><br><span class="line">  # empirically found to be sufficient enough to train the pets dataset. This</span><br><span class="line">  # effectively bypasses the learning rate schedule (the learning rate will</span><br><span class="line">  # never decay). Remove the below line to train indefinitely.</span><br><span class="line">  num_steps: 10000    ## change here</span><br><span class="line">  data_augmentation_options &#123;</span><br><span class="line">    random_horizontal_flip &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  data_augmentation_options &#123;</span><br><span class="line">    ssd_random_crop &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">train_input_reader: &#123;</span><br><span class="line">  tf_record_input_reader &#123;</span><br><span class="line">    input_path: &quot;object_detection/data/dove_train.tfrecord&quot;   ## change here</span><br><span class="line">  &#125;</span><br><span class="line">  label_map_path: &quot;object_detection/data/dove_cholo_label_map.pbtxt&quot;   ## change here</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">eval_config: &#123;</span><br><span class="line">  num_examples: 3438    ## change here</span><br><span class="line">  # Note: The below line limits the evaluation process to 10 evaluations.</span><br><span class="line">  # Remove the below line to evaluate indefinitely.</span><br><span class="line">  max_evals: 10</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">eval_input_reader: &#123;</span><br><span class="line">  tf_record_input_reader &#123;</span><br><span class="line">    input_path: &quot;object_detection/data/dove_validation.tfrecord&quot;   ## change here</span><br><span class="line">  &#125;</span><br><span class="line">  label_map_path: &quot;object_detection/data/dove_cholo_label_map.pbtxt&quot;   ## change here</span><br><span class="line">  shuffle: false</span><br><span class="line">  num_readers: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>Download the pre-trained model<br>  <a class="link"   href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" >modle zoo<i class="fas fa-external-link-alt"></i></a></li><li>fine_tune_checkpoint: “object_detection&#x2F;finetune_cpkt&#x2F;ssd_mobilenet_v1_coco_2018_01_28&#x2F;model.ckpt”   ## change here</li></ul></li></ul><h4 id="legacy-training-同时跑train-py和eval-py"><a href="#legacy-training-同时跑train-py和eval-py" class="headerlink" title="legacy training (同时跑train.py和eval.py)"></a>legacy training (同时跑train.py和eval.py)</h4><ul><li><p>旧的训练方法，path,  &#x2F;models&#x2F;research&#x2F;object_detection&#x2F;legacy&#x2F;train.py</p></li><li><p>旧的训练方法，path,  &#x2F;models&#x2F;research&#x2F;object_detection&#x2F;legacy&#x2F;eval.py</p></li><li><p><a class="link"   href="https://blog.csdn.net/zong596568821xp/article/details/84842688" >ref-eval的使用<i class="fas fa-external-link-alt"></i></a></p></li><li><p>–logtostderr, 日志保存</p></li><li><p>–train_dir, 训练模型保存的位置</p></li><li><p>–pipeline_config_path, 模型配置文件的路径</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">## 可用GPU训练，但常常会cuda out of memory</span><br><span class="line">## 先在trian.py和eval.py中加入以下代码控制gpu的内存使用</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"> </span><br><span class="line">os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;</span><br><span class="line">config = tf.ConfigProto(allow_soft_placement = True)</span><br><span class="line">gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.35)</span><br><span class="line">config.gpu_options.allow_growth = True</span><br><span class="line"> </span><br><span class="line">sess0 = tf.InteractiveSession(config = config)</span><br><span class="line"></span><br><span class="line"># 原文链接：https://blog.csdn.net/baidu_33597755/article/details/102311000</span><br><span class="line"></span><br><span class="line">cd models/research/</span><br><span class="line"></span><br><span class="line">python object_detection/legacy/train.py \</span><br><span class="line">    --pipeline_config_path=object_detection/dove_cholo_od/config/ssd_mobilenet_v2_coco.config \</span><br><span class="line">    --train_dir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_train  \</span><br><span class="line">    --alsologtostderr</span><br><span class="line">    </span><br><span class="line"># 等train.py跑了一会之后，再运行eval.py</span><br><span class="line"></span><br><span class="line">python object_detection/legacy/eval.py \</span><br><span class="line">    --pipeline_config_path=object_detection/dove_cholo_od/config/ssd_mobilenet_v2_coco.config \</span><br><span class="line">    --checkpoint_dir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_train  \</span><br><span class="line">    --eval_dir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_eval  \</span><br><span class="line">    --logtostderr</span><br><span class="line">    </span><br></pre></td></tr></table></figure></li><li><p>Then open the tensorboard to watch the training and eval progress</p></li><li><p>Open two tensorboard at the same time</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_train --port=6005</span><br><span class="line"></span><br><span class="line">tensorboard --logdir=object_detection/dove_cholo_od/dove_train_dir/ssd_m_v2/dove_eval</span><br></pre></td></tr></table></figure><h4 id="modern-training-暂不支持GPU"><a href="#modern-training-暂不支持GPU" class="headerlink" title="modern training(暂不支持GPU)"></a>modern training(暂不支持GPU)</h4><ul><li>新的训练方法，path,   &#x2F;models&#x2F;research&#x2F;object_detection&#x2F;model_main.py<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># From the tensorflow/models/research/ directory</span><br><span class="line">python object_detection/model_main.py \</span><br><span class="line">    --pipeline_config_path=object_detection/training/ssd_mobilenet_v1_coco.config \</span><br><span class="line">    --model_dir=object_detection/training \</span><br><span class="line">    --num_train_steps=50000 \</span><br><span class="line">    --num_eval_steps=2000 \</span><br><span class="line">    --alsologtostderr</span><br></pre></td></tr></table></figure></li></ul><h3 id="Model-evaluation"><a href="#Model-evaluation" class="headerlink" title="Model evaluation"></a>Model evaluation</h3><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. ImportError: cannot import name &#x27;input_reader_pb2&#x27; from &#x27;object_detection.protos&#x27;</span><br><span class="line">solution:</span><br><span class="line"># From tensorflow/models/research/</span><br><span class="line">protoc object_detection/protos/*.proto --python_out=.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2. from nets import inception_resnet_v2 ModuleNotFoundError: No module named &#x27;nets&#x27;</span><br><span class="line">solution:</span><br><span class="line">cd model/research/</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cd model/research/slim/</span><br><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3. Not found: PATH_TO_BE_CONFIGURED; No such file or directory</span><br><span class="line">solution:</span><br><span class="line">download pre-trained cpkt model</span><br><span class="line">go into the config file and Search for &quot;PATH_TO_BE_CONFIGURED&quot; to find the fields that should be configured.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4. No module named &#x27;pycocotools&#x27;</span><br><span class="line">pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;System-Info&quot;&gt;&lt;a href=&quot;#System-Info&quot; class=&quot;headerlink&quot; title=&quot;System Info&quot;&gt;&lt;/a&gt;System Info&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;Git&lt;/l</summary>
      
    
    
    
    
    <category term="tensorflow" scheme="http://example.com/tags/tensorflow/"/>
    
    <category term="object detection" scheme="http://example.com/tags/object-detection/"/>
    
    <category term="API" scheme="http://example.com/tags/API/"/>
    
    <category term="COCO" scheme="http://example.com/tags/COCO/"/>
    
    <category term="SSD-Mobilenet" scheme="http://example.com/tags/SSD-Mobilenet/"/>
    
  </entry>
  
  <entry>
    <title>目标检测模型的metric-mAP</title>
    <link href="http://example.com/2020/06/24/mAP/"/>
    <id>http://example.com/2020/06/24/mAP/</id>
    <published>2020-06-24T01:01:49.000Z</published>
    <updated>2020-06-24T02:03:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="mAP-mean-average-precision"><a href="#mAP-mean-average-precision" class="headerlink" title="mAP, mean average precision"></a>mAP, mean average precision</h2><ul><li>mAP，英文直译是平均精确度(average precision)的平均(mean)，它是目标检测中目前主要的metric。在深入理解mAP之前，我们先来了解一下相关的概念。</li></ul><h3 id="什么是precision"><a href="#什么是precision" class="headerlink" title="什么是precision"></a>什么是precision</h3><ul><li>首先在分类问题中，有一个我们熟悉的混淆矩阵。<br><img src="/img/mAP/TP_FP.png" alt="TP_FP"></li><li>查准率，是指在所有预测为正例中真正例的比例，也就是预测的准确率。Precision &#x3D; TP &#x2F; (TP + FP)</li><li>查全率，是指在所有真正例中被正确预测的比率，也就是预测正确的覆盖率。Recall &#x3D; TP &#x2F; (TP + FN)</li></ul><h3 id="什么是IOU，交并比"><a href="#什么是IOU，交并比" class="headerlink" title="什么是IOU，交并比"></a>什么是IOU，交并比</h3><p><img src="/img/mAP/iou_equation.png" alt="TP_FP"></p><h3 id="怎么计算average-precision呢"><a href="#怎么计算average-precision呢" class="headerlink" title="怎么计算average precision呢"></a>怎么计算average precision呢</h3><ul><li>在物体检测中，每一个预测结果是包含2个部分：bounding box和class probability。bounding box通常是2点的坐标形式，(x_min, y_min, x_max, y_max)。class probility是预测的类别概率。那么，预测正确需要满足以下2个条件：<ul><li>(1) 类别正确且类别概率大于一定的阈值(P_threshold)</li><li>(2) 预测框和真实框的IOU大于一定的阈值(Iou_threshold)</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;mAP-mean-average-precision&quot;&gt;&lt;a href=&quot;#mAP-mean-average-precision&quot; class=&quot;headerlink&quot; title=&quot;mAP, mean average precision&quot;&gt;&lt;/a&gt;mAP, me</summary>
      
    
    
    
    
    <category term="deep learning" scheme="http://example.com/tags/deep-learning/"/>
    
    <category term="object detection" scheme="http://example.com/tags/object-detection/"/>
    
    <category term="mAP" scheme="http://example.com/tags/mAP/"/>
    
    <category term="classification" scheme="http://example.com/tags/classification/"/>
    
    <category term="CNN" scheme="http://example.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>nms_and_iou</title>
    <link href="http://example.com/2020/06/23/nms-and-iou/"/>
    <id>http://example.com/2020/06/23/nms-and-iou/</id>
    <published>2020-06-23T02:02:57.000Z</published>
    <updated>2020-07-13T05:23:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="IOU-Intersection-of-Union-，交并比"><a href="#IOU-Intersection-of-Union-，交并比" class="headerlink" title="IOU(Intersection of Union)，交并比"></a>IOU(Intersection of Union)，交并比</h2><ul><li>给定2个框(bounding box)，计算其交集和并集的比例。</li><li>通常bounding box的坐标表示有2种，第一种是2点坐标(x1, y1, x2, y2); 第二种是中心坐标和宽高(x, y, w, h)。在编写代码时我们会把第二种转换成2点坐标的形式进行计算。</li><li>code</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># params: boxes1: [, [xmin, ymin, xmax, ymax]]</span></span><br><span class="line"><span class="comment"># params: boxes2: [num, [xmin, ymin, xmax, ymax]]</span></span><br><span class="line"><span class="comment"># returns: iou, [weights], length=len(boxes2)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bboxes_iou</span>(<span class="params">boxes1, boxes2</span>):</span><br><span class="line">boxes1 = np.array(boxes1)</span><br><span class="line">boxes2 = np.array(boxes2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算各自面积</span></span><br><span class="line">boxes1_area = (boxes1[..., <span class="number">2</span>] - boxes1[..., <span class="number">0</span>]) * (boxes1[..., <span class="number">3</span>] - boxes1[..., <span class="number">1</span>])</span><br><span class="line">boxes2_area = (boxes2[..., <span class="number">2</span>] - boxes2[..., <span class="number">0</span>]) * (boxes2[..., <span class="number">3</span>] - boxes2[..., <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算交集</span></span><br><span class="line">left_up = np.maximum(boxes1[..., :<span class="number">2</span>], boxes2[..., :<span class="number">2</span>])</span><br><span class="line">right_down = np.maximum(boxes1[..., <span class="number">2</span>:], boxes2[..., <span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">inter_section = np.maximum(right_down - left_up, <span class="number">0.0</span>)</span><br><span class="line">inter_area = inter_section[..., <span class="number">0</span>] * inter_section[..., <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算并集</span></span><br><span class="line">union_area = boxes1_area + boxes2_area - inter_area</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算IOU</span></span><br><span class="line">iou = np.maximum(<span class="number">1.0</span> * inter_area / union_area, np.finfo(np.float32).eps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> iou</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="NMS-Non-Maximum-Suppression-极大值抑制"><a href="#NMS-Non-Maximum-Suppression-极大值抑制" class="headerlink" title="NMS(Non-Maximum Suppression), 极大值抑制"></a>NMS(Non-Maximum Suppression), 极大值抑制</h2><ul><li>通常在物体检测(object detection)中，在head中预测出了很多的框，每个框包含了物体的类别，类别得分和坐标信息。因为在检测过程产生很多冗余的预测框，那么这时候我们就会用到NMS来去除这些冗余。具体做法是：<ul><li>(1) 取得某一张图片上的某一类别的所有预测框</li><li>(2) 对其类别得分进行排序，取出得分最大的框做保留</li><li>(3) 拿得分最大的框作为基准(pivot)，和剩余的框的坐标信息做IOU的计算，并将得到的IOU进行排序</li><li>(4) 设定一个iou_threshold，把IOU大于iou_threshold过滤出来进行冗余处理</li><li>(5) 冗余处理方案有2种，hard NMS和soft NMS，前者是直接把所有过滤的框的得分置为0，后者是利用高斯函数对其得分进行抑制或者减小，IOU越大的得分减少越多或者抑制程度越大，反之就减少越少或者抑制程度越小</li><li>(6) soft NMS是为了防止2个同类密集的框被误删其中得分小的框</li></ul></li><li>code<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># params: bboxes: (xmin, ymin, xmax, ymax, score, class)</span></span><br><span class="line"><span class="comment"># params: iou_threshold, scale value</span></span><br><span class="line"><span class="comment"># params: method, &#x27;nms&#x27;, &#x27;soft-nms&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># return: best_bboxes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">bboxes, iou_threshold, sigma=<span class="number">0.3</span>, method=<span class="string">&#x27;nms&#x27;</span></span>):</span><br><span class="line"><span class="comment"># 找出所有类别的唯一值</span></span><br><span class="line">classes_in_img = <span class="built_in">list</span>(<span class="built_in">set</span>(bboxes[:<span class="number">5</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留的框</span></span><br><span class="line">best_bboxes = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按每个class进行for循环</span></span><br><span class="line"><span class="keyword">for</span> cls <span class="keyword">in</span> classes_in_img:</span><br><span class="line"><span class="comment"># 首先过滤当前class的bboxes，得到cls_bboxes</span></span><br><span class="line">cls_mask = (bboxes[:<span class="number">5</span>] == cls)</span><br><span class="line">cls_bboxes = bboxes[cls_mask]</span><br><span class="line"></span><br><span class="line"><span class="comment"># NMS开始</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(cls_bboxes) &gt; <span class="number">0</span>:</span><br><span class="line"><span class="comment"># 按照score排序得到最大score的bboxes</span></span><br><span class="line">max_ind = np.argmax(cls_bboxes[:, <span class="number">4</span>])</span><br><span class="line">best_bbox = cls_bboxes[max_ind]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从cls_bboxes中剔除最大score的bbox</span></span><br><span class="line">cls_bboxes = np.concatenate([cls_bboxes[:max_ind], cls_bboxes[max_ind+<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算IOU</span></span><br><span class="line">iou = bboxes_iou(best_bbox[np.newaxis, : <span class="number">4</span>], cls_bboxes[:, :<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化weight</span></span><br><span class="line">weight = np.ones(<span class="built_in">len</span>(iou), dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> method <span class="keyword">in</span> [<span class="string">&#x27;nms&#x27;</span>, <span class="string">&#x27;sfot-nms&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># nms过滤冗余</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">&#x27;nms&#x27;</span>:</span><br><span class="line">iou_mask = iou &gt; iou_threshold</span><br><span class="line"><span class="comment"># 把满足冗余的weight设为0</span></span><br><span class="line">weight[iou_mask] = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># soft-nms</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">&#x27;soft-nms&#x27;</span>:</span><br><span class="line"><span class="comment"># 高斯函数</span></span><br><span class="line">weight = np.exp(-(<span class="number">1.0</span> * iou **<span class="number">2</span> / sigma))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改score的值</span></span><br><span class="line">cls_bboxes[:, <span class="number">4</span>] *= weight</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过滤score为0的预测框</span></span><br><span class="line">score_mask = cls_bboxes[:, <span class="number">4</span>] &gt; <span class="number">0</span></span><br><span class="line">cls_bboxes = cls_bboxes[score_mask]</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> best_bboxes</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h2 id="NMS另一种实现方法，from-B站Bubbling"><a href="#NMS另一种实现方法，from-B站Bubbling" class="headerlink" title="NMS另一种实现方法，from B站Bubbling.."></a>NMS另一种实现方法，from B站Bubbling..</h2><ul><li>(0) backbone输出的格式是，boxes: [bs, all_boxes, 4+1+num_classes]</li><li>(1) 先处理坐标转换</li><li>(2) 再拿出1张图片中的所有boxes，对conf_threshold进行过滤</li><li>(3) 处理每个box最后预测的结果num_classes，如共有20个分类，那么取20个分类中的max_score就是这个box预测的类别，在得到相应的下标index</li><li>(4) 按类别划分boxes，然后每个类别进行如下循环<ul><li>取得boxes的conf_score（正&#x2F;背分类得分），按从大到小进行排序</li><li>将得分最大的box保留，然后得分最大的box与剩余的boxes进行iou计算</li><li>nms_threshold就是iou得分的过滤，把iou得分高于nms_threshold的boxes去掉</li><li>直至处理玩当前class的所有boxes</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">boxes, conf_threshold=<span class="number">0.5</span>, nms_threshold=<span class="number">0.4</span></span>):</span><br><span class="line">    <span class="comment"># boxes: [bs, all_boxes, 4+1+num_classes]</span></span><br><span class="line">    bs = np.shape(boxes)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将中心宽高转换成左上角和右下角的形式</span></span><br><span class="line">    shape_boxes = np.zeros_like(boxes[:,:,:<span class="number">4</span>])</span><br><span class="line">    shape_boxes[:,:,<span class="number">0</span>] = boxes[:,:,<span class="number">0</span>] - boxes[:,:,<span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">    shape_boxes[:,:,<span class="number">1</span>] = boxes[:,:,<span class="number">1</span>] - boxes[:,:,<span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">    shape_boxes[:,:,<span class="number">2</span>] = boxes[:,:,<span class="number">2</span>] + boxes[:,:,<span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">    shape_boxes[:,:,<span class="number">3</span>] = boxes[:,:,<span class="number">3</span>] + boxes[:,:,<span class="number">3</span>] / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    boxes[:,:,:<span class="number">4</span>] = shape_boxes</span><br><span class="line">    output = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(bs):</span><br><span class="line">        <span class="comment"># 取每一张图片中的所有预测框prediction</span></span><br><span class="line">        prediction = boxes[i]</span><br><span class="line">        <span class="comment"># 取出每个预测框的前景的二分类得分</span></span><br><span class="line">        score = prediction[:, <span class="number">4</span>]</span><br><span class="line">        <span class="comment"># 过滤满足conf_threshold的框</span></span><br><span class="line">        mask = score &gt; conf_threshold</span><br><span class="line">        detections = prediction[mask]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找出每个预测框box是属于哪个分类的，如80个分类中的哪一个分类label和score</span></span><br><span class="line">        class_conf = np.expand_dim(np.<span class="built_in">max</span>(detections[:,<span class="number">5</span>:], axis=-<span class="number">1</span>), axis=-<span class="number">1</span>)</span><br><span class="line">        class_pred = np.expand_dims(np.argmax(detections[:,<span class="number">5</span>:], axis=-<span class="number">1</span>), axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        detections = np.concatenate(detections[:,:,<span class="number">5</span>], class_conf, class_pred, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        unique_class = np.unique(detections[:, -<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(unique_class) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        best_box = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> unique_class:</span><br><span class="line">            cls_mask = detection[:, -<span class="number">1</span>] == c</span><br><span class="line">            detection = detections[cls_mask]</span><br><span class="line">            scores = detection[:, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">            arg_sort = np.argsort(scores)[:,:,-<span class="number">1</span>]</span><br><span class="line">            detection = detection[arg_sort]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">len</span>(detection) != <span class="number">0</span>:</span><br><span class="line">                best_box.append(detection[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(detection) == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                ious = iou(detection[<span class="number">0</span>], detection[<span class="number">1</span>:])</span><br><span class="line">                detection = detection[<span class="number">1</span>:][ious &lt; nms_threshold]</span><br><span class="line">        output.append(best_box)</span><br><span class="line">    <span class="keyword">return</span> np.array(output)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;IOU-Intersection-of-Union-，交并比&quot;&gt;&lt;a href=&quot;#IOU-Intersection-of-Union-，交并比&quot; class=&quot;headerlink&quot; title=&quot;IOU(Intersection of Union)，交并比&quot;&gt;</summary>
      
    
    
    
    
    <category term="object detection" scheme="http://example.com/tags/object-detection/"/>
    
    <category term="nms" scheme="http://example.com/tags/nms/"/>
    
    <category term="iou" scheme="http://example.com/tags/iou/"/>
    
    <category term="soft-nms" scheme="http://example.com/tags/soft-nms/"/>
    
  </entry>
  
  <entry>
    <title>from darknet yolov3 to tensorRT yolov3</title>
    <link href="http://example.com/2020/03/27/from-darknet-yolov3-to-tensorRT-yolov3/"/>
    <id>http://example.com/2020/03/27/from-darknet-yolov3-to-tensorRT-yolov3/</id>
    <published>2020-03-27T07:53:47.000Z</published>
    <updated>2020-03-27T08:32:23.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Training-your-own-yolov3-with-darknet"><a href="#Training-your-own-yolov3-with-darknet" class="headerlink" title="Training your own yolov3 with darknet"></a>Training your own yolov3 with darknet</h3><ul><li><a class="link"   href="https://chenyuqing.github.io/2018/11/07/Training-your-own-datasets-with-Darknet/" >Training your own datasets with Darknet<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="https://github.com/AlexeyAB/darknet" >An Improved Darknet from AlexeyAB<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="TensorRT-darknet-weigths-model"><a href="#TensorRT-darknet-weigths-model" class="headerlink" title="TensorRT darknet weigths model"></a>TensorRT darknet weigths model</h3><ul><li><p><a class="link"   href="https://github.com/jkjung-avt/tensorrt_demos#yolov3" >tensorrt_demos-yolov3 from jkjung-avt<i class="fas fa-external-link-alt"></i></a></p></li><li><p>Prerequisite</p><ul><li>Computer with GPU (jetson nano, gtx 1080, rtx 2070 .etc)</li><li>OS</li><li>Python3</li><li>Opencv3&#x2F;4</li><li>TensorRT 5 or later</li></ul></li><li><p>Follow the jkjung’s direction on Yolov3</p><ul><li>Install the pycuda</li><li>Install onnx 1.4.1</li></ul></li><li><p>Transform flow</p><ul><li><strong>darknet -&gt; onnx -&gt; tensorrt model</strong></li><li>Prepare your own weights model and cfg file</li><li>cd ${HOME}&#x2F;project&#x2F;tensorrt_demos&#x2F;yolov3_onnx</li><li>python3 yolov3_to_onnx.py –model yolov3-416</li><li>python3 onnx_to_tensorrt.py –model yolov3-416</li></ul></li><li><p>Test your tensorrt model</p><ul><li>cd ${HOME}&#x2F;project&#x2F;tensorrt_demos&#x2F;</li><li>python3 trt_yolov3.py –model yolov3-416 –image –filename ${HOME}&#x2F;Pictures&#x2F;dog.jpg</li><li>You may invole the errors like that [trt_outputs &#x3D; <a class="link"   href="https://github.com/jkjung-avt/tensorrt_demos/issues/55" >output.reshape(shape) for output, shape ValueError: cannot reshape array of size 20577 into shape (1,255,19,19)<i class="fas fa-external-link-alt"></i></a></li><li>You need change your class num and the class labels</li><li>If you don’t have onnx 1.4.1 installed, you might get the error, <a class="link"   href="https://github.com/jkjung-avt/tensorrt_demos/issues/70" >onnx.onnx_cpp2py_export.checker.ValidationError: Op registered for Upsample is deprecated in domain_version of 11<i class="fas fa-external-link-alt"></i></a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Training-your-own-yolov3-with-darknet&quot;&gt;&lt;a href=&quot;#Training-your-own-yolov3-with-darknet&quot; class=&quot;headerlink&quot; title=&quot;Training your own </summary>
      
    
    
    
    
    <category term="yolov3" scheme="http://example.com/tags/yolov3/"/>
    
    <category term="object detection" scheme="http://example.com/tags/object-detection/"/>
    
    <category term="darknet" scheme="http://example.com/tags/darknet/"/>
    
    <category term="TensorRT" scheme="http://example.com/tags/TensorRT/"/>
    
  </entry>
  
  <entry>
    <title>leetcode_binary_tree_series</title>
    <link href="http://example.com/2020/03/09/leetcode-binary-tree-series/"/>
    <id>http://example.com/2020/03/09/leetcode-binary-tree-series/</id>
    <published>2020-03-09T08:55:53.000Z</published>
    <updated>2020-03-11T16:01:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是树"><a href="#什么是树" class="headerlink" title="什么是树"></a>什么是树</h3><ul><li>**树(Tree)**是一种非线性结构，用来模拟具有树状结构性质的数据集合。它是由n(n&gt;0)个有限节点组成一个具有层次关系的集合。树是递归结构，在树的定义中又用到了树的概念。</li></ul><h3 id="什么是二叉树"><a href="#什么是二叉树" class="headerlink" title="什么是二叉树"></a>什么是二叉树</h3><ul><li>**二叉树(Binary tree)**是每个节点最多只有两个分支(即不存在分支度大于2的节点)的树结构。通常分支被称作“左子树”或者“右子树”。二叉树的分支具有左右次序，不能随意颠倒。二叉树可以为空。</li></ul><h3 id="完全二叉树"><a href="#完全二叉树" class="headerlink" title="完全二叉树"></a>完全二叉树</h3><ul><li>若设二叉树的深度为ℎ，除第ℎ层外，其它各层(1～ℎ−1)的结点数都达到最大个数，第ℎ层所有的结点都连续集中在最左边，这就是完全二叉树</li></ul><h3 id="什么是二叉搜索树"><a href="#什么是二叉搜索树" class="headerlink" title="什么是二叉搜索树"></a>什么是二叉搜索树</h3><ul><li>左子树的值都比根节点小，右子树的值都比根节点大。</li></ul><h3 id="树的定义-Python"><a href="#树的定义-Python" class="headerlink" title="树的定义(Python)"></a>树的定义(Python)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Definition for a binary tree node.</span><br><span class="line">class TreeNode:</span><br><span class="line">def __init__(self, x):</span><br><span class="line">self.value = x</span><br><span class="line">self.left = None</span><br><span class="line">self.right = None</span><br></pre></td></tr></table></figure><h3 id="树的遍历"><a href="#树的遍历" class="headerlink" title="树的遍历"></a>树的遍历</h3><h4 id="深度优先，通常有前序-pre-order-，中序-in-order-，后序-post-order-3种遍历方法，每一种又分递归和迭代两种实现"><a href="#深度优先，通常有前序-pre-order-，中序-in-order-，后序-post-order-3种遍历方法，每一种又分递归和迭代两种实现" class="headerlink" title="深度优先，通常有前序(pre-order)，中序(in-order)，后序(post-order)3种遍历方法，每一种又分递归和迭代两种实现"></a>深度优先，通常有前序(pre-order)，中序(in-order)，后序(post-order)3种遍历方法，每一种又分递归和迭代两种实现</h4><ul><li><strong>3种遍历经过的路径都是一样的，而且每个节点都会被访问3次，当我们在第一次访问该节点就打印出来的话，那么这就是前序；当我们第二次访问该节点才打印出来的话，那么这就是中序；当我们第三次访问该节点才打印出来的话，那么这就是后序。</strong></li><li>伪代码，递归实现<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># pseudocode，递归</span><br><span class="line">## 前序</span><br><span class="line">func preorder(root):</span><br><span class="line">if not root: return</span><br><span class="line">**print(root.val)**</span><br><span class="line">preorder(root.left)</span><br><span class="line">preorder(root.right)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 中序, 对于二叉搜索树，中序后的是一个排序数组</span><br><span class="line">func inorder(root):</span><br><span class="line">if not root: return</span><br><span class="line">inorder(root.left)</span><br><span class="line">**print(root.val)**</span><br><span class="line">inorder(root.right)</span><br><span class="line"></span><br><span class="line">## 后序</span><br><span class="line">func postorder(root):</span><br><span class="line">if not root: return</span><br><span class="line">postorder(root.left)</span><br><span class="line">postorder(root.right)</span><br><span class="line">**print(root.val)**</span><br></pre></td></tr></table></figure></li><li><strong>Python 实现</strong><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">## 前序遍历</span><br><span class="line">### 1. 递归实现</span><br><span class="line">class Solution:</span><br><span class="line">def preorderTraversal(self, root: TreeNode) -&gt; List[int]:</span><br><span class="line">ans = []</span><br><span class="line">if not root: return ans</span><br><span class="line">self.preTraversal(root, ans)</span><br><span class="line">return ans</span><br><span class="line"></span><br><span class="line">def preTraversal(self, t, ans):</span><br><span class="line">ans.append(t.val)</span><br><span class="line">if t.left:</span><br><span class="line">self.preTraversal(t.left, ans)</span><br><span class="line">if t.right:</span><br><span class="line">self.preTraversal(t.right, ans)</span><br><span class="line"></span><br><span class="line">### 2. 迭代实现，栈</span><br><span class="line">class Solution:</span><br><span class="line">def preorderTraversal(self, root: TreeNode) -&gt; List[int]:</span><br><span class="line">ans = []</span><br><span class="line">stack = []</span><br><span class="line">if not root:</span><br><span class="line">return ans</span><br><span class="line">p = root</span><br><span class="line">while(p or len(stack)):</span><br><span class="line">if p:</span><br><span class="line">ans.append(p.val)</span><br><span class="line">stack.append(p)</span><br><span class="line">p = p.left</span><br><span class="line">else:</span><br><span class="line">p = stack[-1]</span><br><span class="line">stack.pop(-1)</span><br><span class="line">p = p.right</span><br><span class="line">return ans</span><br><span class="line"></span><br><span class="line">## ---------------split line-------------------</span><br><span class="line"></span><br><span class="line">## 中序序遍历</span><br><span class="line">### 1. 递归实现</span><br><span class="line">class Solution:</span><br><span class="line">def inorderTraversal(self, root: TreeNode) -&gt; List[int]:</span><br><span class="line">ans = []</span><br><span class="line">if not root: return ans</span><br><span class="line">self.inTraversal(root, ans)</span><br><span class="line">return ans</span><br><span class="line"></span><br><span class="line">def inTraversal(self, t, ans):</span><br><span class="line">ans.append(t.val)</span><br><span class="line">if t.left:</span><br><span class="line">self.inTraversal(t.left, ans)</span><br><span class="line">if t.right:</span><br><span class="line">self.inTraversal(t.right, ans)</span><br><span class="line"></span><br><span class="line">### 2. 迭代实现</span><br><span class="line">class Solution:</span><br><span class="line">def inorderTraversal(self, root: TreeNode) -&gt; List[int]:</span><br><span class="line">ans = []</span><br><span class="line">stack = []</span><br><span class="line">if not root:</span><br><span class="line">return ans</span><br><span class="line">p = root</span><br><span class="line">while(p or len(stack)):</span><br><span class="line">if p:</span><br><span class="line">stack.append(p)</span><br><span class="line">p = p.left</span><br><span class="line">else:</span><br><span class="line">p = stack[-1]</span><br><span class="line">ans.append(p.val)</span><br><span class="line">stack.pop(-1)</span><br><span class="line">p = p.right</span><br><span class="line">return ans</span><br><span class="line"></span><br><span class="line">## ---------------split line-------------------</span><br><span class="line"></span><br><span class="line">## 后序序遍历</span><br><span class="line">### 1. 递归实现</span><br><span class="line">class Solution:</span><br><span class="line">def postorderTraversal(self, root: TreeNode) -&gt; List[int]:</span><br><span class="line">ans = []</span><br><span class="line">if not root: return ans</span><br><span class="line">self.postTraversal(root, ans)</span><br><span class="line">return ans</span><br><span class="line"></span><br><span class="line">def postTraversal(self, t, ans):</span><br><span class="line">ans.append(t.val)</span><br><span class="line">if t.left:</span><br><span class="line">self.postTraversal(t.left, ans)</span><br><span class="line">if t.right:</span><br><span class="line">self.postTraversal(t.right, ans)</span><br><span class="line"></span><br><span class="line">### 2. 迭代实现，因为前序是root-&gt;left-&gt;right, 而后序是left-&gt;right-&gt;root，刚好反过来</span><br><span class="line">class Solution:</span><br><span class="line">def postorderTraversal(self, root: TreeNode) -&gt; List[int]:</span><br><span class="line">ans = []</span><br><span class="line">stack = []</span><br><span class="line">if not root:</span><br><span class="line">return ans</span><br><span class="line">p = root</span><br><span class="line">while(p or len(stack)):</span><br><span class="line">if p:</span><br><span class="line">ans.append(p.val)</span><br><span class="line">stack.append(p)</span><br><span class="line">p = p.right</span><br><span class="line">else:</span><br><span class="line">p = stack[-1]</span><br><span class="line">stack.pop(-1)</span><br><span class="line">p = p.left</span><br><span class="line">return [val for val in reversed(ans)]</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h4 id="广度优先"><a href="#广度优先" class="headerlink" title="广度优先"></a>广度优先</h4><ul><li>层次遍历(level-order)</li><li>利用队列(queue)来实现<ul><li><ol><li>根节点先入队列</li></ol></li><li><ol start="2"><li>迭代当前队列上的所有元素</li></ol></li><li><ol start="3"><li>若该元素所指节点的左右孩子节点非空，则将其左右孩子的指针入队列，把当前元素出队列，并保存其数值到结果数组中。</li></ol></li><li><ol start="4"><li>循环重复2-4，直到队列为空。</li></ol></li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">def levelOrder(self, root: TreeNode) -&gt; List[List[int]]:</span><br><span class="line">queue = [root]</span><br><span class="line">res = []</span><br><span class="line">if not root:</span><br><span class="line">return []</span><br><span class="line">while queue:</span><br><span class="line">templist = []</span><br><span class="line">templen = len(queue)</span><br><span class="line">for i in range(templen):</span><br><span class="line">temp = queue.pop(0)</span><br><span class="line">templist.append(temp.val)</span><br><span class="line">if temp.left:</span><br><span class="line">queue.append(temp.left)</span><br><span class="line">if temp.right:</span><br><span class="line">queue.append(temp.right)</span><br><span class="line">res.append(templist)</span><br><span class="line">return res</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="链表-Linked-list-，树-Tree-，图-Graph-的关系"><a href="#链表-Linked-list-，树-Tree-，图-Graph-的关系" class="headerlink" title="链表(Linked list)，树(Tree)，图(Graph)的关系"></a>链表(Linked list)，树(Tree)，图(Graph)的关系</h4><ul><li><strong>链表</strong>是特殊的<strong>树</strong>，<strong>树</strong>是特殊的<strong>图</strong>。</li></ul><h3 id="reference-参考"><a href="#reference-参考" class="headerlink" title="reference-参考"></a>reference-参考</h3><ul><li><a class="link"   href="https://suixinblog.cn/2019/02/binary-tree.html" >二叉树基础知识+Python实现<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="https://leetcode-cn.com/explore/learn/card/data-structure-binary-tree/2/traverse-a-tree/7/" >树的遍历 - 介绍<i class="fas fa-external-link-alt"></i></a></li><li><a class="link"   href="https://www.youtube.com/watch?v=fgEZMCrFrt4" >中国MOOC-二叉树的遍历-video<i class="fas fa-external-link-alt"></i></a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;什么是树&quot;&gt;&lt;a href=&quot;#什么是树&quot; class=&quot;headerlink&quot; title=&quot;什么是树&quot;&gt;&lt;/a&gt;什么是树&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;**树(Tree)**是一种非线性结构，用来模拟具有树状结构性质的数据集合。它是由n(n&amp;gt;0)个有限节点组</summary>
      
    
    
    
    
    <category term="Tree" scheme="http://example.com/tags/Tree/"/>
    
    <category term="binary tree" scheme="http://example.com/tags/binary-tree/"/>
    
    <category term="binary search tree" scheme="http://example.com/tags/binary-search-tree/"/>
    
    <category term="pre-order" scheme="http://example.com/tags/pre-order/"/>
    
    <category term="in-order" scheme="http://example.com/tags/in-order/"/>
    
    <category term="post-order" scheme="http://example.com/tags/post-order/"/>
    
  </entry>
  
  <entry>
    <title>YouTube The Age of AI 01 How far is too far</title>
    <link href="http://example.com/2020/02/19/YT-The-Age-of-AI-01-How-far-is-too-far/"/>
    <id>http://example.com/2020/02/19/YT-The-Age-of-AI-01-How-far-is-too-far/</id>
    <published>2020-02-18T18:37:51.000Z</published>
    <updated>2020-02-18T19:36:26.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Changes-of-Era"><a href="#Changes-of-Era" class="headerlink" title="Changes of Era"></a>Changes of Era</h3><ul><li><p>writing to printing press, to email</p></li><li><p>Now it seems to we have been to the new age , the age of AI</p></li><li><p>The best way to learn one thing is to teach it.</p></li><li><p>Definition of A.I. (Artificial Intelligence)</p><ul><li>A.I. is a branch of computer science dealing with the simulation of intelligent behavior in computers.</li></ul></li><li><p>Intelligence used to be the province of only humans, but it no longer is. We don’t program the machines. They learn by themselves.</p></li><li><p>Buf first of all, you should know how to program as a human being.</p></li></ul><h3 id="Two-cases"><a href="#Two-cases" class="headerlink" title="Two cases"></a>Two cases</h3><h4 id="Virtual-Human-Project"><a href="#Virtual-Human-Project" class="headerlink" title="Virtual Human Project"></a>Virtual Human Project</h4><ul><li>Mark Sagar, CEO of Soul Machines.</li><li><a class="link"   href="https://www.soulmachines.com/solutions/" >soul machine<i class="fas fa-external-link-alt"></i></a> </li><li>Aculand, New Zealand</li><li>Baby X project</li><li>A.I. aids</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Changes-of-Era&quot;&gt;&lt;a href=&quot;#Changes-of-Era&quot; class=&quot;headerlink&quot; title=&quot;Changes of Era&quot;&gt;&lt;/a&gt;Changes of Era&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;writing to pr</summary>
      
    
    
    
    
    <category term="Youtbue" scheme="http://example.com/tags/Youtbue/"/>
    
    <category term="AgeofAI" scheme="http://example.com/tags/AgeofAI/"/>
    
  </entry>
  
  <entry>
    <title>how to build self discipline(如何自律)</title>
    <link href="http://example.com/2020/02/17/how-to-build-self-discipline/"/>
    <id>http://example.com/2020/02/17/how-to-build-self-discipline/</id>
    <published>2020-02-17T02:32:30.000Z</published>
    <updated>2020-02-18T17:16:44.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Marcus-Aurelius"><a href="#Marcus-Aurelius" class="headerlink" title="Marcus Aurelius"></a>Marcus Aurelius</h2><ul><li>hello, you are amazing!</li><li><a class="link"   href="https://www.youtube.com/watch?v=njDLNt-1ugM&feature=youtu.be" >YT-How to build self-discipline<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="Self-discipline-starts-with-finding-your-purpose"><a href="#Self-discipline-starts-with-finding-your-purpose" class="headerlink" title="Self discipline starts with finding your purpose."></a>Self discipline starts with finding your purpose.</h3><ul><li><p>(1) 自律始于找到自己的目标。</p></li><li><p>Marcus Aurelicus believed that we each have a purpose something that we were created for. It’s our duty to carry out that purpose because it is purpose that gets you out of bed each morining.</p></li><li><p>马可 奥勒留相信我们每个人生来就有一个目标。实现这个目标是我们的义务，而这个目标是我们每天早上起床的动力。</p></li><li><p>If you have a clear understanding of your goals and how your task fits within them, you are much more likely to complete it.</p></li><li><p>如果你对你的目标有一个清晰的认知并且知道如何实施，那么你就更有可能实现这个目标。</p></li><li><p>The biggest source of self discipline is to have a reason to do the task.</p></li><li><p>自律的最大动力来源是给自己找到一个完成任务的理由。</p></li><li><p>If you don’t know what to do, just start. Like if you want to be a programmer, just do the coding every day.</p></li><li><p>如果你不知道怎么开始，just do it，做简单的事。例如，你想成为一个程序员，那么你每天练习写代码，准没错。</p></li><li><p>Self discipline is about finding compelling reasons to do something, then commintting yourself to see that task or activity through to the very end.</p></li><li><p>(总结)自律就是找到一个令你信服的理由去做某件事，然后承诺自己付诸行动并且坚持到最后。</p></li></ul><h3 id="Count-on-yourself"><a href="#Count-on-yourself" class="headerlink" title="Count on yourself."></a>Count on yourself.</h3><ul><li><p>(2) 靠自己(全力以赴)。</p></li><li><p>Ture your desire to stone. Quench your appetites. Keep your mind centered on itself.</p></li><li><p>把你的欲望变成石头。抑制你的欲望，集中精力在落实上面。</p></li><li><p>We must be fully committed to doing whatever it takes to get the job done no matter what challenges get in the way.</p></li><li><p>我们必须全力以赴，无论遇到任何困难险阻。</p></li><li><p>Self discipline is the ability to set yourself to take action, to do what you need to do. Regardless of your physical, mental, or emotional state.</p></li><li><p>要自律必须要行动，做你需做的东西。无论你的身体，精神或者情绪是什么状态。一句话，全力以赴。</p></li><li><p>You can start with forming an effective plan of action that is comprised of a deadline for accomplishing your goal.</p></li><li><p>为你的目标制定有效的计划，并且设定一个最后期限。</p></li><li><p>It must also be built on the foundation of mini-milestones that break your goal down into mangeable chunks.</p></li><li><p>可以把你的目标分解成多个容易管理和预测的迷你小任务(里程碑)。</p></li><li><p>What you’re ultimately trying to avoid is succumbing to being overwhelmed. Overwhelming can quickly digress to procrastination, and procrastination can, subsequently, lead to stagnation. And, of course, where there is stagnation, self discipline cannot exist.</p></li><li><p>你应该尽量避免的是自己不给压垮。压垮自己目标就会被拖延，而拖延则会导致停滞不前。当然，停滞不前时则自律也不存在了。</p></li></ul><h3 id="Show-up-every-day"><a href="#Show-up-every-day" class="headerlink" title="Show up every day"></a>Show up every day</h3><ul><li><p>(3) 坚持不懈！</p></li><li><p>We need to show up, every dat. And put in the work.</p></li><li><p>我们应该每天坚持，认真落实。</p></li><li><p>Self-Discipline is nothing more but the habit of consistency, finding the motivation to do something again and again until you do it on autopilot and start seeing results.</p></li><li><p>自律不过是一种持之以恒的习惯，找到重复做某件事的动力，直到你会自动去做这件事并开始看到结果。</p></li><li><p>A bad day doesn’t have to become a bad week, a bad week doesn’t have to become a bad year.</p></li><li><p>糟糕的一天不会变成糟糕的一周，糟糕的一周不会变成糟糕的一年。</p></li><li><p>The moment you wake up, remember that the new day is a new life and move forward by opening your eyes and focusing on what’s in front of you, which is life itself.</p></li><li><p>你起床的每一天都是你人生新的一天，你要每天保持前进，睁开眼关注你面前的，生活！</p></li></ul><h3 id="Practice-voluntary-hardship"><a href="#Practice-voluntary-hardship" class="headerlink" title="Practice voluntary hardship"></a>Practice voluntary hardship</h3><ul><li><p>(4) 培养自愿吃苦(我太南了！)</p></li><li><p>Voluntary hardship means constantly testing ourselves and by making life routinely uncomfortable in some way.</p></li><li><p>自愿吃苦意味着要不断考验自己，习惯过一些不是很舒服的日子。</p></li><li><p>We are hardening ourselves for the day we may live it for real.</p></li><li><p>我们不断让自己进步，以备真正艰苦的那一天的到来。</p></li></ul><h3 id="Practice-dichotomy-of-control"><a href="#Practice-dichotomy-of-control" class="headerlink" title="Practice dichotomy of control"></a>Practice dichotomy of control</h3><ul><li><p>(5) 训练控制二分法</p></li><li><p>Being distressed, being bothered by small things instantly is terrible for discipline.</p></li><li><p>容易被小事情困扰对自律来说是非常糟糕的。意味着你的情绪不受自己控制。</p></li><li><p>Try to apply dichotomy of control.</p></li><li><p>尝试 利用控制二分法。</p></li><li><p>Reinforce to yourself what is within your control and what is out of your control.</p></li><li><p>总结自己能控制什么，不能控制什么。</p></li><li><p>Try to ask yourself these questions</p></li><li><p>Do you have a problem in your life ?</p></li><li><p>No, then don’t worry. </p></li><li><p>Or yes. </p></li><li><p>Can you do something about it? </p></li><li><p>Yes. Then don’t worry.</p></li><li><p>No again, then don’t worry.</p></li></ul><h3 id="Never-Play-the-victim"><a href="#Never-Play-the-victim" class="headerlink" title="Never Play the victim"></a>Never Play the victim</h3><ul><li><p>(6) 永远不要扮演受害者。</p></li><li><p>Do your job, without whining.</p></li><li><p>做好你的事情，不要抱怨。</p></li><li><p>Have you ever said these things ?</p></li><li><p>I was just born this way.</p></li><li><p>I never learned anything different.</p></li><li><p>My parents set terrible example.</p></li><li><p>Every one else does it.</p></li><li><p>Those are execuses for you to become better man.</p></li><li><p>You need to assume responsibility.</p></li><li><p>你要学会承担责任。</p></li><li><p>Your life depends on you determing what’s within your control and taking those things into your own hands.</p></li><li><p>你想过什么生活，取决于你决定你敢承担多少责任或者能控制多少事情。</p></li><li><p>Be that person who steps in to take action not the one who looks the other way and casts blame.</p></li><li><p>做一个主动解决问题的人，而不是那个推卸责任的人。</p></li><li><p>Without a sense of ownership, meaningful progress becomes an impossible task.</p></li><li><p>没有主人翁精神，有意义的进步就会变成不可能的任务。</p></li></ul><h3 id="Practice-Delayed-Gratification"><a href="#Practice-Delayed-Gratification" class="headerlink" title="Practice Delayed Gratification"></a>Practice Delayed Gratification</h3><ul><li><p>(7) 训练延迟满足。</p></li><li><p>Delayed gratification involves the ability to wait to get what you want.</p></li><li><p>延迟满足指的是你需要有耐性，等待你想得到的东西。</p></li><li><p>Put off what we want now so that we can perhaps get something else, something better, later on.</p></li><li><p>暂时搁置我们现在想要的东西，在未来我们可能得到一些其他的，更好的东西。</p></li></ul><h3 id="Ignore-Naysayers"><a href="#Ignore-Naysayers" class="headerlink" title="Ignore Naysayers"></a>Ignore Naysayers</h3><ul><li><p>(8) 忽略反对者。</p></li><li><p>Naysayers just love to say nay, criticise you, objectify you, oppose you.</p></li><li><p>反对者只是意味的说不，批判你，反对你。</p></li><li><p>Don’t hand over your peace of mind to outsiders to disrupt as they please.</p></li><li><p>不要将你内心的平静交与外人，并且按照他们的意愿去破坏。</p></li><li><p>You should actively seek honest feedback from those you respect. But if someone just has a history of being a naysayer, ignore them.</p></li><li><p>你应该积极从你尊敬的人那里获得正面的反馈，但是如果你发现他有naysayer的历史，那么请忽略他。</p></li></ul><h3 id="Find-Wise-People-to-emulate"><a href="#Find-Wise-People-to-emulate" class="headerlink" title="Find Wise People to emulate"></a>Find Wise People to emulate</h3><ul><li><p>(9) 效仿有智慧的人。</p></li><li><p>Identify role models.</p></li><li><p>寻找你的榜样。</p></li><li><p>You can ask yourself ?</p></li><li><p>Who is doing this right now ?</p></li><li><p>Who has successfully achieved this goal?</p></li><li><p>Who has successfully mastered this habit ?</p></li><li><p>Who has successfully made this change ?</p></li><li><p>Who has the necessary self-discipline in this area ?</p></li><li><p>What can I learn from this person that can help me along my journey ?</p></li></ul><h3 id="Honestly-review-your-day"><a href="#Honestly-review-your-day" class="headerlink" title="Honestly review your day"></a>Honestly review your day</h3><ul><li><p>(10) 诚实的回顾你的一天(feed backs.)。</p></li><li><p>self-awareness, self-examination, and self-determination.</p></li><li><p>自我意识，自我反省，自我决定。</p></li><li><p>One of the best ways to become more disciplined is to scrutinize yourself, and find your weak spots.</p></li><li><p>一个更自律的方法是审视自己，发现自己的弱点。</p></li><li><p>Practicing evening retrospections on a consistent basis will allow you to become more self-aware through every step of your day.</p></li><li><p>坚持每天晚上进行回顾会让你每一天变得更有自我意识。</p></li><li><p>Ask yourself today </p></li><li><p>What did I do well ?</p></li><li><p>Where was my discipline tested ?</p></li><li><p>What did I do bad ?</p></li><li><p>How can I improve ?</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Marcus-Aurelius&quot;&gt;&lt;a href=&quot;#Marcus-Aurelius&quot; class=&quot;headerlink&quot; title=&quot;Marcus Aurelius&quot;&gt;&lt;/a&gt;Marcus Aurelius&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;hello, you a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>tensorRT-yolov3</title>
    <link href="http://example.com/2020/02/10/tensorRT-yolov3/"/>
    <id>http://example.com/2020/02/10/tensorRT-yolov3/</id>
    <published>2020-02-10T06:26:53.000Z</published>
    <updated>2020-02-10T07:07:04.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TesorRT-model-process"><a href="#TesorRT-model-process" class="headerlink" title="TesorRT model process"></a>TesorRT model process</h3><ul><li><img src="/img/tensorrt_tu/tf-trt_workflow.png" alt="tf-trt_workflow" title="https://github.com/ardianumam/Tensorflow-TensorRT/blob/master/pictures/tf-trt_workflow.png"></li></ul><h3 id="Train-your-own-yolov3-model-cpkt-x2F-pb"><a href="#Train-your-own-yolov3-model-cpkt-x2F-pb" class="headerlink" title="Train your own yolov3 model(cpkt &#x2F; pb)"></a>Train your own yolov3 model(cpkt &#x2F; pb)</h3><ul><li>Since I train the yolov3 with darknet, I need to transfer the weights into tf model</li><li><a class="link"   href="https://github.com/mystic123/tensorflow-yolo-v3" >ref-tensorflow-yolo-v3<i class="fas fa-external-link-alt"></i></a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;TesorRT-model-process&quot;&gt;&lt;a href=&quot;#TesorRT-model-process&quot; class=&quot;headerlink&quot; title=&quot;TesorRT model process&quot;&gt;&lt;/a&gt;TesorRT model process&lt;/</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>rebuild my own blog after system changed</title>
    <link href="http://example.com/2020/02/03/rebuild-my-own-blog-after-system-changed/"/>
    <id>http://example.com/2020/02/03/rebuild-my-own-blog-after-system-changed/</id>
    <published>2020-02-03T06:09:27.000Z</published>
    <updated>2020-02-03T06:16:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="System-info"><a href="#System-info" class="headerlink" title="System info"></a>System info</h2><ul><li>Ubuntu 16.04</li><li>Git </li><li>Node</li></ul><h2 id="Todos"><a href="#Todos" class="headerlink" title="Todos"></a>Todos</h2><h3 id="install-nodeks-and-npm"><a href="#install-nodeks-and-npm" class="headerlink" title="install nodeks and npm"></a>install nodeks and npm</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 更新</span><br><span class="line">sudo apt-get update</span><br><span class="line"># 安装低版本的node，然后再升级最新</span><br><span class="line">sudo apt-get install nodejs</span><br><span class="line">sudo apt install nodejs-legacy</span><br><span class="line">sudo apt install npm</span><br><span class="line">#安装更新版本的工具N，执行：</span><br><span class="line">sudo npm install n -g</span><br><span class="line">#跟新node版本，执行：</span><br><span class="line">sudo n stable</span><br><span class="line">sudo node -v</span><br><span class="line"></span><br><span class="line"># install hexo-cli</span><br><span class="line">sudo npm install -g hexo-cli</span><br></pre></td></tr></table></figure><h3 id="install-and-config-git"><a href="#install-and-config-git" class="headerlink" title="install and config git"></a>install and config git</h3><ul><li>ref1, <a class="link"   href="https://motoleisure.com/2017/03/02/how-to-use-git-quickly-part1/" >https://motoleisure.com/2017/03/02/how-to-use-git-quickly-part1/<i class="fas fa-external-link-alt"></i></a></li><li>ref2, <a class="link"   href="https://motoleisure.com/2017/03/02/how-to-use-git-quickly-part2/" >https://motoleisure.com/2017/03/02/how-to-use-git-quickly-part2/<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="install-hexo-with-npm"><a href="#install-hexo-with-npm" class="headerlink" title="install hexo with npm"></a>install hexo with npm</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-(1) delete origiginal your own github.io repository</span><br><span class="line">-(2) create the new one</span><br><span class="line">-(3) git clone to local</span><br><span class="line">-(4) hexo init &lt;folder&gt;, and move the contents into local github.io repository</span><br><span class="line">-(5) move your own theme files and confg.yaml file to the proper location</span><br><span class="line">-(6) install the node libs what your theme needs to satisfy</span><br><span class="line"></span><br><span class="line">- for example, I use the indigo theme, I should run below commands</span><br><span class="line">cd &lt;github.io folder&gt;</span><br><span class="line"># 主题默认使用 less 作为 css 预处理工具。</span><br><span class="line">npm install hexo-renderer-less --save</span><br><span class="line"># 用于生成 rss。</span><br><span class="line">npm install hexo-generator-feed --save</span><br><span class="line"># 用于生成静态站点数据，用作站内搜索的数据源。</span><br><span class="line">npm install hexo-generator-json-content --save</span><br><span class="line"># 用于生成微信分享二维码。</span><br><span class="line">npm install hexo-helper-qrcode --save</span><br><span class="line"># error, hexo d后 ERROR Deployer not found: git</span><br><span class="line">npm install --save hexo-deployer-git</span><br></pre></td></tr></table></figure><h3 id="write-posts-and-deploy"><a href="#write-posts-and-deploy" class="headerlink" title="write posts and deploy"></a>write posts and deploy</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br><span class="line">hexo d</span><br><span class="line"></span><br><span class="line"># only fit for me to update my own blog.</span><br></pre></td></tr></table></figure><h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul><li><a class="link"   href="http://www.longmuchen.cn/2019/04/18/tools/2019-04-18-zai-ubuntu-14.04-fu-wu-qi-shang-bu-shu-hexo-bo-ke/" >http://www.longmuchen.cn/2019/04/18/tools/2019-04-18-zai-ubuntu-14.04-fu-wu-qi-shang-bu-shu-hexo-bo-ke/<i class="fas fa-external-link-alt"></i></a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;System-info&quot;&gt;&lt;a href=&quot;#System-info&quot; class=&quot;headerlink&quot; title=&quot;System info&quot;&gt;&lt;/a&gt;System info&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Ubuntu 16.04&lt;/li&gt;
&lt;li&gt;Git &lt;/</summary>
      
    
    
    
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
    <category term="blog" scheme="http://example.com/tags/blog/"/>
    
    <category term="rebuild" scheme="http://example.com/tags/rebuild/"/>
    
    <category term="myself" scheme="http://example.com/tags/myself/"/>
    
  </entry>
  
  <entry>
    <title>A way to deal with git clone slow</title>
    <link href="http://example.com/2019/11/22/A-way-to-deal-with-git-clone-slow/"/>
    <id>http://example.com/2019/11/22/A-way-to-deal-with-git-clone-slow/</id>
    <published>2019-11-22T14:12:45.000Z</published>
    <updated>2020-02-03T06:25:20.000Z</updated>
    
    <content type="html"><![CDATA[<p><a class="link"   href="https://www.jianshu.com/p/3f6477049ece" >转-git clone速度太慢的解决办法<i class="fas fa-external-link-alt"></i></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a class=&quot;link&quot;   href=&quot;https://www.jianshu.com/p/3f6477049ece&quot; &gt;转-git clone速度太慢的解决办法&lt;i class=&quot;fas fa-external-link-alt&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="git" scheme="http://example.com/tags/git/"/>
    
    <category term="github" scheme="http://example.com/tags/github/"/>
    
    <category term="slow" scheme="http://example.com/tags/slow/"/>
    
    <category term="repost" scheme="http://example.com/tags/repost/"/>
    
  </entry>
  
  <entry>
    <title>Review on 轻量级卷积神经网络的设计</title>
    <link href="http://example.com/2019/11/17/light-framework-for-CNN/"/>
    <id>http://example.com/2019/11/17/light-framework-for-CNN/</id>
    <published>2019-11-17T04:54:30.000Z</published>
    <updated>2019-11-17T06:23:44.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么需要轻量网络？"><a href="#为什么需要轻量网络？" class="headerlink" title="为什么需要轻量网络？"></a>为什么需要轻量网络？</h2><h3 id="经典backbone网络存在的问题"><a href="#经典backbone网络存在的问题" class="headerlink" title="经典backbone网络存在的问题"></a>经典backbone网络存在的问题</h3><ul><li>尽管随着近年来backbone网络的快速发展，由AlexNet到VGG，由VGG到GoogleNet，由GoogleNet到ResNet等等。模型的精度越来越高，</li><li>但是通常模型占用内存很大，模型参数很多，这样的话在边缘设备上运行就会很慢。</li><li>所以我们需要设计一些参数量更少的网络结构，这样的模型占用内存小，在边缘设备上跑的更快。</li></ul><h3 id="怎么解决这个问题"><a href="#怎么解决这个问题" class="headerlink" title="怎么解决这个问题"></a>怎么解决这个问题</h3><pre><code>1. 网络压缩2. 轻量网络的设计</code></pre><h2 id="本Review只是讨论轻量网络的设计"><a href="#本Review只是讨论轻量网络的设计" class="headerlink" title="本Review只是讨论轻量网络的设计"></a>本Review只是讨论轻量网络的设计</h2><ul><li>轻量网络设计的发展历程，我们按照提出的时间可以分为3个版本<br>  （1）Version 1， 【2016-2017】，SqueezeNet–&gt; MobileNet –&gt; ShuffleNet –&gt; Xception<br>  （2）Version 2， 【2017-2018】，SqueezeNext–&gt; MobileNetV2 –&gt; ShuffleNetV2<br>  （3）Version 3， 【2018-2019】，AutoShuffleNet–&gt; MobileNetV3</li><li>这里我们总结一下5个轻量网络，<strong>SqueezeNet，MobileNet，ShuffleNet，MobileNetV2，ShuffleNetV2</strong></li></ul><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><ol><li>核心思想<br>a. 用更多的1*1卷积核替代3*3卷积核的使用，因为1*1卷积核可以在保output&#x3D;input的同时减少了通道数<br>b. 在3*3卷积核中用更少的通道数，这样直接减少了模型的参数<br>c. 延迟下采样</li><li>模块化卷积fire module<br><img src="/img/light-cnn/squeezeNet.png" alt="fire module" title="fire module"></li></ol><ul><li>fire module包含2个部分：Squeeze Layer和Expend Layer。</li><li>Squeeze Layer直接用1*1降低了通道数并保持output&#x3D;input</li><li>Expend Layer是1*1卷积和3*3卷积的混合使用</li></ul><ol start="3"><li>实验结果</li></ol><ul><li>SqueezeNet的参数比AlexNet少50x，但是模型性能(accuracy)与AlexNet结晶。</li></ul><h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><ol><li>核心思想，深度可分离卷积结构 depthwise separable convolution</li></ol><ul><li>深度可分离卷积是把一个传统卷积层分解成了2个卷积层，depthwise -wise和point-wise convolutiion.<ul><li>depthwise -wise卷积是单独在通道上做的卷积，是分组卷积中groups&#x3D;channels的极端情况。</li><li>point-wise卷积是用1*1卷积核连接通道之间的特征信息。<br><img src="/img/light-cnn/mobilenetv1.png" alt="mobilenet" title="mobilenet"></li></ul></li><li>可以计算参数如下<br><img src="/img/light-cnn/orig-conv.png" alt="traditional-convolution" title="traditional convolution"><br><img src="/img/light-cnn/depth-wise-sep-con.png" alt="depthwise-separable-convolution" title="depthwise-separable-convolution"></li><li>结构图如下<br><img src="/img/light-cnn/depth-wise-conv.png" alt="depth-wise-convlution" title="depth-wise-convlution"></li></ul><ol start="2"><li>实验结果</li></ol><ul><li>深度可分离卷积实现了和传统卷积相同的输入和输出操作，在保证准确率的条件下，但是计算参数量减少约为原来的1&#x2F;7，计算时间减少约为原来的1&#x2F;9。</li></ul><h2 id="MobileNetV2"><a href="#MobileNetV2" class="headerlink" title="MobileNetV2"></a>MobileNetV2</h2><ol><li>核心思想，Inverted Residuals &amp; Linear Bottlenecks</li></ol><ul><li><p>首先说一下ResNet中的Residual block的思想，就是在进行3*3卷积之前，插入1*1卷积进行通道数的“压缩”，在3*3卷积之后又插入1*1卷积进行通道数的“扩张”（input~&#x3D;output）。<br><img src="/img/light-cnn/bottleneck.png" alt="bottleneck" title="bottleneck"></p></li><li><p>而Inverted Residuals则是反了过来，先用1*1卷积进行通道数的“扩张”，然后经过3*3的depthwise separable convolution，最后插入1*1卷积进行通道数的“压缩”(input~&#x3D;output).<br><img src="/img/light-cnn/inverted-residual.png" alt="inverted-residuals" title="inverted-residuals"></p></li><li><p>Linear Bottlenecks，original bottlenecks是在卷积层之后做elwise + Relu，而linear bottlenecks则是elwise + withou relu。原因是本来参数量不多的情况下，如果用relu让负半轴为0，那么模型的学习能力就更有限了，所以直接删除relu。</p></li><li><p>利用stride&#x3D;2的卷积层做下采样。结构图如下<br><img src="/img/light-cnn/mobilenet-v2.png" alt="mobilenet-v2" title="mobilenet-v2"></p></li></ul><h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h2><ol><li>核心思想，用1*1Group Conv做channel shuffle。<br><img src="/img/light-cnn/channel-shuffle.png" alt="channel-shuffle" title="channel-shuffle"></li></ol><ul><li>shuffle unit<br><img src="/img/light-cnn/shuffleNet-unit.png" alt="shuffle-unit" title="shuffle-unit"></li></ul><h2 id="ShuffleNetV2"><a href="#ShuffleNetV2" class="headerlink" title="ShuffleNetV2"></a>ShuffleNetV2</h2><ol><li>核心思想，Time &#x3D; T(FLOPS) + T(I&#x2F;O)。</li></ol><ul><li>提出了4个指导原则，<br>  a, 当input channel&#x3D;output channel，卷积计算所需的MAC(memory access cost)最为节省。<br>  <img src="/img/light-cnn/shfln-v2-g1.png" alt="Guidie-1" title="Guidie-1"><br>  b, 过多的group convolution操作会加大MAC开销。<br>  <img src="/img/light-cnn/shfln-v2-g2.png" alt="Guidie-2" title="Guidie-2"><br>  c, 网络结构整体的碎片化会减少其可并行优化的程序。<br>  <img src="/img/light-cnn/shfln-v2-g3.png" alt="Guidie-3" title="Guidie-3"><br>  d, Else-wise操作小号的时间较多。<br>  <img src="/img/light-cnn/shfln-v2-g4.png" alt="Guidie-4" title="Guidie-4"></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>实践中，首选是ShuffleNet-V2&#x2F;MobileNet-V2，第二选择是MoblileNet</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>SqueezeNet <a class="link"   href="https://arxiv.org/abs/1602.07360" >https://arxiv.org/abs/1602.07360<i class="fas fa-external-link-alt"></i></a><br>MobileNet <a class="link"   href="https://arxiv.org/abs/1704.04861" >https://arxiv.org/abs/1704.04861<i class="fas fa-external-link-alt"></i></a><br>ShuffleNet <a class="link"   href="https://arxiv.org/abs/1707.01083" >https://arxiv.org/abs/1707.01083<i class="fas fa-external-link-alt"></i></a><br>MobileNetV2 <a class="link"   href="https://arxiv.org/abs/1801.04381" >https://arxiv.org/abs/1801.04381<i class="fas fa-external-link-alt"></i></a><br>ShuffleNetV2 <a class="link"   href="https://arxiv.org/abs/1807.11164" >https://arxiv.org/abs/1807.11164<i class="fas fa-external-link-alt"></i></a><br>Resnet <a class="link"   href="https://arxiv.org/abs/1512.03385" >https://arxiv.org/abs/1512.03385<i class="fas fa-external-link-alt"></i></a></p><p>SqueezeNet<br><a class="link"   href="https://zhuanlan.zhihu.com/p/31558773" >https://zhuanlan.zhihu.com/p/31558773<i class="fas fa-external-link-alt"></i></a><br><a class="link"   href="https://blog.csdn.net/csdnldp/article/details/78648543" >https://blog.csdn.net/csdnldp/article/details/78648543<i class="fas fa-external-link-alt"></i></a></p><p>MobileNet<br><a class="link"   href="https://www.cnblogs.com/adong7639/p/7918527.html" >https://www.cnblogs.com/adong7639/p/7918527.html<i class="fas fa-external-link-alt"></i></a></p><p>MobileNetV2<br><a class="link"   href="https://blog.csdn.net/u011995719/article/details/79135818" >https://blog.csdn.net/u011995719/article/details/79135818<i class="fas fa-external-link-alt"></i></a></p><p>ShuffleNetV2<br><a class="link"   href="https://www.jianshu.com/p/71e32918ea0a" >https://www.jianshu.com/p/71e32918ea0a<i class="fas fa-external-link-alt"></i></a></p><p><a class="link"   href="https://zhuanlan.zhihu.com/p/35405071" >https://zhuanlan.zhihu.com/p/35405071<i class="fas fa-external-link-alt"></i></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;为什么需要轻量网络？&quot;&gt;&lt;a href=&quot;#为什么需要轻量网络？&quot; class=&quot;headerlink&quot; title=&quot;为什么需要轻量网络？&quot;&gt;&lt;/a&gt;为什么需要轻量网络？&lt;/h2&gt;&lt;h3 id=&quot;经典backbone网络存在的问题&quot;&gt;&lt;a href=&quot;#经典ba</summary>
      
    
    
    
    
    <category term="deep learning" scheme="http://example.com/tags/deep-learning/"/>
    
    <category term="light framwork" scheme="http://example.com/tags/light-framwork/"/>
    
    <category term="convolution" scheme="http://example.com/tags/convolution/"/>
    
  </entry>
  
  <entry>
    <title>pa-interview-record</title>
    <link href="http://example.com/2019/09/14/pa-interview-record/"/>
    <id>http://example.com/2019/09/14/pa-interview-record/</id>
    <published>2019-09-14T01:02:20.000Z</published>
    <updated>2020-02-17T02:26:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="2019-09-12-面试记录"><a href="#2019-09-12-面试记录" class="headerlink" title="2019-09-12 面试记录"></a>2019-09-12 面试记录</h2><h3 id="Yolo的发展历程"><a href="#Yolo的发展历程" class="headerlink" title="Yolo的发展历程"></a>Yolo的发展历程</h3><ul><li><p>我的回答是这样的</p></li><li><p>YoloV1</p><ul><li>把检测变成回归问题，去掉region proposal这一步</li><li>将输入图像划分为7*7区域，每个区域回归2个bounding box并做分类</li></ul></li><li><p>YoloV2</p><ul><li>改进网络，Batch Normalization, global average pooling, 1*1</li><li>采用anchor boxes</li><li>引入multi-scale training</li><li>引入维度聚类</li></ul></li><li><p>YoloV3</p><ul><li>改进网络，仿ResNet, 与ResNet-101或ResNet-152准确率接近,但速度更快</li><li>引入FPN的多级预测</li><li>替换softmax，使用logistic 分类器</li></ul></li></ul><h3 id="YoloV3为什么对小物体的检测变好了"><a href="#YoloV3为什么对小物体的检测变好了" class="headerlink" title="YoloV3为什么对小物体的检测变好了"></a>YoloV3为什么对小物体的检测变好了</h3><ul><li>多尺度训练</li><li>引入FPN的多级预测</li></ul><h3 id="IOU-tracker"><a href="#IOU-tracker" class="headerlink" title="IOU tracker"></a>IOU tracker</h3><h3 id="Yolo与Faster-RCNN的区别"><a href="#Yolo与Faster-RCNN的区别" class="headerlink" title="Yolo与Faster RCNN的区别"></a>Yolo与Faster RCNN的区别</h3><ul><li>YOLO与Fast R-CNN相比有较大的定位误差，与基于region proposal的方法相比具有较低的召回率。但是，YOLO在定位识别背景时准确率更高，而 Fast-R-CNN 的假阳性很高。</li></ul><h3 id="Faster-RCNN是如何训练的？"><a href="#Faster-RCNN是如何训练的？" class="headerlink" title="Faster-RCNN是如何训练的？"></a>Faster-RCNN是如何训练的？</h3><ul><li>4-step 交替training<ul><li>fine-tune RPN</li><li>固定RPN的参数fine-tuneFast-RCNN，没有共享卷积</li><li>训练RPN</li><li>训练Fast-RCNN</li></ul></li></ul><h3 id="Pooling反向传播"><a href="#Pooling反向传播" class="headerlink" title="Pooling反向传播"></a>Pooling反向传播</h3><ul><li>max pooling<ul><li>误差在最大的点上是1,其他是0</li></ul></li><li>average pooling<ul><li>误差是平均返回的</li></ul></li></ul><h3 id="CONV反向传播"><a href="#CONV反向传播" class="headerlink" title="CONV反向传播"></a>CONV反向传播</h3><h3 id="MTCNN的解释"><a href="#MTCNN的解释" class="headerlink" title="MTCNN的解释"></a>MTCNN的解释</h3><h3 id="gpu为什么比cpu快"><a href="#gpu为什么比cpu快" class="headerlink" title="gpu为什么比cpu快"></a>gpu为什么比cpu快</h3><ul><li>cuda库，并行处理矩阵计算</li></ul><h3 id="利用5个人脸关键点如何人脸矫正"><a href="#利用5个人脸关键点如何人脸矫正" class="headerlink" title="利用5个人脸关键点如何人脸矫正"></a>利用5个人脸关键点如何人脸矫正</h3><ul><li>做反射，乘以一个矩阵</li></ul><h3 id="Dropout为什么有效"><a href="#Dropout为什么有效" class="headerlink" title="Dropout为什么有效"></a>Dropout为什么有效</h3><ul><li>类似模型融合的思想</li></ul><h3 id="CNN经典网络的总结"><a href="#CNN经典网络的总结" class="headerlink" title="CNN经典网络的总结"></a>CNN经典网络的总结</h3><ul><li><p>VGG</p><ul><li>利用多个小的卷积核替代一个大的卷积核，不但参数减少，而且也增加了非线性</li><li>1×1卷积核的使用</li></ul></li><li><p>GoogLenet</p><ul><li>增加网络的宽度，每一层用多种卷积核的大小，然后通过模型训练，让模型自动选择合适的卷积核</li><li>V1<ul><li>利用1X1卷积核降维</li><li>取消了全链接层，用global average pooling层替代，减少参数</li><li>多级分类预测</li></ul></li><li>V2<ul><li>Batch Normalization, 取消Dropout和LRN</li><li>学习VGG, 用多个小的卷积核替代一个大的卷积核</li></ul></li><li>V3<ul><li>引入Factorization, 将一个较大的二维卷积核拆分成两个较小的一维卷积，比如将3´3卷积拆成1´3卷积和3´1卷积，也称为非对称卷积</li></ul></li><li>V4<ul><li>引入residual connection</li></ul></li></ul></li><li><p>ResNet</p><ul><li>残差网络， F(x)&#x3D;H(x)-x</li><li>全是3x3网络</li><li>使用BN</li><li>取消dropout, 全连接层</li><li>利用1x1降维，设计bottleneck层</li></ul></li><li><p>ResNeXt</p><ul><li>提出了“深”和“宽”之外的第3个维度</li><li>“基数”， cardinality<ul><li>卷积核分组做卷积操作，并行</li></ul></li></ul></li></ul><h3 id="Data-Agumentation"><a href="#Data-Agumentation" class="headerlink" title="Data Agumentation"></a>Data Agumentation</h3><ul><li>图片剪切，旋转，光照的变化，颜色通道的变化，增加噪声</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;2019-09-12-面试记录&quot;&gt;&lt;a href=&quot;#2019-09-12-面试记录&quot; class=&quot;headerlink&quot; title=&quot;2019-09-12 面试记录&quot;&gt;&lt;/a&gt;2019-09-12 面试记录&lt;/h2&gt;&lt;h3 id=&quot;Yolo的发展历程&quot;&gt;&lt;a</summary>
      
    
    
    
    
    <category term="interview" scheme="http://example.com/tags/interview/"/>
    
  </entry>
  
  <entry>
    <title>pa-prepare</title>
    <link href="http://example.com/2019/09/12/pa-prepare/"/>
    <id>http://example.com/2019/09/12/pa-prepare/</id>
    <published>2019-09-12T05:26:17.000Z</published>
    <updated>2019-09-12T05:43:40.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据结构与算法"><a href="#数据结构与算法" class="headerlink" title="数据结构与算法"></a>数据结构与算法</h3><ol><li>快排<ul><li>算法描述</li></ul><ol><li>先从序列中取出一个数作为基准数</li><li>分区过程, 将比这个数大的数全部放到它的右边, 小于或等于它的数全部放到它的左边</li><li>再对左右区间重复第二步, 直到各区间只有一个数</li></ol><ul><li>算法时间复杂度分析<ul><li>$$T(n) &#x3D; 2T(\frac{n}{2}) + f(n)$$</li><li>假设m次递归后结束，则<br>  $$T(n) &#x3D; 2^{m}T(1)+mn$$<br>  $$n &#x3D; 2^{m}T(1)$$</li><li>由于T(1)是常量，所以<br>  $$n &#x3D; 2^{m}, m &#x3D; log(n)$$</li><li>得到<br>  $$T(n) &#x3D; nT(1) + nlog(n)$$</li><li>因为n&gt;2时，nlog(n) &gt; n，所以快排最优情况下的时间复杂度是<br>  $$O(nlogn)$$ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">quick_sort</span>(<span class="params">array, l, r</span>):</span><br><span class="line">    <span class="keyword">if</span> l &lt; r:</span><br><span class="line">        q = partition(array, l, r)</span><br><span class="line">        quick_sort(array, l, q-<span class="number">1</span>)</span><br><span class="line">        quick_sort(array, q+<span class="number">1</span>, r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># partition algorithm</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">partition</span>(<span class="params">array, l, r</span>):</span><br><span class="line">    x = array[r]</span><br><span class="line">    i = l - <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(l, r):</span><br><span class="line">        <span class="keyword">if</span> array[j] &lt;= x:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            array[i], array[j] = array[j], array[i]</span><br><span class="line">    array[i+<span class="number">1</span>], array[r] = array[r], array[i+<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li>二分查找<ul><li>原数组必须是有序的 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search</span>(<span class="params">array, num</span>):</span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    right = <span class="built_in">len</span>(array) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = (left + right) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> num &lt; array[mid]:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> num &gt; array[mid]:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul></li><li>最大堆和最小堆，一般求无序数组中最大(小)的K个数</li></ol><h3 id="计算机视觉图形学基础"><a href="#计算机视觉图形学基础" class="headerlink" title="计算机视觉图形学基础"></a>计算机视觉图形学基础</h3><ul><li>图像本身就是一个函数</li></ul><ol><li>low-level feature<ul><li>图片锐化，拉普拉斯</li><li>边缘检测，sobel</li><li>图片模糊，中值滤波&#x2F;高斯滤波</li><li>图像二值化，灰度图</li><li>图像resize，插值（临近插值，双线性插值）</li></ul></li></ol><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><ol><li><p>Logistic regression</p></li><li><p>   SVM</p><ul><li>首先它是一个二类分类器，核心思想是在特征空间中寻找间隔最大化的分离超平面的线性分类器。<ul><li>当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；</li><li>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；</li><li>当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</li></ul></li><li>SVM 核函数之间的区别<ul><li>一般选择线性核和高斯核，也就是线性核与 RBF 核。 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。 RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。</li></ul></li></ul></li><li><p>   PCA</p></li><li><p>   Kmeans</p><ul><li>K-means的基本算法流程：<ol><li>初始化$k$个聚类中心$c1,c2,…,ck$</li><li>对于每个样本$x_i$和每个聚类中心$c_j$，计算样本与聚类中心之间的距离$d_{ij}$</li><li>对于每个样本$x_i$，基于其最小的$d_{ij}$把其分配到第$j$个类$C_j$</li><li>对于每个类$C_j$，计算其所有样本的均值作为新的聚类中心，重复步骤2和步骤3直至样本点所属的类不再变化或达到最大迭代次数</li></ol></li></ul></li><li><p>   L1和L2 regulation</p><ul><li>从参数W更新，即求梯度的角度来看，</li><li>L1是在原来式子上加多了一个$-\eta\frac{\lambda sgn(W)}{n}$</li><li>上式可知，当w大于0时，更新的参数w变小；当w小于0时，更新的参数w变大；所以，L1正则化容易使参数变为0，即特征稀疏化。</li><li>L2是在原来式子上加多了一个$-\eta\frac{\lambda}{n}W$</li><li>上式可知，当w趋向于0时，参数减小的非常缓慢，因此L2正则化使参数减小到很小的范围，但不为0。</li><li>L1使权重稀疏，L2使权重平滑，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0</li></ul></li></ol><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><ul><li>基础网络<ol><li>   VGG<ul><li>1*1</li><li>多个小卷积代替一个大的卷积</li><li>网络更深</li></ul></li><li>   Inception<ul><li>网络更宽</li><li>让网络自己选择合适的卷积核</li></ul></li><li>   ResNet<ul><li><p>解决网络加深时难以训练的问题</p></li><li><p>skip connection，计算残差</p></li><li><p>总结：</p></li><li><p>Inception V1——构建了1x1、3x3、5x5的 conv 和3x3的 pooling 的分支网络，同时使用 MLPConv 和全局平均池化，扩宽卷积层网络宽度，增加了网络对尺度的适应性；</p></li><li><p>Inception V2——提出了 Batch Normalization，代替 Dropout 和 LRN，其正则化的效果让大型卷积网络的训练速度加快很多倍，同时收敛后的分类准确率也可以得到大幅提高，同时学习 VGG 使用两个3´3的卷积核代替5´5的卷积核，在降低参数量同时提高网络学习能力；</p></li><li><p>Inception V3——引入了 Factorization，将一个较大的二维卷积拆成两个较小的一维卷积，比如将3´3卷积拆成1´3卷积和3´1卷积，一方面节约了大量参数，加速运算并减轻了过拟合，同时增加了一层非线性扩展模型表达能力，除了在 Inception Module 中使用分支，还在分支中使用了分支（Network In Network In Network）；</p></li><li><p>Inception V4——研究了 Inception Module 结合 Residual Connection，结合 ResNet 可以极大地加速训练，同时极大提升性能，在构建 Inception-ResNet 网络同时，还设计了一个更深更优化的 Inception v4 模型，能达到相媲美的性能。</p></li><li><p><a class="link"   href="https://blog.csdn.net/yato0514/article/details/81915800" >原文链接<i class="fas fa-external-link-alt"></i></a></p></li></ul></li><li>   1×1卷积的作用<ul><li>增加非线性, 实现跨通道的交互和信息整合</li><li>进行卷积核通道数的降维和升维</li></ul></li></ol></li><li>目标检测<ol><li>RCNN-series</li><li>Yolo-series</li><li>   SSD</li><li>   目标检测的一些发展趋势</li><li>在目标检测算法中，two stage的算法比one stage在检测小物体上更有效，此说法同意吗，为什么？<ul><li>基本上同意这个说法。</li><li>要说明这个问题主要从感受野的角度去看，one stage的方法，对于SSD，其采取多个特征图进行分类，但由于依赖网络中比较深的层（特征图），感受野很大，因而小物体检测不准确。同样，对于Yolo，由于在方法设计中就把原图分块，即设定了最后用于判断的特征图尺寸，其感受野也很大，因而对小物体判断也不准确。</li><li>相对于one stage方法要求同时分离前景和背景以及做出分类，two stage的方法由于proposal的存在可以先用简单的结构分出前景和背景（此时感受野小，特征图分辨率高），再通过深层网络做进一步分类和精修，提高准确率。</li><li>one stage的方法也有针对这个问题进行过优化，SSD增加相对不那么深的特征图层作判断，以减小感受野增加分辨率，但层数不深的特征图的判别能力有限，无法大幅增加准确率；Yolo v3增加了FPN，用多尺度特征来判断，增加了对小物体判别能力；RetinaNet也是one stage方法，用了FPN判别，此处对小物体检测更有效，另外其设计了focal loss的训练方式，此方式可认为把two stage中proposal达到的正负样本平衡以修改损失函数的方式达到类似效果，提高了训练效率和整体的准确率。</li></ul></li><li>MAP，mean average precision<ul><li><p>Precision &amp; Recall</p><ol><li>预测为Positive当中真正是Positivede比例<br>  $$Precision &#x3D; \frac{TP}{TP+FP}&#x3D;\frac{TP}{all-detections}$$</li><li>实际为Postivie当中被预测为Positive的比例<br>  $$Recall &#x3D; \frac{TP}{TP+FN}&#x3D;\frac{TP}{all-ground-truths}$$</li></ol></li><li><p>mAP的步骤</p><ol><li>对于某个类别$C$，在某一张图片上，首先计算$C$在一张图片上的$Precision$:</li></ol><ul><li>$$Precision &#x3D; 在一张图片上类别C识别正确的个数（也就是IoU&gt;0.5）&#x2F; 一张图片上类别C的总个数}$$</li></ul><ol start="2"><li>依然对于某个类别$C$，可能在多张图片上有该类别，下面计算类别C的AP指数：</li></ol><ul><li>$$AP &#x3D; 每张图片上的Precision求和 &#x2F; 含有类别C的图片数目$$</li></ul><ol start="3"><li>对于整个数据集，存在多个类别$C1、C2、C3$：</li></ol><ul><li>$$mAP &#x3D; 上一步计算的所有类别的AP和 &#x2F; 总的类别数目$$</li><li>相当于所有类别的AP的平均值</li></ul></li></ul></li></ol></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;数据结构与算法&quot;&gt;&lt;a href=&quot;#数据结构与算法&quot; class=&quot;headerlink&quot; title=&quot;数据结构与算法&quot;&gt;&lt;/a&gt;数据结构与算法&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;快排&lt;ul&gt;
&lt;li&gt;算法描述&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;先从序列中取出一</summary>
      
    
    
    
    
    <category term="interview" scheme="http://example.com/tags/interview/"/>
    
    <category term="computer vision" scheme="http://example.com/tags/computer-vision/"/>
    
  </entry>
  
  <entry>
    <title>note-4-cv-class</title>
    <link href="http://example.com/2019/08/29/note-4-cv-2019-08-29/"/>
    <id>http://example.com/2019/08/29/note-4-cv-2019-08-29/</id>
    <published>2019-08-29T08:22:23.000Z</published>
    <updated>2019-09-02T17:47:42.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><a class="link"   href="https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%AC%AC%E5%85%AB%E7%AB%A0_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.md" >ch8 object detection<i class="fas fa-external-link-alt"></i></a> </p></li><li><p><a class="link"   href="https://github.com/amusi/Deep-Learning-Interview-Book/blob/master/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89.md" >amusi-computer-vision<i class="fas fa-external-link-alt"></i></a></p></li></ul><h2 id="Class-2"><a href="#Class-2" class="headerlink" title="Class 2"></a>Class 2</h2><h3 id="CONV-amp-APP-low-level"><a href="#CONV-amp-APP-low-level" class="headerlink" title="CONV &amp; APP (low level)"></a>CONV &amp; APP (low level)</h3><ul><li><p>时间， 2019年8月29日 下午4:24</p></li><li><p>图片上的一阶导和二阶导</p><ul><li><strong>一阶导</strong>是灰度值的变化率</li><li>图像函数， $$y &#x3D; f(x)$$ </li><li>一阶导，粗边（缘）$$f\prime(x) &#x3D; y\prime &#x3D; f(x+1) - f(x)$$</li><li><strong>二阶导</strong>是一阶导的导数</li><li>二阶导，精细结构敏感(sensetive)，双边效应(double edge) $$f\prime\prime(x) &#x3D; f\prime(x+1) - f\prime(x) &#x3D; f(x+2) -f(x+1)-f(x+1)+f(x)$$</li></ul></li><li><p>特别的kernels</p><ul><li>高斯核(gaussian kernel)&#x2F;中值化(median kernel)，图像模糊</li><li>拉普拉斯(laplacian kernel), 图像锐化，颗粒感更强， ori+edge</li><li>sobel kernel, 边缘检测</li></ul></li></ul><h3 id="mid-level"><a href="#mid-level" class="headerlink" title="mid level"></a>mid level</h3><h2 id="Class-3"><a href="#Class-3" class="headerlink" title="Class 3"></a>Class 3</h2><ul><li><p>面试要求</p><ul><li><ol><li>concept(说出具体流程) &amp; code(写出代码)</li></ol></li><li><ol start="2"><li>corner case(边界情况怎么处理)</li></ol></li><li><ol start="3"><li>分析算法时间复杂度</li></ol></li></ul></li><li><p>链式法则，chain Rule</p><ul><li>Problems, 梯度爆炸和梯度弥失</li><li>coding more</li></ul></li><li><h2 id="regularization-正则化"><a href="#regularization-正则化" class="headerlink" title="regularization (正则化)"></a>regularization (正则化)</h2></li></ul><h2 id="Class-7"><a href="#Class-7" class="headerlink" title="Class 7"></a>Class 7</h2><h3 id="Initialization-Methods"><a href="#Initialization-Methods" class="headerlink" title="Initialization Methods"></a>Initialization Methods</h3><ul><li>网络参数初始化是一个很重要的开始，因为好的初始化会让网络更好更快的收敛。</li><li>常见的参数初始化方法<ul><li>Gaussian &#x2F; Xavier &#x2F; Kaiming Initialization，都是把参数初始化成一个均值(mean&#x3D;0)为零，标准差为一个常数(constant)的正态分布。</li><li>原因是， Help to pass the gradient&#x2F;less explosion &amp; vanishing</li><li><strong>Xavier: [Bengio 2010]</strong>, $$std(标准差)&#x3D;\sqrt{\frac{2} {f_{in}+f_{out}}}$$, $$f_{in} &#x3D;  input_channels * kernel_w * kernel_h$$，针对的是没有activation function或者activation function是线性的或者tanh等</li><li><strong>Kaiming (MSRA&#x2F;He) [He, 2015]</strong>,$$std(标准差)&#x3D;\sqrt{\frac{2} {f_{in}}}$$，针对的activation function是relu</li></ul></li></ul><h3 id="Image-Preprocessing"><a href="#Image-Preprocessing" class="headerlink" title="Image Preprocessing"></a>Image Preprocessing</h3><ul><li>Traditional Ways<ul><li><p>Mean subtraction</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X -= np.mean(X, axis=0) # 0: row, 1: col</span><br></pre></td></tr></table></figure></li><li><p>Normalization</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X /= np.std(X, axis=0)</span><br></pre></td></tr></table></figure></li><li><p>PCA</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = np.random.randn(<span class="number">1000</span>, <span class="number">500</span>)</span><br><span class="line">X -= np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">cov = np.dot(X.T, X) / (X.shape[<span class="number">0</span>] - <span class="number">1</span>)</span><br><span class="line">U, S, V = np.linalg.svd(cov)</span><br><span class="line"><span class="comment"># Xrot = np.dot(X, U)</span></span><br><span class="line">Xrot_reduced = np.dot(X, U[:, :<span class="number">100</span>])</span><br></pre></td></tr></table></figure><ul><li>找到相互正交投影方向，使得数据投影后最大的方差。方差越大，结果不可预测，信息量越大。</li><li>先找一个轴，报上去方差最大</li><li>再找2nd轴，报上去方差最大</li><li>。。。</li><li><strong>那如何找到方差最大的轴呢？</strong></li></ul></li><li><p>Whitening (Seldom use)</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xwhite = Xrot / np.sqrt(S + 1e-5)</span><br></pre></td></tr></table></figure></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a class=&quot;link&quot;   href=&quot;https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch08_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B</summary>
      
    
    
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
    <category term="machine learning" scheme="http://example.com/tags/machine-learning/"/>
    
    <category term="image processing" scheme="http://example.com/tags/image-processing/"/>
    
  </entry>
  
  <entry>
    <title>Reaction to Machine Learning Yearning</title>
    <link href="http://example.com/2019/08/29/Reaction-to-Machine-Learning-Yearning/"/>
    <id>http://example.com/2019/08/29/Reaction-to-Machine-Learning-Yearning/</id>
    <published>2019-08-29T01:20:35.000Z</published>
    <updated>2019-08-29T03:20:36.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a class="link"   href="https://www.deeplearning.ai/machine-learning-yearning/" >Machine Learning Yearning<i class="fas fa-external-link-alt"></i></a></li></ul><h3 id="1-Why-Machine-Learning-Strategy"><a href="#1-Why-Machine-Learning-Strategy" class="headerlink" title="1. Why Machine Learning Strategy"></a>1. Why Machine Learning Strategy</h3><ul><li>为什么要选择策略，因为一个机器学习的project，我们可以有很多思路，譬如<ul><li>增加图片</li><li>收集更多不同的训练数据集</li><li>增加训练时长</li><li>训练一个更多的网络</li><li>训练一个更小的网络</li><li>尝试添加正则化（如L2正则化）</li><li>改变神经网络的结构（激活函数， 隐藏单元个数等等）</li><li>。。。</li></ul></li><li>所以，面对如此多的ideas，如果我们选择了好的ideas，那么我们就会事半功倍。本书会告诉我们，对于大多数的机器学习问题，哪些ideas是有效的，哪些ideas是无效的。这便是所谓的机器学习的策略。</li></ul><h3 id="2-How-to-use-this-book-to-help-your-team"><a href="#2-How-to-use-this-book-to-help-your-team" class="headerlink" title="2. How to use this book to help your team"></a>2. How to use this book to help your team</h3><ul><li>简单来说，你可以把这本书打印出来，给你的teammates看，然后告诉他们，为什么你会推荐这样的机器学习策略。要知道，对于策略中优先次序的小小改变可能会对你团队的生产效率有巨大的影响。</li></ul><h3 id="3-Prerequisites-and-Notation"><a href="#3-Prerequisites-and-Notation" class="headerlink" title="3. Prerequisites and Notation"></a>3. Prerequisites and Notation</h3><ul><li>请参考Courera上的课程<a class="link"   href="http://ml-class.org/" >Machine Learning<i class="fas fa-external-link-alt"></i></a> 。</li></ul><h3 id="4-Scale-drives-machine-learning-progress"><a href="#4-Scale-drives-machine-learning-progress" class="headerlink" title="4. Scale drives machine learning progress"></a>4. Scale drives machine learning progress</h3><ul><li>大多数的深度学习思想已经被提出来数十年了，为什么现在才其作用呢？</li><li>最重要的2大推动<ul><li>数字数据爆炸式增长</li><li>计算力爆棚</li></ul></li></ul><h3 id="5-Your-development-and-test-sets"><a href="#5-Your-development-and-test-sets" class="headerlink" title="5. Your development and test sets"></a>5. Your development and test sets</h3><ul><li>就拿识别猫的项目做例子。我们通常在网上爬了一批猫(正例)和非猫(反例)，然后把他们7&#x2F;3比例划分训练集和测试集。就这样，我们训练了一个在训练集和测试集上表现比较好的模型。</li><li>然而，当我们部署到手机上的时候，发现效果真的非常差。原因是用户从手机上上传的图片可能是非常差，像素低，模糊，亮度低。但是你的训练集和测试集是从网上爬取的，这两套图片是属于不同的分布的。</li><li>另外，传统的数据划分大都是7&#x2F;3。这在做demo时是有效的，但实际应用上往往是一个bad idea。</li><li>dev set和test set的主要目的是指导你的团队如何对机器学习系统作出重大的改变的。</li><li>总结来说，得保证训练的和实际上的数据是同分布的。这一点非常重要。</li></ul><h3 id="6-Your-dev-and-test-sets-should-come-from-the-same-distribution"><a href="#6-Your-dev-and-test-sets-should-come-from-the-same-distribution" class="headerlink" title="6. Your dev and test sets should come from the same distribution"></a>6. Your dev and test sets should come from the same distribution</h3><ul><li>切记一点，你的训练集和测试集应该来自同一分布。</li><li>如果你的团队做到了以上这一点，然而你的模型在训练集上的变现非常好，而在测试集上的表现非常差，那么我们可以清楚的知道，模型已经对训练集overfit了。最直接的解决方案是增加更多的训练集数据。</li><li>但是，如果你的训练集和测试集是来自不同的分布， 而且又发生了上一条的情况的话，这样的情况比较不明朗。可能的原因是：<ul><li>对训练集（dev set）overfit</li><li>测试集比训练集更难拟合。你的算法尽力了。</li><li>测试集没必要那么难，所以你对训练集上做的大部分努力都是白费的。</li></ul></li><li>事实上，在一个分布上训练得到的模型在其他的分别上有强的泛化能力，这是一个很重要也很难的研究课题。但是现实中，如果我们是开发某一个特定的领域的应用，那么专注于同一分布的数据集来我们来说更有效，更有意义。</li></ul><h3 id="7-How-large-do-the-dev-x2F-test-sets-need-to-be"><a href="#7-How-large-do-the-dev-x2F-test-sets-need-to-be" class="headerlink" title="7. How large do the dev&#x2F;test sets need to be ?"></a>7. How large do the dev&#x2F;test sets need to be ?</h3><ul><li>dev set size : 一般是100～10,000之间。</li><li>test set size : 一般是总数据集的30%，但是现在是要大到足够给出系统性能的一个较高的置信。</li></ul><h3 id="8-Establish-a-single-number-evaluation-metric-for-your-team-to-optimize"><a href="#8-Establish-a-single-number-evaluation-metric-for-your-team-to-optimize" class="headerlink" title="8. Establish a single-number evaluation metric for your team to optimize"></a>8. Establish a single-number evaluation metric for your team to optimize</h3><p>-     </p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a class=&quot;link&quot;   href=&quot;https://www.deeplearning.ai/machine-learning-yearning/&quot; &gt;Machine Learning Yearning&lt;i class=&quot;fas fa-external</summary>
      
    
    
    
    
    <category term="Andrew Ng" scheme="http://example.com/tags/Andrew-Ng/"/>
    
    <category term="Machine Learning" scheme="http://example.com/tags/Machine-Learning/"/>
    
    <category term="Strategy" scheme="http://example.com/tags/Strategy/"/>
    
  </entry>
  
  <entry>
    <title>物体识别算法-RCN, Fast-RCNN, Faster-RCNN和YOLO</title>
    <link href="http://example.com/2019/08/27/Object-Detection-algo-RCNN-Fast-RCNN-Faster-RCNN-Yolo/"/>
    <id>http://example.com/2019/08/27/Object-Detection-algo-RCNN-Fast-RCNN-Faster-RCNN-Yolo/</id>
    <published>2019-08-27T01:37:35.000Z</published>
    <updated>2019-08-27T05:05:42.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>Origin link, <a class="link"   href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" > R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms<i class="fas fa-external-link-alt"></i></a> </p></li><li><p><img src="/img/media-od-rcnn-yolo/view.jpg" alt="view" title="view"></p></li></ul><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul><li>计算机视觉是一个交叉学科，(自从CNN发展以来)它已经取得了极大的进步并且无人驾驶汽车已经占据了舞台中心。另一个计算机视觉的组成部分是物体检测。物体识别在姿态预估，车辆识别和无人监控等地方都提升了应用。物体检测算法和分类算法的不同是，我们尝试在图片中利用bounding box来定位出我们感兴趣的物体。另外，你可能在一张图片上不止画出一个bouding box， 你可能会画出多个bouding box来表示我们预先不知道的我们所感兴趣的不同物体。<br><img src="/img/media-od-rcnn-yolo/cat.png" alt="cat" title="cat"><br><img src="/img/media-od-rcnn-yolo/duck.png" alt="duck" title="duck"></li><li>解决物体检测问题，为什么不能直接建立一个标准的CNN+FC的网络结构呢？主要的原因是输出层的长度是可变的，而不是固定的，因为我们感兴趣的物体出现的次数也是可变的。一个最naive的方法来解决这个问题就是我们从图片上划出大小不同的区域，然后利用CNN来进行分类。而这个方法的难点是物体出现的区域是大小不同的，有着不同的长宽比。所以，你需要选择巨量的区域框，而这是一个指数增长的计算量。所以，像RCNN和YOLO这些算法就被提出用来找到这些区域框，并且是要快速的找到。</li></ul><h3 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h3><ul><li>为了绕过要选择巨量的区域框的问题，<a class="link"   href="https://arxiv.org/pdf/1311.2524.pdf" >Ross Girshick et al<i class="fas fa-external-link-alt"></i></a> ，提出了一个方法，我们利用<strong>selective serach</strong>这个算法从一张图片上提取出2k个区域框，我们称之为区域候选框。然后，我们就直接对这2k个区域候选框进行分类，而不是像之前的巨量的区域框了。<strong>selective serach</strong>算法步骤如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Selective search:</span><br><span class="line">1. Generate initial sub-segmentation, we generate many candidate regions</span><br><span class="line">2. Use greedy algorithm to recursively combine similar regions into larger ones </span><br><span class="line">3. Use the generated regions to produce the final candidate region proposals</span><br></pre></td></tr></table></figure><img src="/img/media-od-rcnn-yolo/rcnn.png" alt="rcnn" title="rcnn"></li><li>想了解更多的<strong>selective serach</strong>算法，请参考以下<a class="link"   href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf" >链接<i class="fas fa-external-link-alt"></i></a> 。得到的2k个区域候选框会被压缩成一个正方形，然后喂给了CNN网络，它的输出是一个4096维的特征向量。CNN就是一个特征提取器，输出层包含了图片的特征，然后这个特征层就会喂给了<strong>SVM</strong>用以分类。另外，为了预测候选框上是否存在物体，算法同样预测了4个groud truth的偏移量，一增加bouding box的精度。譬如，给定一个区域候选框，算法可能预测了人的存在，但是那个人的头一半是在候选框之外，所以这个偏移量是用来调整bouding box的。<br><img src="/img/media-od-rcnn-yolo/rcnn-2.png" alt="rcnn-2" title="rcnn-2"></li></ul><h4 id="RCNN的问题"><a href="#RCNN的问题" class="headerlink" title="RCNN的问题"></a>RCNN的问题</h4><ul><li>每张图片要训练2k次网络是一个非常耗时的操作</li><li>测试的时候每张图片需要花费47秒</li><li><strong>selective serach</strong>算法是一个固定的算法，所以，这个算法是没有学习的过程的。这样就容易产生一些差的区域候选框</li></ul><h3 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h3><p><img src="/img/media-od-rcnn-yolo/fast-rcnn.png" alt="fast rcnn" title="fast rcnn"></p><ul><li>RCNN相同的作者提出了解决RCNN本身存在的问题的算法， 我们称之为<strong>Faster RCNN</strong>。这个算法和RCNN有比较多的相同之处，不同的是Faster RCNN不是喂区域候选框给CNN网络，而是直接喂原图片，然后生成一个卷积feature map。从这个卷积feature map中，我们生产候选框，然后压缩成正方形，再利用ROI pooling 处理成固定大小的feature map，然后喂给了全连接层。输出的ROI特征向量，我们利用softmax层来预测候选框中的物体类别和bounding box的偏移量。</li><li>Fast RCNN比RCNN快的原因是你不需要每次都喂2k个候选框给CNN网络，进行2k次CNN的操作。而是一张图只进行一次CNN网络的操作，然后得到feature map。<br><img src="/img/media-od-rcnn-yolo/fast-rcnn-cmp.png" alt="fast rcnn cmp" title="fast rcnn cmp"></li><li>由以上对比图，我们可知fast-rcnn在训练和测试阶段都比rcnn提升了非常多的时间。当你看到fast rcnn的测试时间， 区域候选框的提取成为了fast rcnn的bottlenecks。这就是我们在faster rcnn中需要解决的问题。</li></ul><h3 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h3><p><img src="/img/media-od-rcnn-yolo/faster-rcnn.png" alt="faster rcnn" title="faster rcnn"></p><ul><li><strong>RCNN</strong>和<strong>Fast RCNN</strong>都使用了<strong>selective search</strong>算法来找出候选框。而<strong>selective search</strong>是一个非常耗时的过程，这严重影响了算法的性能。所以， <a class="link"   href="https://arxiv.org/pdf/1506.01497.pdf" >Shaoqing Ren et al<i class="fas fa-external-link-alt"></i></a> ，提出了一个物体检测算法，它剔除了<strong>selective search</strong>算法，而是让网络自己学习找多区域候选框。</li><li>和<strong>Fast RCNN</strong>相似，图片直接喂给CNN网络进行特征提取，得到feature map。然后我们不用<strong>selective search</strong>来提前候选框，而是利用另一个CNN网络来预测区域候选框。预测的区域候选框之后再通过ROI pooling层，得到输出的特征层，最后在这个特征层上做区域候选框的预测和bounding box的偏移量预测。<br><img src="/img/media-od-rcnn-yolo/rcnn-test-time.png" alt="RCNN Test Time" title="RCNN Test Time"></li><li>由以上图可见到，Faster RCNN比RCNN和Fast RCNN在测试速度方面都提升了非常多。所以，它甚至是可以用到实时的物体检测了。</li></ul><h3 id="YOLO-你只需看一次"><a href="#YOLO-你只需看一次" class="headerlink" title="YOLO-你只需看一次"></a>YOLO-你只需看一次</h3><iframe src="https://giphy.com/embed/10VZfgNU9YBPpK" width="480" height="360" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a class="link"   href="https://giphy.com/gifs/explosion-throwing-yolo-10VZfgNU9YBPpK" >via GIPHY<i class="fas fa-external-link-alt"></i></a></p>- **RCNN系列**的检测算法都是利用区域来定位图片中的物体。CNN网络并没有看到完整的图片。但是，图片的某些部分是非常有可能存在物体的。YOLO和**RCNN系列**算法有很大的不同。在YOLO当中，单个CNN网络预测了bouding boxes和这些boxes中的物体的类别。![yolo](/img/media-od-rcnn-yolo/yolo.png  "yolo")- **YOLO**算法直接把一张图片分成**SxS**的网格，在每个网格中我们取m个bounding boxes。对每个bounding box，网络输出了类别概率和bounding box的偏移量。bounding boxes满足类别概率大于设定阈值的被用来定位图片中的物体。- YOLO是比其他检测方法快一个数量级的（达到45/fps）。YOLO的限制是对小物体的识别比较难。譬如，它很男检测到一群鸟。这是由于算法的空间限制。### 总结- 计算机视觉会议每年都收到大量新的新颖的想法，我觉得我们会一步步的利用AI迈向了更好的检测表现。会越来越好。我希望这些你们能够清晰的读懂了这些概念。谢谢！<iframe src="https://giphy.com/embed/DCHmHrxi4PG92" width="480" height="329" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a class="link"   href="https://giphy.com/gifs/batman-dislike-DCHmHrxi4PG92" >via GIPHY<i class="fas fa-external-link-alt"></i></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Origin link, &lt;a class=&quot;link&quot;   href=&quot;https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorith</summary>
      
    
    
    
    
    <category term="translation" scheme="http://example.com/tags/translation/"/>
    
    <category term="物体识别" scheme="http://example.com/tags/%E7%89%A9%E4%BD%93%E8%AF%86%E5%88%AB/"/>
    
    <category term="Faster-RCNN" scheme="http://example.com/tags/Faster-RCNN/"/>
    
    <category term="RCNN" scheme="http://example.com/tags/RCNN/"/>
    
    <category term="YOLO" scheme="http://example.com/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>iou tracker implementation</title>
    <link href="http://example.com/2019/08/14/iou-tracker-implementation/"/>
    <id>http://example.com/2019/08/14/iou-tracker-implementation/</id>
    <published>2019-08-14T02:11:09.000Z</published>
    <updated>2019-08-14T06:43:10.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a class="link"   href="http://elvera.nue.tu-berlin.de/files/1517Bochinski2017.pdf" >[paper] High-Speed Tracking-by-Detection Without Using Image Information<i class="fas fa-external-link-alt"></i></a> </li><li>It’s a simple tracker algorithm that just use the IOU (Intersection over Union). Obviously it’s so fast if the detector runs fast.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a class=&quot;link&quot;   href=&quot;http://elvera.nue.tu-berlin.de/files/1517Bochinski2017.pdf&quot; &gt;[paper] High-Speed Tracking-by-Detection Witho</summary>
      
    
    
    
    
  </entry>
  
</feed>
